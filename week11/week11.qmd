---
title: Week 11
---

::: content-hidden
$$
\newcommand{\E}[0]{\mathbb E}

\newcommand\independent{\perp\!\!\!\perp}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcommand{\Var}[0]{\text{Var}}
\newcommand{\Cov}[0]{\text{Cov}}
\newcommand{\Pr}[0]{\mathrm{Pr}}
\newcommand{\e}[0]{\epsilon}
\newcommand{\t}[1]{\text{#1}}
\newcommand{\bmat}[1]{\begin{bmatrix} #1 \end{bmatrix}}
$$

window.MathJax = {
  loader: {load: ['[tex]/cancel']},
  tex: {packages: {'[+]': ['cancel']}}
};
:::

Recap: 

The score function for $\beta_j$ in a GLM can be written as 

$$\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \frac{\partial \mu_i}{\partial \eta_i} \frac{x_{ij}}{V(\mu_i)a_i(\phi)} (y_i - \mu_i)$$

Which depends on the distribution $Y_i$ solely through $\E[Y_i] = \mu_i$ and
$\Var[Y_i] = V(\mu_i)a_i(\phi)$. 

Notice that the $(p+1)$ score equations for $\beta$ do not depend on 
$\phi$. Consequently, obtaining the MLE of $\beta$ doesn't require knowledge of 
$\phi$. 

Inference does require an estimate of $\phi$. 

# Asymptotic Sampling Distribution 

Subject to appropriate regularity conditions, 

$$\bmat{\hat \beta_{MLE} \\ \hat \phi_{MLE}} \sim \text{MVN} \left( 
    \bmat{\beta \\ \phi}, \mathcal I_n (\beta, \phi)^{-1}
    \right).$$

Now we compute the components of $\mathcal I_n(\beta, \phi):$ 

$$\frac{\partial^2 \ell}{\partial \beta_j \partial \beta_k} = \frac{\partial}{\partial \beta_k} \underbrace{\left\{ \frac{\partial \mu_i}{\partial \eta_i} \frac{x_{ij}}{V(\mu_i) a_i(\phi)} (y_i - \mu_i) \right\}}_{\text{score for } \beta_j}.$$

$$ = (y_i - \mu_i) \frac{\partial}{\partial \beta_k} \left\{ \frac{\partial \mu_i}{\partial \eta_i} \frac{x_{ij}}{V(\mu_i) a_i(\phi)} \right\} - \left( \frac{\partial \mu_i}{\partial \eta_i} \right)^2 \frac{x_{ij} x_{ik}}{V(\mu_i) a_i(\phi)}.$$

Hence 
$$-\E\left[\frac{\partial^2 \ell}{\partial \beta_j \partial \beta_k} \right] = 
\sum_{i=1}^n \left( \frac{\partial \mu_i}{\partial \eta_i} \right)^2 \frac{x_{ij}x_{ik}}{V(\mu_i) a_i(\phi)}.$$

$$
\begin{aligned}
\frac{\partial^2 \ell}{\partial \beta_j \partial \phi} & = 
\frac{\partial}{\partial \phi} \left\{ 
    \frac{\partial \mu_i}{\partial \eta_i} \frac{x_{ij}}{V(\mu_i)a_i(\phi)} (y_i - \mu_i)
\right\} \\ 
& = - \frac{a_i'(\phi) \partial \mu_i x_{ij}}{a_i(\phi)^2 \partial \eta_i V(\mu_i)} (y_i - \mu_i) 
\end{aligned}
$$

Thus 

$$-E \left[ \frac{\partial^2 \ell}{\partial \beta_j \partial \phi} \right] = 0$$

$$\begin{aligned}\frac{\partial^2 \ell}{\partial \phi \partial \phi} & = \frac{\partial}{\partial \phi} 
\left\{ 
    - \frac{a_i'(\phi)}{a_i(\phi)^2} (y_i\theta_i - b(\theta_i)) + c'(y_i, \phi)
\right\} \\ 
& = - \left\{ \frac{a_i(\phi)^2 a_i''(\phi) - 2a_i(\phi) a_i'(\phi)^2}{a_i(\phi)^4}\right\} [y_i \theta_i - b(\theta_i)] + c''(y_i, \phi) \\ 
& = -K(\phi)[y_i\theta_i - b(\theta_i)] + c''(y_i, \phi)
\end{aligned}$$

and thus 
$$-\E\left[ \frac{\partial^2 \ell}{\partial \phi \partial \phi} \right] = 
\sum_{i=1}^n K(\phi)[b'(\theta_i)\theta_i - b(\theta_i)] - \E[c''(Y_i, \phi)].$$

## Form of the Fisher Matrix 

We can write the expected information matrix in block-diagonal form: 

$$\mathcal I_n(\beta, \phi) = \bmat{
    \mathcal I_{\beta \beta} & 0 \\ 
    0 & \mathcal I_{\phi \phi}
}.$$

The inverse of the information matrix is the asymptotic variance

$$\Var[\hat \beta_{MLE}, \hat \phi_{MLE}] = \mathcal I_n(\beta, \phi)^{-1} = 
\bmat{\mathcal I^{-1}_{\beta \beta} & 0 \\ 0 & \mathcal I_{\phi \phi}^{-1}}.$$


The block diagonal structure of $\Var[\hat \beta_{MLE}, \hat \phi_{MLE}]$ implies
that for GLMs, valid characterization of the uncertainty in $\hat \beta_{MLE}$ does
not require the propagation of uncertainty in the estimate of $\phi$. 

For example, for linear regression on Normally distributed outcomes, we plug in an 
estimate of $\sigma^2$ into 
$$\Var[\hat \beta_{MLE}] = \sigma^2 (X^T X)^{-1}$$
without worrying about uncertainty in estimation of $\sigma^2$. 

For GLMs, therefore, estimation of $\Var(\hat \beta_{MLE})$ proceeds by plugging in the values of $(\hat \beta_{MLE}, \hat \phi)$ into $\mathcal I_{\beta \beta}^{-1}$, i.e., 
$$\widehat{\Var}[\hat \beta_{MLE}] = \hat{\mathcal{I}}_{\beta \beta}^{-1}$$
where $\hat \phi$ is *any* consistent estimator of $\phi$. 

### Matrix notation 

Recall that the $(j,k)^{\text{th}}$ element of $\mathcal I_{\beta \beta}$ has the form 

$$(\mathcal I_{\beta \beta})_{jk} = \sum_{i=1}^n \underbrace{\left( \frac{\partial \mu_i}{\partial \eta_i} \right)^2 \frac{1}{V(\mu_i) a_i(\phi)}}_{\coloneqq W_i} x_{ij}x_{jk},$$

so we can write 

$$(\mathcal I_{\beta \beta})_{jk} = \sum_{i=1}^n W_i x_{ij}x_{jk}.$$

$W_i$ sort of looks like the inverse of the variance of $Y_i$. 

We can therefore write: 

$$\mathcal I_{\beta \beta} = X'WX \quad \text{ and } \quad \Var(\hat \beta_{MLE}) = \mathcal I_{\beta \beta}^{-1} = (X'WX)^{-1},$$

where $W = \text{diag}(W_1, ..., W_n)$ and $X$ is the design matrix.

Note that $(X'WX)^{-1}$ looks a lot  like $\Var(\hat \beta_{GLS})$, just instead
of $W$ we used $\Sigma^{-1}$. 

# Hypothesis Testing

For the linear predictor $x_i' \beta$, suppose we partition $\beta = (\beta_1, \beta_2)$ and we are interested in testing: 

$$H_0 : \beta_1 = \beta_{1,0} \quad \text{vs.} \quad H_a : \beta_1 \neq \beta_{1,0}$$

The length of $\beta_1$ is $q \leq (p+1)$ and $\beta_2$ is left arbitrary. 

In most settings $\beta_{1,0} = 0$ which represents some form of 'no effect' 

Following our review of asymptotic theory, there are three common hypothesis testing 
frameworks: Wald, score, and likelihood ratio testing.

## Wald Test

Let $\hat \beta_{MLE} = (\hat \beta_{1,MLE}, \hat \beta_{MLE})$. 

Under $H_0$

$$(\hat \beta_{1,MLE} - \beta_{1,0})' \widehat{\Var}[\hat \beta_{1,MLE}]^{-1}(\hat\beta_{1,MLE} - \beta_{1,0}) \longrightarrow_d \chi^2_q$$

where $\widehat{\Var}[\hat \beta_{1,MLE}]$ is the $q\times q$ sub-matrix of 
$\mathcal{I}_{\beta\beta}^{-1}$ corresponding to $\beta_1$, evaluated at 
$\left( \hat \beta_{MLE}, \hat \phi_{MLE} \right).$

This is the multivariate analog of $((\text{Est} - \text{Null})/\text{SE})^2$.

The Wald test is just looking at how far our estimate of $\beta$ is from the 
null and seeing if that distance is large relative to the uncertainty in our 
estimate. 


## Score Test 

Let $\hat \beta_{0,MLE} = (\beta_{1,0}, \hat \beta_{2,MLE})$ and $\hat \phi_{0,MLE}$
denote the MLEs under $H_0$. 

Under $H_0$, 

$$U(\hat \beta_{0,MLE}, \hat \phi_{0,MLE}; y)' \mathcal I_n(\hat \beta_{0,MLE}; \hat \phi_{0, MLE})^{-1} U(\hat \beta_{0,MLE}, \hat \phi_{0,MLE}; y) \longrightarrow_d \chi^2_q.$$

## Likelihood Ratio Test 

Obtain unrestricted MLEs: $(\hat \beta_{MLE}, \hat \phi_{MLE})$, and obtain MLEs under 
$H_0:$ $(\hat \beta_{0, MLE}, \hat \phi_{0,MLE})$. 

Under $H_0$,

$$2(\ell(\hat \beta_{MLE}, \hat \phi_{MLE}; y) - \ell(\hat \beta_{0,MLE}, \hat \phi_{0, MLE} ; y)) \longrightarrow_d \chi^2_q.$$

# Estimating $\beta$: Newton-Raphson Algorithm

We saw that the score equation for $\beta_j$ is 

$$\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \frac{\partial \mu_i}{\partial \eta_i} \frac{x_{ij}}{V(\mu_i) a_i(\phi)} (y_i - \mu_i) = 0$$

Estimation of $\beta$ requires solving $(p+1)$ of these equations simultaneously. This is tricky because $\beta$ appears in several places 
(implicitly anywhere a $\mu$ appears). 

No closed form solution in general is available (though there is one
for linear settings), so we need an iterative method. A commonly used 
general purpose optimization routine is the Newton-Raphson algorithm. 

General idea of Newton-Raphson and Fisher Scoring 

General idea of Newton-Raphson 

1. Approximate log-likelihood via 2nd order Taylor series 
2. Take derivative of approximate log likelihood, set equal to zero, and solve
3. Leads to updates of the form 

$$\beta^{(r+1)} = \beta^{(r)} - \left[ \mathcal{I}_{\beta \beta}^{(r)}\right]^{-1} U^{(r)}_{\beta}$$

Fisher scoring is an adaptation of the Newton-Raphson algorithm that uses
the expected information, $\mathcal I_{\beta \beta}$, rather than observed
information $\mathcal I_{\beta \beta}$ for the update. 

There's a close relationship between Fisher scoring and IRLS. Often this is exploited for computation in software packages, many of which use IRLS 
by default (including R's `glm()`). 

The idea: 

Define 

$$Z_i = \overbrace{g(\mu_i) + (Y_i - \mu_i)g'(\mu_i)}^{\text{First order Taylor approx of }g(Y_i)}$$ 

to be an "adjusted response variable" to show that 

$$\Var(Z_i) = (W_i)^{-1}, \text{ where }W_i = \left( \frac{\partial \mu_i}{\partial \eta_i} \right)^2 \frac{1}{V(\mu_i)a_i(\phi)}$$

Then note that 

$$\E[Z_i] = x_i'\beta$$

so we can consider a linear model for $Z$ to estimate $\beta$, i.e., 
using the GLS estimator $\hat \beta_{GLS} = (X'WX)^{-1} X'WZ$. 

Two immediate challenges to estimating $\hat \beta_{GLS}$ in practice are

  1. We don't observe $Z$
  2. $W$ depends on $\beta$ 

However, we can use IRLS to do estimation. 

Suppose the current estimate of $\beta$ is $\hat \beta^{(r)}$. Compute

$$\eta_i^{(r)} = x_i'\hat \beta^{(r)}$$
$$\mu_i^{(r)} = g^{-1} (\eta_i^{(r)})$$
$$W_i^{(r)} = \left( \frac{\partial \mu_i}{\partial \eta_i} \biggr \mid_{\eta_i^{(r)}} \right)^2 \frac{1}{V(\mu_i^{(r)})}$$
$$z_i^{(r)} = \eta_i^{(r)} + (y_i - \mu_i^{(r)}) \frac{\partial \eta_i}{\partial \mu_i} \mid_{\mu_i^{(r)}}$$ 

$W_i$ is called the 'working weight'

Check the McCullagh and Nelder (1989) book which shows that using
IRLS for estimation here is equivalent to Fisher scoring. 
For more detail, see Agresti 4.6.3 or <https://grodri.github.io/glms/notes/>

The updated version of $\hat \beta$ is obtained as the WLS estimate to 
the regression of $Z$ on $X$: 

$$\hat \beta^{(r+1)} = (X' W^{(r)} X)^{-1} (X' W^{(r)} Z^{(r)})$$

$X$ is the $n \times (p+1)$ design matrix from the initial specification of the model. $W^{(r)}$ is the diagonal $n \times n$ matrix with entries 
$\{ W_1^{(r)}, ..., W_n^{(r)}}$. $Z^{(r)}$ is the $n$-vector $(z_1^{(r)}, ..., z_n^{(r)})$. 

Iterate until the $\hat \beta$ values converges. 

# Fitting GLMs in R 

A generic call to `glm()` is given by 

`fit0 <- glm(formula, family, data, ...)`

`formula` specifies the structure of the linear predictor $\eta_i = x_i' \beta$. 

`family` jointly specifies the probability distribution $f_Y()$, link
function $g()$ and variance function $V()$. 

E.g.,

`glm(Y ~ X_1 + X2, family = binomial(), data = df)`

Most common family objects are `binomial()` or `poisson()`. 

```{r}
myFamily <- binomial()

names(myFamily)

myFamily$link
```

The `residuals` inside a `glm()` are called working residuals and
used internally for iteratively reweighted least squares. 

# Residuals 

In linear models, we used residual diagnostics to examine 
the adequacy of model fit and investigate potential data issues
such as outliers. Adequacy of model fit included functional form for
terms in the linear predictor and the homoscedasticity assumption.

In GLMs, residual diagnostics are much more complex and visual inspection of
residual plots is typically less informative. 

In general, residuals are meant to represent variation in the outcome that is not explained by the model. 

Variation after the systematic component is accounted for, and therefore
residuals are therefore model-specific. 

:::{.bluetip}
In linear models, residuals were considered as empirical estimates of 
the true error, e.g., $\hat \varepsilon \coloneqq Y_i - \hat Y_i$. 

Raw residuals are not generally useful in the GLM setting.

For example, with a binary predictor and a binary outcome, there's only 
four possible values the raw residuals could take on. 

Another issue is heteroscedasticity — because many of the families
fit with GLMs there is a mean-variance relationship. 
:::

*Pearson residuals* account for the heteroscedasticity via standardization. 

$$r_i^p = \frac{y_i - \hat \mu_i}{\sqrt{\Var(Y_i)}}$$

We'll see that Pearson residuals can be useful for diagnosing departures
from our model's variance assumption. 

The *deviance residual* (sometimes called the "preferred residual") is defined as 

$$r_i^d = \text{sign}(y_i - \hat \mu_i)\sqrt{d_i}$$

where, letting $\ell(\hat \mu_i, \phi; y_i)$ be the log-likelihood contribution
of unit $i$ evaluated as $\hat \mu_i$ and $\phi$, then 
$$d_i = 2 \phi(\ell(y_i, \phi; y_i) - \ell(\hat \mu_i, \phi; y_i))$$

We can think about this as the distance between $\hat \mu_i$ and $y_i$ on the log-likelihood scale. 

Pierce and Shafer (JASA, 1986) examined various residuals for GLMs. 
<https://www.jstor.org/stable/2289071> 

One can get both deviance and Pearson residuals from `glm()` in R: 

```
fit <- glm(Y ~ X, family = binomial)

residuals(fit)

residuals(fit, type='pearson')
```


# Intro to Western Collaborative Group Study 

A prospective study of coronary heart disease (CHD) with $n = 3,154$ male 
participants aged 39-54 at risk for CHD, employed at 10 companies in California
with a baseline survey and measurements taken at intake (1960-61) along with 
annual surveys until December 1969 to assess incident CHD. 

Our primary goal is to investigate the relationship between 'behavior pattern' and
risk of CHD. 

Participants were categorized into one of two behavior pattern groups. 

Type A: characterized by enhanced aggressiveness, ambitiousness, competitive drive, and chronic sense of urgency.

Type B: characterized by more relaxed and non-competitive. 

For now, we will ignore any loss to follow-up and consider the binary 
outcome of 

$$Y = \left\{ \begin{array} 
1 \quad & \text{occurrence of CHD during follow-up} \\ 
0 & \text{otherwise}
\end{array} \right. $$

8.1% of participants developed CHD during follow-up, while 50.4 of participants were classified as Type A at baseline. 

$$\hat P(\t{CHD} = 1 \mid \text{Type} = A) = 0.112$$
$$\hat P(\t{CHD} = 1 \mid \t{Type} = B) = 0.050$$

I.e., 11.2% of Type A men and 5% of Type B men developed CHD during follow-up. 

Often we will use the generic term 'risk,' e.g., "risk of CHD" rather
than probability of CHD during follow-up. 


## Types of Contrasts

We need to pick a contrast — 

Risk difference: 

$$RD = P(Y = 1 \mid x = 1) - P(Y = 1 \mid x = 0)$$
$$\widehat{RD} = \hat P(Y = 1 \mid x = 1) - \hat P(Y=1 \mid x = 0) = .112 - .050 = .062$$

The difference in the estimated risk of CHD during follow-up between type A and 
type B men is 0.062 (or 6.2%). This characterizes the way in which the additional risk of CHD of being a type A person manifests through an *absolute* increase.

We've already seen the OR as a relative-scale contrast. A more interpretable option is the 
relative risk: 

$$RR = \frac{P(Y = 1 \mid x = 1)}{P(Y = 1 \mid x = 0)}$$

$$\widehat{RR} = \frac{\hat P(Y = 1 \mid x = 1)}{\hat P(Y = 1 \mid x = 0)} = \frac{0.112}{0.050} = 2.24 $$

### Log-likelihood for GLMs with binary outcomes 

$$\ell(\beta; y) = \sum_{i=1}^n y_i \theta_i - b(\theta_i)$$

$$ = \sum_{i=1}^n y_i \theta_i - \log(1 + \exp\{\theta_i\})$$

Where $\theta_i$ is a function of $\beta$ via 

$$\mu_i = b'(\theta_i) = \frac{\exp \theta_i}{1 + \exp \theta_i}$$

and 

$$\mu_i = g^{-1}(x_i'\beta)$$

## Score and information for GLMs with Binary outcomes 

The score function for $\beta_j$ is 

$$\frac{\partial \ell }{\partial \beta_j} = \sum_{i=1}^n \frac{\partial \mu_i}{\partial \eta_i} \frac{x_{ij}}{\mu_i (1-\mu_i)} (y_i - \mu_i)$$

where the expression for $\partial \mu_i / \partial \eta_i$ depends on the 
choice of $g()$. 

Since $\phi$ is fixed and known, the expected information matrix is just 
$$\mathcal I_{\beta \beta} = X' W X$$

where $X$ is the design matrix for the model and $W$ is a diagonal matrix with $i^{\t{th}}$ diagonal element 

$$W_i = \left( \frac{\partial \mu_i}{\partial \eta_i} \right)^2 \frac{1}{\mu_i (1-\mu_i)},$$

which we've said previously looks like an inverse variance of $Y$. 

