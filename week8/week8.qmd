---
title: Week 8
---

::: content-hidden
$$
\newcommand{\E}[0]{\mathbb E}

\newcommand\independent{\perp\!\!\!\perp}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcommand{\Var}[0]{\text{Var}}
\newcommand{\Cov}[0]{\text{Cov}}
\newcommand{\Pr}[0]{\mathrm{Pr}}
\newcommand{\e}[0]{\epsilon}
\newcommand{\t}[1]{\text{#1}}
$$

window.MathJax = {
  loader: {load: ['[tex]/cancel']},
  tex: {packages: {'[+]': ['cancel']}}
};
:::

# Recap

Last time we saw several ways of dealing with heteroscedasticity: 

  * transformations, which we've already seen, but is not ideal since it changes the scientific question being asked.
  * robust variance estimators and the bootstrap â€” ways to estimate the variance alongside the OLS estimates of $\beta$. 
  * and we just started with generalized least squares.

## Generalized Least Squares

The Gauss-Markov theorem says that $\hat \beta_{OLS}$ is BLUE when $\Sigma = \sigma^2 I$, but now we're 
in a more general setting. 

If we estimate $\beta$ by minimizing the Mahalanobis distance between $Y$ and $X \beta$, we can get an estimator
that is BLUE under arbitrary $\Sigma$. 

The squared Mahalanobis distance .. 

GLS can be used for any variance-covariance matrix $\Sigma$ including with correlated data. Supposing that 
$\Sigma$ is known, one can easily show that $\hat \beta_{GLS}$ is unbiased, that it's the MLE 
if we assume $\varepsilon \sim \mathcal N(0,\Sigma)$, and its varinace is 

$$
\Var[\hat \beta_{GLS}] = (X' \Sigma X)^{-1}.
$$

We show that $\hat \beta_{GLS}$ is BLUE under heteroscedasticity by: 

  1. Writing the GLS estimator as an OLS estimator in a transformed space that has homoscedastic error, and 
  2. Applying the Gauss-Markov Theorem. 

For any positive-definite matrix $\Sigma$ we can find a nonsingular matrix $A$ such that $\Sigma = AA$. 

Assuming $\Sigma$ is known, then define $Z = A^{-1} Y$ and $C = A^{-1} X$. Linear regression in the 
transformed space is 

$$
Z = C \gamma + \varepsilon^*,
$$

where $\varepsilon^* = A^{-1} \varepsilon$. 

Note that $\E[\varepsilon^*] = 0$ and $\Var[\varepsilon^*] = I_n$, so we have homoscedastic errors in the transformed space. 

Also note that both of the following hold: 

$$\E[Z] = C \gamma
$$

$$
\E[Z] = A^{-1} \E[Y] = A^{-1} X \beta = C \beta 
$$

So $\gamma \equiv \beta$. 

The OLS estimator for this transformed regression problem is 

$$
\begin{aligned}
\hat \gamma_{OLS} & = (C'C)^{-1} C' Z \\ 
& = (X'\Sigma^{-1}X)^{-1} X' \Sigma^{-1} Y \\ 
& = \hat \beta_{GLS}
\end{aligned}
$$

so $\hat \beta_{GLS}$ is equivalent to the OLS estimator in the transformed space. 

By the Gauss-Markov Theorem, $\hat \beta_{GLS}$ is BLUE for $\gamma = \beta$ in the 
transformed space. 

Since $\E[\varepsilon^*] = 0$ and $\Var[\varepsilon^*] = I_n$, this implies that for any other unbiased 
estimator $\tilde \beta = DZ$, $\Var(\hat \beta_{GLS}) \leq \Var(\tilde \beta)$. 

The optimality in the transformed space is retained by $\hat \beta_{GLS}$ is the original space. This is because for any linear unbiased estimator in the original space $\tilde \beta = LY$ can also be written as a linear unbiased estimator in the transformed space, $\tilde \beta = LY = LAZ$. 

So $\hat \beta_{GLS}$ is BLUE for the linear model where $\Var[\varepsilon] = \Sigma$. 

# Weighted Least Squares 

Returning to our special case when $\Sigma = \text{diag}(\sigma_1^2, ..., \sigma_n^2)$, which 
implies the observations are independent with heteroscedasticity, this case is sometimes referred to as 
"weighted least squares," because 

$$
(Y - X \beta)' \Sigma^{-1} (Y-X\beta) = \sum_{i=1}^n \frac{1}{\sigma_i^2} (y_i - x_i' \beta)^2
$$

so that we are estimating $\beta$ by minimizing a weighted version of the SSE with weights $1/\sigma_i^2$. 

This effectively upweights units with low variance and downweights those with high variance so that estimates are selected to give better fit for units with low variance. 

## Estimating $\hat \beta_{GLS}$ 

In practice we don't know the $\sigma_i^2$ values and need to estimate them.  We had the 
same challenge with the robust variance estimator for the OLS estimator, $\widehat{\Var}_{HW}[\hat\beta_{OLS}]$. 

Recall our three potential estimation strategies: 

   1. Estimate $\sigma_i^2$ directly from the squared residual: $\hat \varepsilon_i^2 = (y_i - \hat \mu_i)^2
   2. Assume $\sigma_i^2 = V(\mu_i)$ and estimate $V(\cdot)$ via smoothing of $\varepsilon_i^2$ vs. $\hat \mu_i$. 
   3. Assume $\sigma_i^2 = V(\mu_i)$ and estimate $V(\cdot)$ via grouping based on $\hat \mu_i$. 

However, each of these strategies requires $\hat \mu_i = x_i' \hat \beta_{GLS}$, but estimating 
$\hat \beta_{GLS}$ requires estimates of $\sigma_i^2$. 

## Iteratively re-weighted least squares 

:::{.chilltip}
*The iteratively reweighted least squares algorithm.*

Start with an initial estimate of $\beta$, denoted $\hat \beta^{(1)}$, for which we could use the 
OLS estimator $\hat \beta_{OLS}$. 

At the $j^{th}$ iteration: 

  1. Calculate $\hat \mu_i^{(j)} = x_i' \hat \beta^{(j)}$
  2. Use the $\hat \mu_i^{(j)}$ to compute $\hat \sigma_i^{2(j)}$ using one of the three approaches outlined above
  3. Obtain a new estimate $\hat \beta^{(j+1)}$ by minimizing 

  $$
  \sum_{i=1}^n \frac{1}{\hat \sigma_i^{2(j)}} (y_i - x_i' \beta)^2
  $$

  with respect to $\beta$.

  Repeat steps (1)-(3) until the value of $\hat \beta$ converges. 
:::

<br>

:::{.cooltip}
Some resources that I might be interested in: 

  * [A Nice Writeup](https://anilkeshwani.github.io/files/iterative-reweighted-least-squares-12.pdf) 
  * [Wikipedia](https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares)
  * [SE Thread on Convergence](https://stats.stackexchange.com/questions/36250/definition-and-convergence-of-iteratively-reweighted-least-squares) 
  * [SE Thread on IRLS Confusions](https://stats.stackexchange.com/questions/521065/iteratively-reweighted-least-squares-weights-confusion) 
:::

# Inference for Binomial Propoertions and Contingency Tables 

## The Binomial Distribution 

We will use data from a study on the teratogenicity of anti-epilectic drugs (AEDs) to motivate modeling and inference for discrete data. The data come from 922 births in 5 Boston area hospitals during 1986 and 1993 and are described in Holmes et al. (2000). The teratogenicity of anticonvulsant drugs. *New England Journal of Medicine* 344: 1132-1138. 

The study included 316 pregnant women taking anticonvulsants (intentionally over-sampled) and 606 pregnant control women and recorded whether the infants had any malformation. 

:::{.bluetip}
A teratogen is any agent that causes an abnormality following fetal exposure during pregnancy
:::

| Infant Status | N | Percent | 
|---|---|---|
|No major malformation | 895 | 97.1% | 
|Major malformation | 27 | 2.9% | 

The percent of infants with a major malformation is slightly higher than the US national rate of 2%. 

Is this difference statistically significant? 

What is the 95% confidence interval for the risk of having a major malformation based on this data? 

### The Binomial Distribution 

A binomially distributed random variable is the number of events resulting from $n$ independent Bernoulli trials, each with probability success $\pi$. 

Let $Y_i=1$ indicate that infant $i$ has a major malformation and $Y_i = 0$ otherwise. 

Each $Y_i$ comes from a Bernoulli distribution such that 

$$
\begin{aligned}
Pr(Y_i = y_i) & = \pi^{y_i} (1-\pi)^{1-y_i} \\ 
Pr(Y_i = 0) & = 1- \pi \\ 
Pr(Y_i = 1) & = \pi.
\end{aligned}
$$

Let $Y = \sum_{i=1}^n Y_i$. Then $Y$ is binomial distributed and its pmf is 

$$
Pr(Y = y) = { n \choose y } \pi^y (1-\pi)^{n-y}
$$

where $y = 0, ..., n.$

Let's remind ourselves of the derivation of the mean and the variance for $Y$, a binomial distributed random variable 

$$
\begin{aligned}
\E(Y_i) & = \sum_{y_i=0}^1 y_i Pr(Y_i = y_i) \\ 
& = 0 \times Pr(Y_i = 0) + 1 \times Pr(Y_i = 1) \\ 
& = \pi \\ 
\E(Y) & = \E(Y_1) + ... + \E(Y_n) \\ 
& = n \pi.
\end{aligned}
$$

And the variance 

$$
\begin{aligned}
\Var(Y_i) & = \E(Y_i - \E(Y_i))^2 \\ 
& = (0-\pi)^2 \times Pr(Y_i = 0) + (1-\pi)^2 \times Pr(Y_i = 1) \\ 
& = \pi^2 (1-\pi) + (1-\pi)^2 \pi \\ 
& = \pi(1-\pi).
\end{aligned}
$$

$$
\begin{aligned}
\Var(Y) & = \Var(Y_1 + ... + Y_n) \\ 
& = \Var(Y_1) + ... + \Var(Y_n) \\ 
& = n \pi (1 - \pi).
\end{aligned}
$$

Note that we make use of the independence assumption. Note also that the binomial variance is a function of the mean. 

```{r}
#| out.width: 100% 
#| fig.width: 5.5
#| fig.height: 10
library(tidyverse)
library(patchwork)

plt1 <- tibble(
  x = seq(0, 10),
  y = dbinom(x = x, size = 10, 0.1) * 10000
) |> 
ggplot(aes(x = x, y = y)) + 
geom_col(fill = "#f0932b", alpha = 0.6) + 
theme_bw() + 
ggtitle("10,000 Draws from a Binomial(10,0.1) Distribution") + 
labs(y = "Frequency", x = expression(Y[i]))

plt2 <- tibble(
  x = seq(0, 10),
  y = dbinom(x = x, size = 10, 0.5) * 10000
) |> 
ggplot(aes(x = x, y = y)) + 
geom_col(fill = "#f0932b", alpha = 0.6) + 
theme_bw() + 
ggtitle("10,000 Draws from a Binomial(10,0.5) Distribution") + 
labs(y = "Frequency", x = expression(Y[i]))

plt3 <- tibble(
  x = seq(0, 50),
  y = dbinom(x = x, size = 922, 0.029) * 10000
) |> 
ggplot(aes(x = x, y = y)) + 
geom_col(fill = "#f0932b", alpha = 0.6) + 
theme_bw() + 
ggtitle("10,000 Draws from a Binomial(022,0.029) Distribution") + 
labs(y = "Frequency", x = expression(Y[i]))

plt1 / plt2 / plt3
```

The central limit theorem states that standardized sums or averages of independent random variables are asymptotically normal (under light regularity conditions). 

Thus asymptotically, 

$$
\frac{Y - \E(Y)}{s.e.(Y)} \sim \mathcal N(0, 1)
$$

and for $\hat \pi = Y/n$, 

$$
\begin{aligned}
\frac{\hat \pi - \E[\hat \pi]}{s.e.(\hat \pi)} & \stackrel{\cdot}{\sim} \mathcal N(0,1) \quad \text{ where } \stackrel{\cdot}{\sim} \text{ indicates as } n \to \infty \\ 
\E[\hat \pi] & = \E(\frac{1}{n} - \sum Y_i) = \frac{1}{n} \cdot n \pi = \pi \\ 
\Var[\hat \pi] & = \Var\left(\frac{1}{n} \sum Y_i\right) = \frac{1}{n^2} \cdot n \pi (1-\pi) = \frac{\pi (1-\pi)}{n},
\end{aligned}
$$

where $\hat \pi = \frac{\sum Y_i}{n}.$

## Large Sample Inference: A Single Proportion 

Consider testing whether the probability of major malformation in our study sample is equal to the national probability of $\pi_0 = 0.02$. 

$$
H_0 \colon \pi = \pi_0 \quad \text{versus} \quad H_A \colon \pi \neq \pi_0.
$$

Using a Wald test: 

$$
Z_W = \frac{\hat \pi - \pi_0}{\sqrt{\hat \pi (1 - \hat \pi) / n }} \stackrel \cdot \sim \mathcal N(0,1).
$$

Plugging in to compute the Z-statistic gives 

$$
Z_W = \frac{0.0293 âˆ’ 0.02}{\sqrt{0.0293(1 âˆ’ 0.0293)/922}} = 1.67.
$$

The associated two-tailed $p$-value is given by 

$$
p = 2 \times Pr(\mathbb Z \geq 1.67) = 0.0949
$$ 

where $\mathbb Z$ is a standard normal random variable.

## Problems with Wald-based inference

Suppose we observe $y = 0$ or $y = n$, which correspond to $\hat \pi = 0$ and $\hat \pi = 1$ scenarios.

The probability of this happening is non-negligible when $n$ is small and/or when $\pi$ is close to zero or one. 

In either case, the Wald statistic, 

$$
Z_W = \frac{\hat \pi - \pi_0} { \sqrt{\hat \pi ( 1 - \hat \pi ) / n }}
$$

has a zero in the denominator and is therefore undefined. 

:::{.chilltip}
Note that in the regression scenario, the 
Wald statistic is $t$-distributed because the 
$\hat \beta$ is normally distributed and 
dividing it by its standard error yields
a $t$-distribution. 

Here, we're using large sample asymptotics
(that the distribution of estimated proportions
are normally distributed in a certain kind of limiting sense) to get a $Z$-score Wald statistic.
:::

## A better test: the Score Statistic

The score test averts this problem by using the null rather than the estimated
standard error. 

$$
\begin{aligned}
Z_S & = \frac{\hat \pi - \pi_0}{\sqrt{\pi_0 ( 1- \pi_0)/n}} \\ 
& = \frac{0.0293 âˆ’ 0.02}{\sqrt{0.02(1 âˆ’ 0.02)/922}} = 2.02.
\end{aligned}
$$

The associated two-tailed $p$-value is given by 

$$
p = 2 \times Pr(\mathbb Z \geq 2.02) = 0.0440
$$

where $\mathbb Z$ is a standard normal random variable. 

The evidence against $H_0$ is slightly stronger using the more appropriate score test. 


```{r}
prop.test(27, 922, p = 0.02, correct = FALSE)
```

(For the purposes of this class, we'll want to specify `correct = FALSE` â€” in other settings one may want to perform the continuity correction available in `prop.test`, but we won't be using it in this class.)

The $\chi^2$ value (`X-squared`) in the output above is $(Z_S)^2$, which is approximately 
$\chi^2$ distributed under the null. The $p$-value from this $\chi^2$ test is the same as the one based on $Z_S$. 

## Confidence Intervals

:::{.chilltip}
One way of describing the confidence interval for a parameter is:

*The set of values which, when used as the null value in a hypothesis test on the observed sample, would lead to failure to reject.*
:::

Suppose we have a 95% confidence interval for $\pi$, $(\pi_L, \pi_U)$, and were to do a hypothesis
test in the observed sample using $\alpha = 0.05$ using as our null value anything in $(\pi_L, \pi_U)$,
we would fail to reject.

Recall that often we find CIs by inverting test statistics.

Let $z_{1-\frac{\alpha}{2}}$ denote the $1 - \frac{\alpha}/2$ quantile of a standard normal random variable. 

An approximate $100(1-\alpha)$% Wald confidence interval is found by inverting the Wald statistic. We will fail to reject in a Wald test with $\alpha = 0.05$ when 

$$-z_{1-\frac{\alpha}{2}} \le \frac{\hat \pi - \pi}{s.e.(\hat \pi)} \le z_{1-\frac{\alpha}{2}} 
$$

$$-z_{1-\frac{\alpha}{2}}s.e.(\hat \pi) \le \hat \pi - \pi \le z_{1-\frac{\alpha}{2}}s.e.(\hat \pi) 
$$

$$-\hat\pi-z_{1-\frac{\alpha}{2}}s.e.(\hat \pi) \le - \pi \le -\hat + \pi z_{1-\frac{\alpha}{2}}s.e.(\hat \pi) 
$$

$$ \hat\pi+z_{1-\frac{\alpha}{2}}s.e.(\hat \pi) \ge \pi \ge \hat \pi - z_{1-\frac{\alpha}{2}}s.e.(\hat \pi) 
$$

$$ \hat\pi-z_{1-\frac{\alpha}{2}}s.e.(\hat \pi) \le \pi \le \hat \pi + z_{1-\frac{\alpha}{2}}s.e.(\hat \pi) 
$$

The 95% confidence interval for the risk of having a major malformation is given by 

$$(\hat \pi - 1.96 \times s.e.(\hat \pi), \hat \pi + 1.96 \times s.e.(\hat pi))
$$ 

$$(0.0293 - 1.96 \times 0.0056, 0.0293 + 1.96 \times 0.0056)
$$

$$
(0.0184, 0.0402)
$$

We can conclude this sample is consistent with the 2% prevalence 
reported in the general US population. 

Note that if $y = 0$, the Wald interval will be (0,0) and if $y = n$ it will be (1,1). The Wald CI suffers from similar issues as the Wald test. 

### Score Interval for a Single Proportion 

A simple fix is to invert the score test instead to get a score CI. 

We will fail to reject with the score test when 

$$-z_{1-\alpha/2} \leq \frac{\hat \pi - \pi_0}{\sqrt{\pi_0 (1-\pi_0)/n}} \leq 
z_{1-\alpha/2}
$$

The upper and lower CI limits are solutions for $\pi_0$ from the equations 

$$(\hat \pi - \pi_0) / \sqrt{\pi_0(1-\pi_0)/n} = \pm z_{1-\alpha/2}
$$
which are quadratic in $\pi_0$. 

The resulting $100(1-\alpha)$% score interval is (Wilson, 1927): 

$$
\begin{aligned}
& \hspace{1in} \underbrace{\hat \pi \left( \frac{n}{n + z_{1+\alpha/2}^2} \right) + \frac{1}{2} \left( 
\frac{z_{1+\alpha/2}^2}{n + z_{1+\alpha/2}^2}\right)}_{\text{the estimate part}} \pm z_{1+\alpha/2} 
 \, \times \\ 
& \sqrt{
  \frac{1}{n + z_{1+\alpha/2}^2 } \left[ 
    \hat \pi (1 - \hat \pi) \left( \frac{n}{n + z_{1+\alpha/2}^2} \right) + 
    \left(\frac{1}{2}\right)\left(\frac{1}{2}\right) 
    \left( \frac{z_{1+\alpha/2}^2}{n + z_{1+\alpha/2}^2} \right)
  \right]
}
\end{aligned}
$$

The "estimate part" (midpoint) is a weighted average of $\hat \pi$ and 1/2. 

The "variance part" involves a weighted average of $\hat \pi (1-\hat \pi)$ and (1/2)(1-1/2). 

Thus the interval works with an estimator that shrinks the sample proportion back towards 1/2. 

We can obtain the score CI in R from `prop.test()`

```{verbatim, lang='r'}
prop.test(27, 922, p=.02,correct=FALSE)

    1-sample proportions test without continuity correction

data: 27 out of 922, null probability 0.02
X-squared = 4.0547, df = 1, p-value = 0.04405
alternative hypothesis: true p is not equal to 0.02
95 percent confidence interval:
0.02020271 0.04227177
sample estimates:
         p
0.02928416
```

## Exact Inference for Proportions 

In the same study, investigators were interested in assessing whether exposure *in utero* to the specific
AED carbamazepine is associated with an elevated rate of major malformation. 

3 out of 58 study infants exposed to carbamazepine had a major malformation. 

Is the risk for these infants compatible with the risk for the general US population (i.e., 2%)? 

```{r}
#| out.width: 100% 
#| fig.width: 5.5
#| fig.height: 3.5

hist(rbinom(n = 10000, size = 58, 0.02))

tibble(
  x = seq(0, 10),
  y = dbinom(x = x, size = 58, 0.02) * 10000
) |> 
ggplot(aes(x = x, y = y)) + 
geom_col(fill = "#f0932b", alpha = 0.6) + 
theme_bw() + 
ggtitle("10,000 Draws from a Binomial(58,0.02) Distribution") + 
labs(y = "Frequency", x = expression(Y[i]))
```

According to these data, what is an appropriate 95% confidence interval for the risk of having a major malformation for an infant receiving carbamazepine? 

We don't have to rely on large sample approximation, the validity of which may be in question here. We can instead answer these questions using exact statistical inference by leveraging our assumption that the number of events follows a binomial distribution. 

Say we want to test 

$$
H_0 \colon \pi = \pi_0 \quad \quad \text{vs} \quad \quad H_A \colon \pi < \pi_0
$$

Our test statistic is $Y = $ number of infants with major malformations out of $n = 58$ births. 

Under the null hypothesis, $Y \sim \text{Binomial}(n, \pi_0)$, i.e.

$$\mathrm{Pr}(Y = y \mid H_0 \colon \pi = \pi_0) = 
{n \choose y } \pi_0^y (1-\pi_0)^{n-y}
$$

The $p$-value is the sum of the probabilities for events that are at least or more extreme (in the direction of the alternative hypothesis) than what we proposed, given that the null hypothesis $H_0 \colon \pi = \pi_0$ is true. 

If you observe $Y = y$ successes, the exact $p$-value is 

$$
\begin{aligned}
p & = \mathrm{Pr}(Y \le y \mid H_0 \colon \pi = \pi_0) \\ 
& = \sum_{j = 0}^y {n \choose y} \pi_0^j (1-\pi_0)^{n-j}
\end{aligned}
$$

:::{.bluetip}
For our example, we would want to use a sum over all possible $Y$ values that are *greater* than what we've observed, rather than *less than*. 
:::

Say we want to test 
$$H_0 \colon \pi = \pi_0 \quad \quad \text{vs} \quad \quad H_A \colon \pi > \pi_0
$$

The exact $p$-value is 

$$
\begin{aligned}
p & = \mathrm{Pr}(Y \ge y \mid H_0 \colon \pi = \pi_0) \\ 
& = \sum_{j=0}^y {n \choose y} \pi_0^j (1-\pi_0)^{n-j}
\end{aligned}
$$

1. Calculate the probability of the observed result under $H_0$

$$
p = \mathrm{Pr}(Y = y \mid \pi = \pi_0) = {n \choose y} \pi_0^y (1-\pi_0)^{n-y}
$$

2. Calculate the probabilities of all other outcomes 

$$p_j = \mathrm{Pr}(Y=y \mid \pi=\pi_0) = {n \choose j} \pi_0^j (1-\pi_0)^{n-j},
$$
for $j = 0,...,n.$

3. Sum the probabilities $p_j$ in (2) that are less than or equal to the observed probability $p$ in (1),

$$
p = \sum_{j=0}^n p_j I(p_j \leq p), \quad \quad \text{where}
$$

$$I(p_j \leq p) = \left\{ \begin{array}{l} 1 \text{ if } p_j \leq p \\ 0 \text{ if } p_j > p \end{array}\right. .
$$

3 of the 58 infants exposed had a major malformation. Is the risk compatible with the US average? 

The relevant probabilities are 

```{r}
tibble(
  x = seq(0, 10),
  y = dbinom(x = x, size = 58, 0.02)
) |> 
ggplot(aes(x = x, y = y, label = scales::number_format(accuracy = 0.001)(y))) + 
geom_col(fill = "#f0932b", alpha = 0.6) + 
geom_text(aes(y = y + .02)) + 
theme_bw() + 
scale_x_continuous(breaks = seq(0,10,by=2)) + 
ggtitle("Probability Distribution for a Binomial(58,0.02) Distribution") + 
labs(y = "Frequency", x = expression(Y[i]))
```

We just want to sum up the values in the following bars: 

```{r}
tibble(
  x = seq(0, 10),
  y = dbinom(x = x, size = 58, 0.02)
) |> 
ggplot(aes(x = x, y = y, fill = x > 3, label = scales::number_format(accuracy = 0.001)(y))) + 
geom_col(alpha = 0.6) + 
geom_text(aes(y = y + .02), size = 3) + 
scale_fill_manual(values = c(`TRUE` = 'firebrick', `FALSE` = '#f0932b'),
labels = c('excluded', 'included in p-value')) + 
theme_bw() + 
ggtitle("Probability Distribution for a Binomial(58,0.02) Distribution") + 
scale_x_continuous(breaks = seq(0,10,by=2)) + 
labs(y = "Frequency", x = expression(Y[i])) + 
theme(legend.position = 'bottom')
```

```{verbatim, lang='r'}
binom.test(3, 58, p = 0.02,

        alternative = "two.sided",
        conf.level = 0.95)

    Exact binomial test

data: 3 and 58
number of successes = 3, number of trials = 58,
p-value = 0.1101

alternative hypothesis: true probability of success
is not equal to 0.02

95 percent confidence interval:
0.01079648 0.14380463
sample estimates:
probability of success
            0.05172414
```

We can check that this $p$-value given is the same 
as 
```{r, eval=FALSE}
sum(dbinom(x = seq(0,10), size = 58, 0.02)[4:10])
#> 0.1101484
```

Recall our definition of an exact test. 

An exact test is when $\Pr(\text{Type I Error}) \leq \alpha$, while a conservative (exact) test is when $\Pr(\text{Type I Error}) < \alpha$. Remember that 
$\Pr(\text{Type I Error}) = \Pr(\text{Rejecting } \, \, H_0)$. 

When we say $\alpha = 0.05$, exact tests are typically conservative (i.e., the actual Type I error rate is less than the nominal Type I error rate). This is due to the discreteness of the binomial distribution. This is the binomial analogue to the permutation test we considered previously. 

To illustrate, consider the carbamazepine example. 
We did not reject for $y=3\,\, (p=0.110)$. Suppose we had observed $y = 4$. Then $p = 0.029$. Therefore, the rejection region $R$ for the test 

$$
H_0 \colon \pi = 0.02 \quad \quad \text{vs} \quad \quad H_A \colon \pi \neq 0.02
$$

when $n = 58$ is $R = \{ 4, 5, 6, ... \}$.

By definition, then, the Type I error rate for the test is 

$$P(\mathrm{Reject }\,\, H_0 \mid H_0) = P(Y \in \{ 4, 5, 6, ... \} \mid H_0 ) = 0.029
$$

Does this conservativeness increase or decrease as the sample size increases? 

<!-- 
Increase!
-->

## Clopper-Pearson Exact Confidence Intervals 

Exact confidence intervals can be obtained by 
inverting the exact test. 

The "tail method" for forming confidence intervals
requires each one sided p-value to exceed $\alpha / 2$. 

The upper confidence limit, $\pi_U$ is obtained by solving 

$$
\begin{aligned}
\alpha / 2 &= \Pr(Y \le y \mid \pi_0 = \pi_U, n) \\ 
& = \sum_{j=0}^y {n \choose j} \pi_U^j (1-\pi_U)^{n-j}
\end{aligned}
$$

That is, the upper limit is 

$$
\sup \{ \pi \mid \Pr(\text{Binom}(n, \pi) \leq y) > \alpha / 2 \}. 
$$

This is the largest value of $\pi$ such that if we used it as $\pi_0$ in a lower tailed $\alpha/2$ test, we'd fail to reject. 

The lower confidence limit, $\pi_L$, is obtained by 
solving 

$$
\begin{aligned}
\alpha / 2 & = \Pr(Y \ge y \mid \pi_0 = \pi_L, n) \\ 
& = \sum_{j=y}^n {n \choose j} \pi_L^j (1- \pi_L)^{n-j} 
\end{aligned}
$$

That is, the lower limit is the 

$$ \inf \{ \pi \mid \Pr(\text{Binom}(n,\pi) \ge y) > \alpha / 2 \}
$$

3/58 infants exposd to carbamazepine had a major malformation. The R function for computing the Clopper-Pearson interval is `Hmisc::binconf`

```{r}
suppressMessages(library(Hmisc))
binconf(3, 58, method = 'exact')
```

Just like the exact test is conservative, the confidence interval is conservative. 
It is guaranteed to have at least $100(1-\alpha)$ coverage, which leads to wider confidence intervals. 

:::{.chilltip}
Generally the Wald test doesn't perform well at all 
for proportions. Not recommended in practice at all. Score test works well for moderately sized datasets. The score tests, one tends to see only for really small samples as they're more computationally intensive.
:::