<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Methods (BST 232) Notes - 12&nbsp; Week 11</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../week12/week12.html" rel="next">
<link href="../week10/week10.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-title">Week 11</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Methods (BST 232) Notes</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Methods (BST 232) Notes</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week1/week1.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 1</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week2/week2.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 2</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week3/week3.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 3</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week4/week4.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 4</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week5/week5.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 5</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week6/week6.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 6</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week7/week7.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 7</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week8/week8.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 8</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week9/week9.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 9</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week10/week10.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 10</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week11/week11.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Week 11</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week12/week12.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 12</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#asymptotic-sampling-distribution" id="toc-asymptotic-sampling-distribution" class="nav-link active" data-scroll-target="#asymptotic-sampling-distribution">Asymptotic Sampling Distribution</a>
  <ul class="collapse">
  <li><a href="#form-of-the-fisher-matrix" id="toc-form-of-the-fisher-matrix" class="nav-link" data-scroll-target="#form-of-the-fisher-matrix">Form of the Fisher Matrix</a>
  <ul class="collapse">
  <li><a href="#matrix-notation" id="toc-matrix-notation" class="nav-link" data-scroll-target="#matrix-notation">Matrix notation</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#hypothesis-testing" id="toc-hypothesis-testing" class="nav-link" data-scroll-target="#hypothesis-testing">Hypothesis Testing</a>
  <ul class="collapse">
  <li><a href="#wald-test" id="toc-wald-test" class="nav-link" data-scroll-target="#wald-test">Wald Test</a></li>
  <li><a href="#score-test" id="toc-score-test" class="nav-link" data-scroll-target="#score-test">Score Test</a></li>
  <li><a href="#likelihood-ratio-test" id="toc-likelihood-ratio-test" class="nav-link" data-scroll-target="#likelihood-ratio-test">Likelihood Ratio Test</a></li>
  </ul></li>
  <li><a href="#estimating-beta-newton-raphson-algorithm" id="toc-estimating-beta-newton-raphson-algorithm" class="nav-link" data-scroll-target="#estimating-beta-newton-raphson-algorithm">Estimating <span class="math inline">\(\beta\)</span>: Newton-Raphson Algorithm</a></li>
  <li><a href="#fitting-glms-in-r" id="toc-fitting-glms-in-r" class="nav-link" data-scroll-target="#fitting-glms-in-r">Fitting GLMs in R</a></li>
  <li><a href="#residuals" id="toc-residuals" class="nav-link" data-scroll-target="#residuals">Residuals</a></li>
  <li><a href="#intro-to-western-collaborative-group-study" id="toc-intro-to-western-collaborative-group-study" class="nav-link" data-scroll-target="#intro-to-western-collaborative-group-study">Intro to Western Collaborative Group Study</a>
  <ul class="collapse">
  <li><a href="#types-of-contrasts" id="toc-types-of-contrasts" class="nav-link" data-scroll-target="#types-of-contrasts">Types of Contrasts</a>
  <ul class="collapse">
  <li><a href="#log-likelihood-for-glms-with-binary-outcomes" id="toc-log-likelihood-for-glms-with-binary-outcomes" class="nav-link" data-scroll-target="#log-likelihood-for-glms-with-binary-outcomes">Log-likelihood for GLMs with binary outcomes</a></li>
  </ul></li>
  <li><a href="#score-and-information-for-glms-with-binary-outcomes" id="toc-score-and-information-for-glms-with-binary-outcomes" class="nav-link" data-scroll-target="#score-and-information-for-glms-with-binary-outcomes">Score and information for GLMs with Binary outcomes</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">Week 11</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Recap:</p>
<p>The score function for <span class="math inline">\(\beta_j\)</span> in a GLM can be written as</p>
<p><span class="math display">\[\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \frac{\partial \mu_i}{\partial \eta_i} \frac{x_{ij}}{V(\mu_i)a_i(\phi)} (y_i - \mu_i)\]</span></p>
<p>Which depends on the distribution <span class="math inline">\(Y_i\)</span> solely through <span class="math inline">\(\mathbb E[Y_i] = \mu_i\)</span> and <span class="math inline">\(\text{Var}[Y_i] = V(\mu_i)a_i(\phi)\)</span>.</p>
<p>Notice that the <span class="math inline">\((p+1)\)</span> score equations for <span class="math inline">\(\beta\)</span> do not depend on <span class="math inline">\(\phi\)</span>. Consequently, obtaining the MLE of <span class="math inline">\(\beta\)</span> doesn’t require knowledge of <span class="math inline">\(\phi\)</span>.</p>
<p>Inference does require an estimate of <span class="math inline">\(\phi\)</span>.</p>
<section id="asymptotic-sampling-distribution" class="level1">
<h1>Asymptotic Sampling Distribution</h1>
<p>Subject to appropriate regularity conditions,</p>
<p><span class="math display">\[\begin{bmatrix} \hat \beta_{MLE} \\ \hat \phi_{MLE} \end{bmatrix} \sim \text{MVN} \left(
    \begin{bmatrix} \beta \\ \phi \end{bmatrix}, \mathcal I_n (\beta, \phi)^{-1}
    \right).\]</span></p>
<p>Now we compute the components of <span class="math inline">\(\mathcal I_n(\beta, \phi):\)</span></p>
<p><span class="math display">\[\frac{\partial^2 \ell}{\partial \beta_j \partial \beta_k} = \frac{\partial}{\partial \beta_k} \underbrace{\left\{ \frac{\partial \mu_i}{\partial \eta_i} \frac{x_{ij}}{V(\mu_i) a_i(\phi)} (y_i - \mu_i) \right\}}_{\text{score for } \beta_j}.\]</span></p>
<p><span class="math display">\[ = (y_i - \mu_i) \frac{\partial}{\partial \beta_k} \left\{ \frac{\partial \mu_i}{\partial \eta_i} \frac{x_{ij}}{V(\mu_i) a_i(\phi)} \right\} - \left( \frac{\partial \mu_i}{\partial \eta_i} \right)^2 \frac{x_{ij} x_{ik}}{V(\mu_i) a_i(\phi)}.\]</span></p>
<p>Hence <span class="math display">\[-\mathbb E\left[\frac{\partial^2 \ell}{\partial \beta_j \partial \beta_k} \right] =
\sum_{i=1}^n \left( \frac{\partial \mu_i}{\partial \eta_i} \right)^2 \frac{x_{ij}x_{ik}}{V(\mu_i) a_i(\phi)}.\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial^2 \ell}{\partial \beta_j \partial \phi} &amp; =
\frac{\partial}{\partial \phi} \left\{
    \frac{\partial \mu_i}{\partial \eta_i} \frac{x_{ij}}{V(\mu_i)a_i(\phi)} (y_i - \mu_i)
\right\} \\
&amp; = - \frac{a_i'(\phi) \partial \mu_i x_{ij}}{a_i(\phi)^2 \partial \eta_i V(\mu_i)} (y_i - \mu_i)
\end{aligned}
\]</span></p>
<p>Thus</p>
<p><span class="math display">\[-E \left[ \frac{\partial^2 \ell}{\partial \beta_j \partial \phi} \right] = 0\]</span></p>
<p><span class="math display">\[\begin{aligned}\frac{\partial^2 \ell}{\partial \phi \partial \phi} &amp; = \frac{\partial}{\partial \phi}
\left\{
    - \frac{a_i'(\phi)}{a_i(\phi)^2} (y_i\theta_i - b(\theta_i)) + c'(y_i, \phi)
\right\} \\
&amp; = - \left\{ \frac{a_i(\phi)^2 a_i''(\phi) - 2a_i(\phi) a_i'(\phi)^2}{a_i(\phi)^4}\right\} [y_i \theta_i - b(\theta_i)] + c''(y_i, \phi) \\
&amp; = -K(\phi)[y_i\theta_i - b(\theta_i)] + c''(y_i, \phi)
\end{aligned}\]</span></p>
<p>and thus <span class="math display">\[-\mathbb E\left[ \frac{\partial^2 \ell}{\partial \phi \partial \phi} \right] =
\sum_{i=1}^n K(\phi)[b'(\theta_i)\theta_i - b(\theta_i)] - \mathbb E[c''(Y_i, \phi)].\]</span></p>
<section id="form-of-the-fisher-matrix" class="level2">
<h2 class="anchored" data-anchor-id="form-of-the-fisher-matrix">Form of the Fisher Matrix</h2>
<p>We can write the expected information matrix in block-diagonal form:</p>
<p><span class="math display">\[\mathcal I_n(\beta, \phi) = \begin{bmatrix}
    \mathcal I_{\beta \beta} &amp; 0 \\
    0 &amp; \mathcal I_{\phi \phi}
\end{bmatrix}.\]</span></p>
<p>The inverse of the information matrix is the asymptotic variance</p>
<p><span class="math display">\[\text{Var}[\hat \beta_{MLE}, \hat \phi_{MLE}] = \mathcal I_n(\beta, \phi)^{-1} =
\begin{bmatrix} \mathcal I^{-1}_{\beta \beta} &amp; 0 \\ 0 &amp; \mathcal I_{\phi \phi}^{-1} \end{bmatrix}.\]</span></p>
<p>The block diagonal structure of <span class="math inline">\(\text{Var}[\hat \beta_{MLE}, \hat \phi_{MLE}]\)</span> implies that for GLMs, valid characterization of the uncertainty in <span class="math inline">\(\hat \beta_{MLE}\)</span> does not require the propagation of uncertainty in the estimate of <span class="math inline">\(\phi\)</span>.</p>
<p>For example, for linear regression on Normally distributed outcomes, we plug in an estimate of <span class="math inline">\(\sigma^2\)</span> into <span class="math display">\[\text{Var}[\hat \beta_{MLE}] = \sigma^2 (X^T X)^{-1}\]</span> without worrying about uncertainty in estimation of <span class="math inline">\(\sigma^2\)</span>.</p>
<p>For GLMs, therefore, estimation of <span class="math inline">\(\text{Var}(\hat \beta_{MLE})\)</span> proceeds by plugging in the values of <span class="math inline">\((\hat \beta_{MLE}, \hat \phi)\)</span> into <span class="math inline">\(\mathcal I_{\beta \beta}^{-1}\)</span>, i.e., <span class="math display">\[\widehat{\text{Var}}[\hat \beta_{MLE}] = \hat{\mathcal{I}}_{\beta \beta}^{-1}\]</span> where <span class="math inline">\(\hat \phi\)</span> is <em>any</em> consistent estimator of <span class="math inline">\(\phi\)</span>.</p>
<section id="matrix-notation" class="level3">
<h3 class="anchored" data-anchor-id="matrix-notation">Matrix notation</h3>
<p>Recall that the <span class="math inline">\((j,k)^{\text{th}}\)</span> element of <span class="math inline">\(\mathcal I_{\beta \beta}\)</span> has the form</p>
<p><span class="math display">\[(\mathcal I_{\beta \beta})_{jk} = \sum_{i=1}^n \underbrace{\left( \frac{\partial \mu_i}{\partial \eta_i} \right)^2 \frac{1}{V(\mu_i) a_i(\phi)}}_{\coloneqq W_i} x_{ij}x_{jk},\]</span></p>
<p>so we can write</p>
<p><span class="math display">\[(\mathcal I_{\beta \beta})_{jk} = \sum_{i=1}^n W_i x_{ij}x_{jk}.\]</span></p>
<p><span class="math inline">\(W_i\)</span> sort of looks like the inverse of the variance of <span class="math inline">\(Y_i\)</span>.</p>
<p>We can therefore write:</p>
<p><span class="math display">\[\mathcal I_{\beta \beta} = X'WX \quad \text{ and } \quad \text{Var}(\hat \beta_{MLE}) = \mathcal I_{\beta \beta}^{-1} = (X'WX)^{-1},\]</span></p>
<p>where <span class="math inline">\(W = \text{diag}(W_1, ..., W_n)\)</span> and <span class="math inline">\(X\)</span> is the design matrix.</p>
<p>Note that <span class="math inline">\((X'WX)^{-1}\)</span> looks a lot like <span class="math inline">\(\text{Var}(\hat \beta_{GLS})\)</span>, just instead of <span class="math inline">\(W\)</span> we used <span class="math inline">\(\Sigma^{-1}\)</span>.</p>
</section>
</section>
</section>
<section id="hypothesis-testing" class="level1">
<h1>Hypothesis Testing</h1>
<p>For the linear predictor <span class="math inline">\(x_i' \beta\)</span>, suppose we partition <span class="math inline">\(\beta = (\beta_1, \beta_2)\)</span> and we are interested in testing:</p>
<p><span class="math display">\[H_0 : \beta_1 = \beta_{1,0} \quad \text{vs.} \quad H_a : \beta_1 \neq \beta_{1,0}\]</span></p>
<p>The length of <span class="math inline">\(\beta_1\)</span> is <span class="math inline">\(q \leq (p+1)\)</span> and <span class="math inline">\(\beta_2\)</span> is left arbitrary.</p>
<p>In most settings <span class="math inline">\(\beta_{1,0} = 0\)</span> which represents some form of ‘no effect’</p>
<p>Following our review of asymptotic theory, there are three common hypothesis testing frameworks: Wald, score, and likelihood ratio testing.</p>
<section id="wald-test" class="level2">
<h2 class="anchored" data-anchor-id="wald-test">Wald Test</h2>
<p>Let <span class="math inline">\(\hat \beta_{MLE} = (\hat \beta_{1,MLE}, \hat \beta_{MLE})\)</span>.</p>
<p>Under <span class="math inline">\(H_0\)</span></p>
<p><span class="math display">\[(\hat \beta_{1,MLE} - \beta_{1,0})' \widehat{\text{Var}}[\hat \beta_{1,MLE}]^{-1}(\hat\beta_{1,MLE} - \beta_{1,0}) \longrightarrow_d \chi^2_q\]</span></p>
<p>where <span class="math inline">\(\widehat{\text{Var}}[\hat \beta_{1,MLE}]\)</span> is the <span class="math inline">\(q\times q\)</span> sub-matrix of <span class="math inline">\(\mathcal{I}_{\beta\beta}^{-1}\)</span> corresponding to <span class="math inline">\(\beta_1\)</span>, evaluated at <span class="math inline">\(\left( \hat \beta_{MLE}, \hat \phi_{MLE} \right).\)</span></p>
<p>This is the multivariate analog of <span class="math inline">\(((\text{Est} - \text{Null})/\text{SE})^2\)</span>.</p>
<p>The Wald test is just looking at how far our estimate of <span class="math inline">\(\beta\)</span> is from the null and seeing if that distance is large relative to the uncertainty in our estimate.</p>
</section>
<section id="score-test" class="level2">
<h2 class="anchored" data-anchor-id="score-test">Score Test</h2>
<p>Let <span class="math inline">\(\hat \beta_{0,MLE} = (\beta_{1,0}, \hat \beta_{2,MLE})\)</span> and <span class="math inline">\(\hat \phi_{0,MLE}\)</span> denote the MLEs under <span class="math inline">\(H_0\)</span>.</p>
<p>Under <span class="math inline">\(H_0\)</span>,</p>
<p><span class="math display">\[U(\hat \beta_{0,MLE}, \hat \phi_{0,MLE}; y)' \mathcal I_n(\hat \beta_{0,MLE}; \hat \phi_{0, MLE})^{-1} U(\hat \beta_{0,MLE}, \hat \phi_{0,MLE}; y) \longrightarrow_d \chi^2_q.\]</span></p>
</section>
<section id="likelihood-ratio-test" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-ratio-test">Likelihood Ratio Test</h2>
<p>Obtain unrestricted MLEs: <span class="math inline">\((\hat \beta_{MLE}, \hat \phi_{MLE})\)</span>, and obtain MLEs under <span class="math inline">\(H_0:\)</span> <span class="math inline">\((\hat \beta_{0, MLE}, \hat \phi_{0,MLE})\)</span>.</p>
<p>Under <span class="math inline">\(H_0\)</span>,</p>
<p><span class="math display">\[2(\ell(\hat \beta_{MLE}, \hat \phi_{MLE}; y) - \ell(\hat \beta_{0,MLE}, \hat \phi_{0, MLE} ; y)) \longrightarrow_d \chi^2_q.\]</span></p>
</section>
</section>
<section id="estimating-beta-newton-raphson-algorithm" class="level1">
<h1>Estimating <span class="math inline">\(\beta\)</span>: Newton-Raphson Algorithm</h1>
<p>We saw that the score equation for <span class="math inline">\(\beta_j\)</span> is</p>
<p><span class="math display">\[\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \frac{\partial \mu_i}{\partial \eta_i} \frac{x_{ij}}{V(\mu_i) a_i(\phi)} (y_i - \mu_i) = 0\]</span></p>
<p>Estimation of <span class="math inline">\(\beta\)</span> requires solving <span class="math inline">\((p+1)\)</span> of these equations simultaneously. This is tricky because <span class="math inline">\(\beta\)</span> appears in several places (implicitly anywhere a <span class="math inline">\(\mu\)</span> appears).</p>
<p>No closed form solution in general is available (though there is one for linear settings), so we need an iterative method. A commonly used general purpose optimization routine is the Newton-Raphson algorithm.</p>
<p>General idea of Newton-Raphson and Fisher Scoring</p>
<p>General idea of Newton-Raphson</p>
<ol type="1">
<li>Approximate log-likelihood via 2nd order Taylor series</li>
<li>Take derivative of approximate log likelihood, set equal to zero, and solve</li>
<li>Leads to updates of the form</li>
</ol>
<p><span class="math display">\[\beta^{(r+1)} = \beta^{(r)} - \left[ \mathcal{I}_{\beta \beta}^{(r)}\right]^{-1} U^{(r)}_{\beta}\]</span></p>
<p>Fisher scoring is an adaptation of the Newton-Raphson algorithm that uses the expected information, <span class="math inline">\(\mathcal I_{\beta \beta}\)</span>, rather than observed information <span class="math inline">\(\mathcal I_{\beta \beta}\)</span> for the update.</p>
<p>There’s a close relationship between Fisher scoring and IRLS. Often this is exploited for computation in software packages, many of which use IRLS by default (including R’s <code>glm()</code>).</p>
<p>The idea:</p>
<p>Define</p>
<p><span class="math display">\[Z_i = \overbrace{g(\mu_i) + (Y_i - \mu_i)g'(\mu_i)}^{\text{First order Taylor approx of }g(Y_i)}\]</span></p>
<p>to be an “adjusted response variable” to show that</p>
<p><span class="math display">\[\text{Var}(Z_i) = (W_i)^{-1}, \text{ where }W_i = \left( \frac{\partial \mu_i}{\partial \eta_i} \right)^2 \frac{1}{V(\mu_i)a_i(\phi)}\]</span></p>
<p>Then note that</p>
<p><span class="math display">\[\mathbb E[Z_i] = x_i'\beta\]</span></p>
<p>so we can consider a linear model for <span class="math inline">\(Z\)</span> to estimate <span class="math inline">\(\beta\)</span>, i.e., using the GLS estimator <span class="math inline">\(\hat \beta_{GLS} = (X'WX)^{-1} X'WZ\)</span>.</p>
<p>Two immediate challenges to estimating <span class="math inline">\(\hat \beta_{GLS}\)</span> in practice are</p>
<ol type="1">
<li>We don’t observe <span class="math inline">\(Z\)</span></li>
<li><span class="math inline">\(W\)</span> depends on <span class="math inline">\(\beta\)</span></li>
</ol>
<p>However, we can use IRLS to do estimation.</p>
<p>Suppose the current estimate of <span class="math inline">\(\beta\)</span> is <span class="math inline">\(\hat \beta^{(r)}\)</span>. Compute</p>
<p><span class="math display">\[\eta_i^{(r)} = x_i'\hat \beta^{(r)}\]</span> <span class="math display">\[\mu_i^{(r)} = g^{-1} (\eta_i^{(r)})\]</span> <span class="math display">\[W_i^{(r)} = \left( \frac{\partial \mu_i}{\partial \eta_i} \biggr \mid_{\eta_i^{(r)}} \right)^2 \frac{1}{V(\mu_i^{(r)})}\]</span> <span class="math display">\[z_i^{(r)} = \eta_i^{(r)} + (y_i - \mu_i^{(r)}) \frac{\partial \eta_i}{\partial \mu_i} \mid_{\mu_i^{(r)}}\]</span></p>
<p><span class="math inline">\(W_i\)</span> is called the ‘working weight’</p>
<p>Check the McCullagh and Nelder (1989) book which shows that using IRLS for estimation here is equivalent to Fisher scoring. For more detail, see Agresti 4.6.3 or <a href="https://grodri.github.io/glms/notes/" class="uri">https://grodri.github.io/glms/notes/</a></p>
<p>The updated version of <span class="math inline">\(\hat \beta\)</span> is obtained as the WLS estimate to the regression of <span class="math inline">\(Z\)</span> on <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[\hat \beta^{(r+1)} = (X' W^{(r)} X)^{-1} (X' W^{(r)} Z^{(r)})\]</span></p>
<p><span class="math inline">\(X\)</span> is the <span class="math inline">\(n \times (p+1)\)</span> design matrix from the initial specification of the model. <span class="math inline">\(W^{(r)}\)</span> is the diagonal <span class="math inline">\(n \times n\)</span> matrix with entries <span class="math inline">\(\{ W_1^{(r)}, ..., W_n^{(r)}}\)</span>. <span class="math inline">\(Z^{(r)}\)</span> is the <span class="math inline">\(n\)</span>-vector <span class="math inline">\((z_1^{(r)}, ..., z_n^{(r)})\)</span>.</p>
<p>Iterate until the <span class="math inline">\(\hat \beta\)</span> values converges.</p>
</section>
<section id="fitting-glms-in-r" class="level1">
<h1>Fitting GLMs in R</h1>
<p>A generic call to <code>glm()</code> is given by</p>
<p><code>fit0 &lt;- glm(formula, family, data, ...)</code></p>
<p><code>formula</code> specifies the structure of the linear predictor <span class="math inline">\(\eta_i = x_i' \beta\)</span>.</p>
<p><code>family</code> jointly specifies the probability distribution <span class="math inline">\(f_Y()\)</span>, link function <span class="math inline">\(g()\)</span> and variance function <span class="math inline">\(V()\)</span>.</p>
<p>E.g.,</p>
<p><code>glm(Y ~ X_1 + X2, family = binomial(), data = df)</code></p>
<p>Most common family objects are <code>binomial()</code> or <code>poisson()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>myFamily <span class="ot">&lt;-</span> <span class="fu">binomial</span>()</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(myFamily)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "family"     "link"       "linkfun"    "linkinv"    "variance"  
 [6] "dev.resids" "aic"        "mu.eta"     "initialize" "validmu"   
[11] "valideta"   "simulate"   "dispersion"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>myFamily<span class="sc">$</span>link</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "logit"</code></pre>
</div>
</div>
<p>The <code>residuals</code> inside a <code>glm()</code> are called working residuals and used internally for iteratively reweighted least squares.</p>
</section>
<section id="residuals" class="level1">
<h1>Residuals</h1>
<p>In linear models, we used residual diagnostics to examine the adequacy of model fit and investigate potential data issues such as outliers. Adequacy of model fit included functional form for terms in the linear predictor and the homoscedasticity assumption.</p>
<p>In GLMs, residual diagnostics are much more complex and visual inspection of residual plots is typically less informative.</p>
<p>In general, residuals are meant to represent variation in the outcome that is not explained by the model.</p>
<p>Variation after the systematic component is accounted for, and therefore residuals are therefore model-specific.</p>
<div class="bluetip">
<p>In linear models, residuals were considered as empirical estimates of the true error, e.g., <span class="math inline">\(\hat \varepsilon \coloneqq Y_i - \hat Y_i\)</span>.</p>
<p>Raw residuals are not generally useful in the GLM setting.</p>
<p>For example, with a binary predictor and a binary outcome, there’s only four possible values the raw residuals could take on.</p>
<p>Another issue is heteroscedasticity — because many of the families fit with GLMs there is a mean-variance relationship.</p>
</div>
<p><em>Pearson residuals</em> account for the heteroscedasticity via standardization.</p>
<p><span class="math display">\[r_i^p = \frac{y_i - \hat \mu_i}{\sqrt{\text{Var}(Y_i)}}\]</span></p>
<p>We’ll see that Pearson residuals can be useful for diagnosing departures from our model’s variance assumption.</p>
<p>The <em>deviance residual</em> (sometimes called the “preferred residual”) is defined as</p>
<p><span class="math display">\[r_i^d = \text{sign}(y_i - \hat \mu_i)\sqrt{d_i}\]</span></p>
<p>where, letting <span class="math inline">\(\ell(\hat \mu_i, \phi; y_i)\)</span> be the log-likelihood contribution of unit <span class="math inline">\(i\)</span> evaluated as <span class="math inline">\(\hat \mu_i\)</span> and <span class="math inline">\(\phi\)</span>, then <span class="math display">\[d_i = 2 \phi(\ell(y_i, \phi; y_i) - \ell(\hat \mu_i, \phi; y_i))\]</span></p>
<p>We can think about this as the distance between <span class="math inline">\(\hat \mu_i\)</span> and <span class="math inline">\(y_i\)</span> on the log-likelihood scale.</p>
<p>Pierce and Shafer (JASA, 1986) examined various residuals for GLMs. <a href="https://www.jstor.org/stable/2289071" class="uri">https://www.jstor.org/stable/2289071</a></p>
<p>One can get both deviance and Pearson residuals from <code>glm()</code> in R:</p>
<pre><code>fit &lt;- glm(Y ~ X, family = binomial)

residuals(fit)

residuals(fit, type='pearson')</code></pre>
</section>
<section id="intro-to-western-collaborative-group-study" class="level1">
<h1>Intro to Western Collaborative Group Study</h1>
<p>A prospective study of coronary heart disease (CHD) with <span class="math inline">\(n = 3,154\)</span> male participants aged 39-54 at risk for CHD, employed at 10 companies in California with a baseline survey and measurements taken at intake (1960-61) along with annual surveys until December 1969 to assess incident CHD.</p>
<p>Our primary goal is to investigate the relationship between ‘behavior pattern’ and risk of CHD.</p>
<p>Participants were categorized into one of two behavior pattern groups.</p>
<p>Type A: characterized by enhanced aggressiveness, ambitiousness, competitive drive, and chronic sense of urgency.</p>
<p>Type B: characterized by more relaxed and non-competitive.</p>
<p>For now, we will ignore any loss to follow-up and consider the binary outcome of</p>
<p><span class="math display">\[Y = \left\{ \begin{array}
1 \quad &amp; \text{occurrence of CHD during follow-up} \\
0 &amp; \text{otherwise}
\end{array} \right. \]</span></p>
<p>8.1% of participants developed CHD during follow-up, while 50.4 of participants were classified as Type A at baseline.</p>
<p><span class="math display">\[\hat P(\text{CHD} = 1 \mid \text{Type} = A) = 0.112\]</span> <span class="math display">\[\hat P(\text{CHD} = 1 \mid \text{Type} = B) = 0.050\]</span></p>
<p>I.e., 11.2% of Type A men and 5% of Type B men developed CHD during follow-up.</p>
<p>Often we will use the generic term ‘risk,’ e.g., “risk of CHD” rather than probability of CHD during follow-up.</p>
<section id="types-of-contrasts" class="level2">
<h2 class="anchored" data-anchor-id="types-of-contrasts">Types of Contrasts</h2>
<p>We need to pick a contrast —</p>
<p>Risk difference:</p>
<p><span class="math display">\[RD = P(Y = 1 \mid x = 1) - P(Y = 1 \mid x = 0)\]</span> <span class="math display">\[\widehat{RD} = \hat P(Y = 1 \mid x = 1) - \hat P(Y=1 \mid x = 0) = .112 - .050 = .062\]</span></p>
<p>The difference in the estimated risk of CHD during follow-up between type A and type B men is 0.062 (or 6.2%). This characterizes the way in which the additional risk of CHD of being a type A person manifests through an <em>absolute</em> increase.</p>
<p>We’ve already seen the OR as a relative-scale contrast. A more interpretable option is the relative risk:</p>
<p><span class="math display">\[RR = \frac{P(Y = 1 \mid x = 1)}{P(Y = 1 \mid x = 0)}\]</span></p>
<p><span class="math display">\[\widehat{RR} = \frac{\hat P(Y = 1 \mid x = 1)}{\hat P(Y = 1 \mid x = 0)} = \frac{0.112}{0.050} = 2.24 \]</span></p>
<section id="log-likelihood-for-glms-with-binary-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="log-likelihood-for-glms-with-binary-outcomes">Log-likelihood for GLMs with binary outcomes</h3>
<p><span class="math display">\[\ell(\beta; y) = \sum_{i=1}^n y_i \theta_i - b(\theta_i)\]</span></p>
<p><span class="math display">\[ = \sum_{i=1}^n y_i \theta_i - \log(1 + \exp\{\theta_i\})\]</span></p>
<p>Where <span class="math inline">\(\theta_i\)</span> is a function of <span class="math inline">\(\beta\)</span> via</p>
<p><span class="math display">\[\mu_i = b'(\theta_i) = \frac{\exp \theta_i}{1 + \exp \theta_i}\]</span></p>
<p>and</p>
<p><span class="math display">\[\mu_i = g^{-1}(x_i'\beta)\]</span></p>
</section>
</section>
<section id="score-and-information-for-glms-with-binary-outcomes" class="level2">
<h2 class="anchored" data-anchor-id="score-and-information-for-glms-with-binary-outcomes">Score and information for GLMs with Binary outcomes</h2>
<p>The score function for <span class="math inline">\(\beta_j\)</span> is</p>
<p><span class="math display">\[\frac{\partial \ell }{\partial \beta_j} = \sum_{i=1}^n \frac{\partial \mu_i}{\partial \eta_i} \frac{x_{ij}}{\mu_i (1-\mu_i)} (y_i - \mu_i)\]</span></p>
<p>where the expression for <span class="math inline">\(\partial \mu_i / \partial \eta_i\)</span> depends on the choice of <span class="math inline">\(g()\)</span>.</p>
<p>Since <span class="math inline">\(\phi\)</span> is fixed and known, the expected information matrix is just <span class="math display">\[\mathcal I_{\beta \beta} = X' W X\]</span></p>
<p>where <span class="math inline">\(X\)</span> is the design matrix for the model and <span class="math inline">\(W\)</span> is a diagonal matrix with <span class="math inline">\(i^{\text{th}}\)</span> diagonal element</p>
<p><span class="math display">\[W_i = \left( \frac{\partial \mu_i}{\partial \eta_i} \right)^2 \frac{1}{\mu_i (1-\mu_i)},\]</span></p>
<p>which we’ve said previously looks like an inverse variance of <span class="math inline">\(Y\)</span>.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../week10/week10.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Week 10</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../week12/week12.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Week 12</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>