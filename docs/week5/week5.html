<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Methods (BST 232) Notes - 6&nbsp; Week 5</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../week6/week6.html" rel="next">
<link href="../week4/week4.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-title">Week 5</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Methods (BST 232) Notes</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Methods (BST 232) Notes</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week1/week1.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 1</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week2/week2.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 2</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week3/week3.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 3</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week4/week4.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 4</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week5/week5.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Week 5</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week6/week6.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 6</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week7/week7.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 7</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week8/week8.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 8</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week9/week9.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 9</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week10/week10.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 10</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#model-selection-substantive-statistical-and-predictive-criteria" id="toc-model-selection-substantive-statistical-and-predictive-criteria" class="nav-link active" data-scroll-target="#model-selection-substantive-statistical-and-predictive-criteria">Model Selection: Substantive, Statistical, and Predictive Criteria</a>
  <ul class="collapse">
  <li><a href="#hierarchically-well-formulated-models" id="toc-hierarchically-well-formulated-models" class="nav-link" data-scroll-target="#hierarchically-well-formulated-models">Hierarchically Well Formulated Models</a></li>
  <li><a href="#causal-selection" id="toc-causal-selection" class="nav-link" data-scroll-target="#causal-selection">Causal Selection</a></li>
  <li><a href="#statistical-criteria" id="toc-statistical-criteria" class="nav-link" data-scroll-target="#statistical-criteria">Statistical Criteria</a>
  <ul class="collapse">
  <li><a href="#akaike-information-criteria" id="toc-akaike-information-criteria" class="nav-link" data-scroll-target="#akaike-information-criteria">Akaike Information Criteria</a></li>
  <li><a href="#best-subsets-selection" id="toc-best-subsets-selection" class="nav-link" data-scroll-target="#best-subsets-selection">Best subsets selection</a></li>
  </ul></li>
  <li><a href="#predictive-performance-train-vs.-test-error" id="toc-predictive-performance-train-vs.-test-error" class="nav-link" data-scroll-target="#predictive-performance-train-vs.-test-error">Predictive Performance: Train vs.&nbsp;Test Error</a>
  <ul class="collapse">
  <li><a href="#model-selection-using-test-error" id="toc-model-selection-using-test-error" class="nav-link" data-scroll-target="#model-selection-using-test-error">Model Selection Using Test Error</a></li>
  <li><a href="#k-fold-validation" id="toc-k-fold-validation" class="nav-link" data-scroll-target="#k-fold-validation">K-Fold Validation</a></li>
  <li><a href="#n-fold-validation-and-the-press-statistic" id="toc-n-fold-validation-and-the-press-statistic" class="nav-link" data-scroll-target="#n-fold-validation-and-the-press-statistic">n-fold Validation and the PRESS Statistic</a></li>
  </ul></li>
  <li><a href="#bias-variance-tradeoff" id="toc-bias-variance-tradeoff" class="nav-link" data-scroll-target="#bias-variance-tradeoff">Bias-Variance Tradeoff</a></li>
  <li><a href="#penalized-regression-methods-for-variable-selection" id="toc-penalized-regression-methods-for-variable-selection" class="nav-link" data-scroll-target="#penalized-regression-methods-for-variable-selection">Penalized Regression Methods for Variable Selection</a></li>
  </ul></li>
  <li><a href="#ridge-regression" id="toc-ridge-regression" class="nav-link" data-scroll-target="#ridge-regression">Ridge Regression</a></li>
  <li><a href="#lab-4" id="toc-lab-4" class="nav-link" data-scroll-target="#lab-4">Lab 4</a>
  <ul class="collapse">
  <li><a href="#bias-variance-tradeoff-1" id="toc-bias-variance-tradeoff-1" class="nav-link" data-scroll-target="#bias-variance-tradeoff-1">Bias-Variance Tradeoff</a></li>
  <li><a href="#penalized-regression" id="toc-penalized-regression" class="nav-link" data-scroll-target="#penalized-regression">Penalized Regression</a>
  <ul class="collapse">
  <li><a href="#ridge-regression-application" id="toc-ridge-regression-application" class="nav-link" data-scroll-target="#ridge-regression-application">Ridge Regression Application</a></li>
  <li><a href="#lasso-application" id="toc-lasso-application" class="nav-link" data-scroll-target="#lasso-application">LASSO Application</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">Week 5</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="model-selection-substantive-statistical-and-predictive-criteria" class="level1">
<h1>Model Selection: Substantive, Statistical, and Predictive Criteria</h1>
<p>Main reading for this week is Sections 5.1, 6.1-6.2 from <a href="https://trevorhastie.github.io/ISLR/"><em>Introduction to Statistical Learning</em></a>.</p>
<p>So far we have assumed that we know what variables should be included in a regression model. We’ve focused on specification and interpretation of the linear regression model and regression diagnostics for testing if we have the correct functional form or verifying the underlying assumptions.</p>
<p>However, we might not be sure which variables should be included in the model to begin with.</p>
<p>Model selection will be dependent on the study design and objectives.</p>
<p>Interest could be on:</p>
<ul>
<li>The association between an outcome and some predictor(s), where we would want to make sure that:
<ul>
<li>The estimates are not impacted by confounding.</li>
<li>We are not oversimplifying these associations by ignoring important effect modification.</li>
<li>We have a parsimonious model for interpretability.
<ul>
<li><span class="vocab">Parsimony</span> refers to the principle of preferring the less complex model that performs just as well to a more complex model.</li>
</ul></li>
</ul></li>
<li>If our goal is prediction, we would want to make sure that
<ul>
<li>We have identified the important predictors of the outcome.</li>
<li>The selected set of regressors minimizes prediction error.</li>
<li>Less interested in causal inferences.</li>
<li>And to be careful of overfitting</li>
</ul></li>
</ul>
<p>In all situations, it is important that the model be as parsimonious as possible:</p>
<ul>
<li>Interpretation becomes more complex for larger models;</li>
<li>Unnecessary variables may increase the variability of <span class="math inline">\(\hat \beta\)</span> because they waste degrees of freedom without increasing SSR or decreasing SSE.
<ul>
<li>This is because <span class="math inline">\(\widehat{\text{Var}}(\hat \beta) = MSE(X'X)^{-1}\)</span> and <span class="math inline">\(MSE = SSE/(n-p-1)\)</span>, so adding useless variables will decrease the denominator while leaving the SSE unchanged, thus inflating the MSE.</li>
</ul></li>
<li>Multicollinearity can cause problems in estimation.</li>
</ul>
<p>However, we don’t want the model to be so overly simplistic that it yields biased estimates.</p>
<section id="hierarchically-well-formulated-models" class="level3">
<h3 class="anchored" data-anchor-id="hierarchically-well-formulated-models">Hierarchically Well Formulated Models</h3>
<p>We should always consider <span class="vocab">hierarchically well formulated models</span>.</p>
<p>A model is said to be hierarchically well formulated when all lower order components of any term are included in a model.</p>
<p>That is, all main effects should be included in models containing two-way interactions.</p>
<p>Similarly, the appropriate two-way interactions need to be included in models which contain three-way interactions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.2     ✔ readr     2.1.4
✔ forcats   1.0.0     ✔ stringr   1.5.0
✔ ggplot2   3.4.2     ✔ tibble    3.2.1
✔ lubridate 1.9.2     ✔ tidyr     1.3.0
✔ purrr     1.0.1     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> .<span class="dv">5</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">2</span>) </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> x2<span class="sc">*</span><span class="dv">2</span> <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">sd =</span> .<span class="dv">25</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x1 =</span> x1, <span class="at">x2 =</span> <span class="fu">factor</span>(x2), <span class="at">y =</span> y),</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">x =</span> x1,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>         <span class="at">y =</span> y,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>         <span class="at">color =</span> x2,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>         <span class="at">shape =</span> x2</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>       )) <span class="sc">+</span> </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">'lm'</span>, <span class="fu">aes</span>(<span class="at">group =</span> <span class="dv">1</span>)) <span class="sc">+</span> </span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"A Model with Inappropriate Hierarchical Structured"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`geom_smooth()` using formula = 'y ~ x'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: The following aesthetics were dropped during statistical transformation:
colour, shape
ℹ This can happen when ggplot fails to infer the correct grouping structure in
  the data.
ℹ Did you forget to specify a `group` aesthetic or to convert a numerical
  variable into a factor?</code></pre>
</div>
<div class="cell-output-display">
<p><img src="week5_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> x1, <span class="at">x2 =</span> <span class="fu">factor</span>(x2), <span class="at">y =</span> y)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">x =</span> x1,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">y =</span> y,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>         <span class="at">color =</span> x2,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">shape =</span> x2</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>       )) <span class="sc">+</span> </span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">data =</span> df, </span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">'lm'</span>,  </span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>                <span class="at">formula =</span> y <span class="sc">~</span> <span class="dv">0</span> <span class="sc">+</span> x) <span class="sc">+</span> </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span> </span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Another Model with Inappropriate Hierarchical Structured"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="week5_files/figure-html/unnamed-chunk-1-2.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="causal-selection" class="level2">
<h2 class="anchored" data-anchor-id="causal-selection">Causal Selection</h2>
<p>Ideally, the primary piece of model building and model selection (choosing what terms to include) should be substantive knowledge.</p>
<p>On a causal DAG, an association between two variables (exposure/treatment) and Y (outcome) can arise in 3 ways:</p>
<ol type="1">
<li><span class="math inline">\(A\)</span> causes <span class="math inline">\(Y\)</span>:</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="standalone_figures/direct/direct.svg" class="img-fluid figure-img" style="width:55.0%"></p>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Confounding: Common causes of variables considered in the model that are not conditioned on.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="standalone_figures/confounder/confounder.svg" class="img-fluid figure-img" style="width:55.0%"></p>
</figure>
</div>
</div>
</div>
<ol start="3" type="1">
<li>Collider: Variables that have a common effect on another variable which is conditioned on.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="standalone_figures/collider/collider.svg" class="img-fluid figure-img" style="width:55.0%"></p>
</figure>
</div>
</div>
</div>
<p>In the case of confounders, we <em>should</em> adjust for them. On the other hand, we should <em>not</em> adjust for colliders (which creates collider stratification bias).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="standalone_figures/confounder/controlled_confounder/controlled_confounder.svg" class="img-fluid figure-img" style="width:55.0%"></p>
</figure>
</div>
</div>
</div>
<p>Another scenario we might be interested in is <span class="vocab">mediation</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="standalone_figures/mediation/mediation.svg" class="img-fluid figure-img" style="width:55.0%"></p>
</figure>
</div>
</div>
</div>
<p>Conditioning on a mediator will attenuate the observed effects.</p>
<p>If one fits a model like <span class="math inline">\(Y \sim A + M\)</span> the coefficient on <span class="math inline">\(A\)</span> can be interpreted as the direct effect of <span class="math inline">\(A\)</span> on <span class="math inline">\(Y\)</span> not through <span class="math inline">\(M\)</span>.</p>
</section>
<section id="statistical-criteria" class="level2">
<h2 class="anchored" data-anchor-id="statistical-criteria">Statistical Criteria</h2>
<p>Some model selection criteria we might use are <span class="math inline">\(R^2\)</span>, Adjusted <span class="math inline">\(R^2\)</span>, or the Akaike Information Criterion (AIC).</p>
<p>Recall that <span class="math display">\[R^2 \stackrel{def}{=} \frac{SSR}{SST} = 1 - \frac{SSE}{SST}.\]</span></p>
<p>The SSE will never go down as we add more predictors. More particularly, the SSE is exactly equivalent to the measure we try to minimize with respect to <span class="math inline">\(\beta\)</span> (<span class="math inline">\(S(\beta)\)</span>). As we add more degrees of freedom (or from a linear algebra perspective, adding more orthogonal vectors to the span of our prediction space), we are able to more closely fit the <span class="math inline">\(y\)</span> values. Or in other words, if we add another predictor to the regression that adds no additional explanatory value, then the <span class="math inline">\(\hat \beta\)</span> coefficient on that term can be set to 0 and we won’t increase <span class="math inline">\(S(\beta)\)</span> at all.</p>
<p>Thus <span class="math inline">\(R^2\)</span> has the undesirable property that adding more predictors will never decrease it.</p>
<p>Recall that adjusted <span class="math inline">\(R^2\)</span> is</p>
<p><span class="math display">\[R^2_{adj} = 1 - \frac{MSE}{SST/(n-1)},\]</span></p>
<p>where using MSE instead of the SSE penalizes the addition of predictors that don’t improve model performance.</p>
<section id="akaike-information-criteria" class="level3">
<h3 class="anchored" data-anchor-id="akaike-information-criteria">Akaike Information Criteria</h3>
<p>AIC is a general variable selection criterion defined for any likelihood-based model. For a model with parameters <span class="math inline">\(\theta \in \mathbb R^d\)</span> and log-likelihood <span class="math inline">\(\ell (\theta)\)</span></p>
<p><span class="math display">\[\begin{aligned}AIC &amp; = -2 \left(\ell(\hat \theta_{MLE}) - d\right) \\
&amp; = -2 \ell (\hat \theta_{MLE}) + 2d \end{aligned}\]</span></p>
<p><span class="math inline">\(\ell(\hat \theta)\)</span> captures goodness of fit on the observed sample.</p>
<p><span class="math inline">\(d\)</span> captures model complexity.</p>
<p>Lower AIC is preferred, thus AIC penalizes models with large numbers of parameters (high model complexity) by adding <span class="math inline">\(2d\)</span>.</p>
<p>For a regression model with <span class="math inline">\(n\)</span> observations and normally distributed errors, the log-likelihood is</p>
<p><span class="math display">\[\ell(\beta, \sigma^2 ; y) = c - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2}
\sum_{i=1}^n (y_i - x_i' \beta)^2.\]</span></p>
<p>Plugging in the MLEs, we have that</p>
<p><span class="math display">\[\ell(\hat \beta, \hat \sigma^2_{MLE}; y) = c - \frac{n}{2} \log(\hat \sigma^2_{MLE}) - \frac{n}{2}\]</span> subtracting off the number of parameters <span class="math inline">\((p+1)\)</span>, and multiplying by <span class="math inline">\(-2\)</span>, we have that</p>
<p><span class="math display">\[AIC = n \log(SSE/n) + 2(p+1) + c.\]</span></p>
<p>From a set of candidate models, we could then select the model that leads to the lowest AIC.</p>
</section>
<section id="best-subsets-selection" class="level3">
<h3 class="anchored" data-anchor-id="best-subsets-selection">Best subsets selection</h3>
<p>We could use the best subset selection algorithm to fit separate models for each possible combination of the <span class="math inline">\(p\)</span> predictors and try to pick the best.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>Best subsets selection algorithm</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="op">--------------------------------</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>For j <span class="op">=</span> <span class="dv">1</span><span class="op">,...,</span>p</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="op">(</span>a<span class="op">)</span> Fit all <span class="op">(</span>p choose j<span class="op">)</span> models containing exactly j predictors<span class="op">.</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="op">(</span>b<span class="op">)</span> Pick the best model from <span class="kw">this</span> set<span class="op">,</span> i<span class="op">.</span>e<span class="op">.,</span> the model with the highest R<span class="op">^</span><span class="dv">2</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>      <span class="kw">or</span> lowest SSE<span class="op">.</span> Call it M_j<span class="op">.</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>End</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>Among M_1<span class="op">,...,</span>M_p<span class="op">,</span> select the single best model as the one with the highest</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>adjusted R<span class="op">^</span><span class="dv">2</span> <span class="kw">or</span> lowest AIC<span class="op">.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Exhaustive search for best subsets can be performed in R using the <code>{leaps}</code> package with the function <code>leaps::regsubsets</code>.</p>
<p>In addition to considering all model subsets, people have traditionally used automated algorithms for model building. Most commonly these include forward selection, backward elimination, stepwise selection.</p>
<p>These typically inflate Type I error rates by doing tons and tons of hypothesis tests.</p>
<p>For a discussion of these, see Harrell F.E. (2015) <em>Regression Modeling Strategies</em>.</p>
</section>
</section>
<section id="predictive-performance-train-vs.-test-error" class="level2">
<h2 class="anchored" data-anchor-id="predictive-performance-train-vs.-test-error">Predictive Performance: Train vs.&nbsp;Test Error</h2>
<p>In machine learning, model selection is usually aimed at obtaining good predictive performance (small amount of error in predictions).</p>
<p>Very flexible/complex models are common (not usually worried about interpretation!), so overfitting is a key concern.</p>
<p>They usually focus on a variant of the MSE defined as</p>
<p><span class="math display">\[MSE^* = \frac{1}{n} \sum_{i=1}^n (y_i - \hat y_i)^2 = SSE/n\]</span></p>
<p>When computed on the sample used to fit the model (or “training data”), by design MSE* always decreases as more predictors are added in linear models.</p>
<p>Thus we should evaluate and compare <span class="math inline">\(MSE^*\)</span> of models when applied to data that is independent from the training data (“test data”).</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/MSE.png" class="img-fluid figure-img" width="1127"></p>
<p></p><figcaption class="figure-caption">Figure 2.9 from the Introduction to Statistical Learning</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The black line in the left panel shows the true mean <span class="math inline">\(\mathbb E[Y|X]\)</span>. A linear model (orange), a spline model (blue) and a wildly wiggly model (green) are fit. On the right-hand-side, we see that the more complex models overfit the data and start to diverge in the test-data <span class="math inline">\(MSE^*\)</span>.</p>
<p>Because the data were generated, we know the “true” minimum possible MSE in the test dataset, indicated in the dashed line at <span class="math inline">\(Y=1\)</span>.</p>
<section id="model-selection-using-test-error" class="level3">
<h3 class="anchored" data-anchor-id="model-selection-using-test-error">Model Selection Using Test Error</h3>
<p>Suppose we have fitted candidate model(s) on the training data sample and collected new data. Let <span class="math inline">\(m = 1, ..., M\)</span> index the observations in the test set and <span class="math inline">\(y_m^{new}\)</span> and <span class="math inline">\(\hat y_m^{new}\)</span> be their observed outcomes and predicted outcomes from a given model fit on the training data, respectively.</p>
<p>We define the train and test <span class="math inline">\(MSE^*\)</span> as</p>
<p><span class="math display">\[MSE_{train}^* = \frac{1}{n} \sum_{i=1}^n (y_i - \hat y_i)^2\]</span> <span class="math display">\[MSE_{test}^* = \frac{1}{n} \sum_{i=1}^n (y_{new} - \hat y_{new})^2\]</span></p>
<p>Usually we don’t have the ability to collect entirely new data to evaluate our models. A simple way to estimate the test error in this setting is to randomly split your one observed dataset into a training and testing and then use the same procedures as in the previous slide.</p>
<p>Disadvantages:</p>
<ul>
<li>You lose samples from the training data.</li>
<li>Results could depend heavily on the choice of points held out of the model.</li>
</ul>
<div class="hottip">
<p>Questions:</p>
<ul>
<li>How, in general, should one choose what data to hold out? Should it match the prediction problem, somehow?</li>
<li>Should one use test/train splitting to validate choice of model and then report on a model trained on the entire dataset?</li>
</ul>
</div>
</section>
<section id="k-fold-validation" class="level3">
<h3 class="anchored" data-anchor-id="k-fold-validation">K-Fold Validation</h3>
<p>Divide the observations into <span class="math inline">\(K\)</span> groups (“folds”) of roughly equal size.</p>
<p>Make <span class="math inline">\(K\)</span> passes, where in pass <span class="math inline">\(k = 1, ..., K\)</span>, fold <span class="math inline">\(k\)</span> is treated as a testing set and the rest is a training set.</p>
<p>Estimate the <span class="math inline">\(MSE_{testcv}^*\)</span> as the average of the <span class="math inline">\(K\)</span> estimates of <span class="math inline">\(MSE_{test}^*\)</span> from each pass and compare <span class="math inline">\(MSE_{testcv}^*\)</span> across candidate models.</p>
<div class="hottip">
<p>Should one calculate dispersion (variance) metrics across the <span class="math inline">\(MSE_{testcv, k}^*\)</span> measures? Because one could imagine that one may not want a model that has low <span class="math inline">\(\bar{MSE_{testcv}^*}\)</span> but occasionally performs very poorly. This is essentially getting my wondering about <em>reliability</em>. Rachel’s suggestion is to measure the “MSE of the MSE” — here meaning the average squared deviation of the <span class="math inline">\(MSE^*_{k, testcv}\)</span> from <span class="math inline">\(\bar{MSE_{k, testcv}^*}\)</span>.</p>
</div>
<p>Typical numbers for <span class="math inline">\(K\)</span> are 5 or 10. One important special case is when <span class="math inline">\(K = n\)</span>, leave-one-out cross-validation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># using a tidymodels approach to k-fold cross-validation: </span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># https://www.tidymodels.org/start/resampling/ </span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(here)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>hers <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(here<span class="sc">::</span><span class="fu">here</span>(<span class="st">"data/hers.csv"</span>))</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>hers_split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(hers)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>hers_train <span class="ot">&lt;-</span> <span class="fu">training</span>(hers_split)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>hers_test  <span class="ot">&lt;-</span> <span class="fu">testing</span>(hers_split)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>rf_mod <span class="ot">&lt;-</span> </span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rand_forest</span>(<span class="at">trees =</span> <span class="dv">1000</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"ranger"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"prediction"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="n-fold-validation-and-the-press-statistic" class="level3">
<h3 class="anchored" data-anchor-id="n-fold-validation-and-the-press-statistic">n-fold Validation and the PRESS Statistic</h3>
<p>Clearly the DFFITS, Cook’s Distance, and jackknife residuals are related to <span class="math inline">\(n\)</span>-fold cross validation (aka leave-one-out cross-validation).</p>
<p>The PRESS Statistic is defined</p>
<p><span class="math display">\[PRESS = \sum_{i=1}^n \left(Y_i - \underbrace{\hat Y_{i(-i)}}_{\substack{\text{prediction from a model} \\ \text{that doesn't include obs. } i}}\right)^2\]</span></p>
<p>PRESS is just <span class="math inline">\(n \times MSE_{testcv}^*\)</span> from leave-one-out cross-validation.</p>
<p>As we have seen from other “delete-one” diagnostics, this can be computed from the original fit in linear models.</p>
<p>One can use the <code>PRESS()</code> function from the <code>{MPV}</code> package, which will compute the <span class="math inline">\(MSE\)</span> from n-fold cross-validation.</p>
</section>
</section>
<section id="bias-variance-tradeoff" class="level2">
<h2 class="anchored" data-anchor-id="bias-variance-tradeoff">Bias-Variance Tradeoff</h2>
<p>As model complexity increases, test error initially decreases as we better approximate the true model form, and then we pass through an inflection point, after-which the model starts to overfit.</p>
<p>This is due to the bias-variance decomposition, i.e., that MSE can be decomposed as:</p>
<p><span class="math display">\[\text{MSE}_{\text{test}}^* = \text{bias}^2 + \text{variance} + \text{noise}.\]</span></p>
<p>Formally, suppose that <span class="math inline">\(Y_i = f(x_i) + \varepsilon_i\)</span> and we fit some model that gives us predictions <span class="math inline">\(\hat Y_i = \hat g(x_i).\)</span> In this class, we usually have that <span class="math inline">\(\hat g(x_i) = x_i' \hat \beta\)</span>.</p>
<p>Then the expected squared error of the prediction for any test data point is</p>
<p><span class="math display">\[\mathbb E((\hat g(x_0) - Y_0)^2) = (\mathbb E(\hat g(x_0)) - f(x_0))^2 + \text{Var}(\hat g(x_0)) + \text{Var}(\varepsilon)\]</span></p>
<p><a href="https://allenkunle.me/bias-variance-decomposition" class="uri">https://allenkunle.me/bias-variance-decomposition</a></p>
</section>
<section id="penalized-regression-methods-for-variable-selection" class="level2">
<h2 class="anchored" data-anchor-id="penalized-regression-methods-for-variable-selection">Penalized Regression Methods for Variable Selection</h2>
<p>The two best-known penalized regression techniques are <em>ridge regression</em> and <em>LASSO</em>.</p>
<p>The basic idea is to shrink parameters toward zero. This increases bias but decreases variance.</p>
<p>In least-squares, we find coefficients that minimize the SSE. Meanwhile, in <em>penalized regression</em>, we minimize</p>
<p><span class="math display">\[SSE + \lambda \text{Penalty}(\beta).\]</span></p>
<p><span class="math inline">\(\lambda \geq 0\)</span> and the penalty function can take various forms.</p>
<ul>
<li>Ridge or <span class="math inline">\(\ell_2\)</span>: <span class="math inline">\(\text{Penalty}(\beta) = \sum_{j=1}^p \beta_j^2.\)</span></li>
<li>LASSO or <span class="math inline">\(\ell_1\)</span>: <span class="math inline">\(\text{Penalty}(\beta) = \sum_{j=1}^p |\beta_j|.\)</span></li>
</ul>
</section>
</section>
<section id="ridge-regression" class="level1">
<h1>Ridge Regression</h1>
<p><span class="math display">\[SSE = \sum_{i=1}^n \left(
y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}
\right)^2\]</span></p>
<p>In ridge regression the coefficients are estimated by minimizing the SSE while constraining the sum of the squared coefficients:</p>
<p><span class="math display">\[\min_{\beta} \left\{
\sum_{i=1}^n \left(
y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}
\right)
\right\} \quad s.t. \quad
\sum_{j=1}^p \beta_j^2 \leq s.
\]</span></p>
<p>By the theory of Lagrange multipliers (<a href="https://en.wikipedia.org/wiki/Joseph-Louis_Lagrange">Joseph-Louis Lagrange, 1735-1813</a>) for constrained optimization, this is equivalent to minimizing the SSE with a penalty.</p>
<p>In particular the ridge regression coefficient estimates <span class="math inline">\(\beta_{\lambda}^R\)</span> are the values that minimize</p>
<p><span class="math display">\[
\sum_{i=1}^n \left(
y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}
\right)^2 +
\lambda \sum_{j=1}^p \beta_j^2
\]</span></p>
<p>where <span class="math inline">\(\lambda \geq 0\)</span> is a tuning parameter that relates to, but is not the same as <span class="math inline">\(s\)</span>.</p>
<p>The intuition is that the sum of squared terms drives the minimization to fit the data, but at the same time, the second summation penalizes the <span class="math inline">\(\beta_j\)</span> coefficients towards zero.</p>
<div class="hottip">
<p>Note that we usually want to standardize the <span class="math inline">\(x_j\)</span> and do not want to shrink <span class="math inline">\(\beta_0\)</span>.</p>
</div>
<p>For fixed <span class="math inline">\(\lambda\)</span>, the solution <span class="math inline">\(\beta_{\lambda}^R\)</span> to the ridge regression problem is given by</p>
<p><span class="math display">\[\hat{\beta_{\lambda}^R} = (X'X + \lambda D)^{-1} X'y\]</span></p>
<p>where <span class="math inline">\(D = \text{Diag}(0,1_p)\)</span>.</p>
<p>Inspecting this, we can see that</p>
<ul>
<li>As <span class="math inline">\(\lambda \to 0, \, \hat\beta_\lambda^R \to \hat\beta_{OLS}\)</span></li>
<li>As <span class="math inline">\(\lambda \to \infty, \, \hat \beta_{\lambda}^R \to 0\)</span> for all predictors except the intercept.</li>
</ul>
<p>Typically one uses cross-validation techniques to determine the optimal value of <span class="math inline">\(\lambda\)</span> according to pre-specified criteria.</p>
<p>As <span class="math inline">\(\lambda\)</span> increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.</p>
<div class="cooltip">
<p>Thus ridge regression is a good option in situations where OLS estimates have high variance, including when:</p>
<ul>
<li><span class="math inline">\(p \approx n\)</span> or <span class="math inline">\(p \geq n\)</span></li>
<li>High multicollinearity</li>
</ul>
<p>Ridge regression also has substantial computational advantages over best subset selection.</p>
</div>
<div class="cooltip">
<p>It’s worth reflecting on when <span class="math inline">\(p \geq n\)</span> happens (i.e., we have more potential predictors than observations). In situations like genetics where we have samples from populations, it’s straightforward to imagine that we could have genetic data from 100s or 1000s (or even 10s of 1000s), while the <a href="https://en.wikipedia.org/wiki/Human_genome">human genome</a> is estimated to have something like ~20,000 protein-coding genes in it.</p>
</div>
</section>
<section id="lab-4" class="level1">
<h1>Lab 4</h1>
<p>Two major themes of our course are those of inference and prediction.</p>
<p>Why would we ever want to use a linear model for prediction?</p>
<ul>
<li>avoids over-fitting</li>
<li>interpretability</li>
</ul>
<p>When does OLS perform poorly?</p>
<ul>
<li>multicollinearity
<ul>
<li>the reason for this is because the <span class="math inline">\(\beta\)</span> coefficients are calculated via <span class="math inline">\((X^T)^{-1} XY\)</span> where if the column-spaces of <span class="math inline">\(X\)</span> are highly correlated, the inverse becomes highly unstable.</li>
</ul></li>
<li>if the number of predictors is close to or above the number of observations.</li>
<li>heteroscedasticity.</li>
</ul>
<p>How would we know if our model predicts poorly?</p>
<ul>
<li><span class="math inline">\(R^2\)</span> or adjusted <span class="math inline">\(R^2\)</span> <span class="math display">\[R^2 = 1 - \frac{SSE}{SST}\]</span></li>
<li>Cross-validation</li>
<li>Akaike Information Criterion <span class="math display">\[2p-2 \hat \ell = 2p - n \log (SSE) - n\log (n)\]</span></li>
</ul>
<p>Note that all of these focus on using the <span class="math inline">\(SSE\)</span>.</p>
<section id="bias-variance-tradeoff-1" class="level2">
<h2 class="anchored" data-anchor-id="bias-variance-tradeoff-1">Bias-Variance Tradeoff</h2>
<p>This week we mentioned the “bias-variance tradeoff.”</p>
<p>Show that <span class="math display">\[MSE(\hat \theta) = \text{Var}(\hat \theta) + \text{Bias}(\hat \theta)^2.\]</span></p>
<div class="cooltip">
<p>Use the formulas</p>
<p><span class="math display">\[\text{Bias}(\hat \theta) = \mathbb E(\hat \theta) - \theta\]</span> <span class="math display">\[\text{Var}(\hat \theta) = \mathbb E\left[\hat \theta - \mathbb E(\hat theta))^2 \right]\]</span> <span class="math display">\[\text{MSE}(\hat \theta) = \mathbb E(\hat \theta - \theta)^2\]</span></p>
<p>I think the idea is that we should take the MSE and add a convenient expression for 0 to it.</p>
<p><span class="math display">\[\begin{aligned}\text{MSE}(\hat \theta) &amp; = \mathbb E(\hat \theta - \mathbb E(\hat \theta) + \mathbb E(\hat \theta) - \theta)^2 \\
&amp; = \mathbb E(\hat \theta - \mathbb E(\hat \theta))^2 + \mathbb E(\hat \theta - \theta)^2 +
2 \underbrace{\mathbb E(\theta - \mathbb E(\hat \theta))}_{=2(\mathbb E\hat \theta - \mathbb E\hat \theta) = 0}(\mathbb E\hat \theta - \theta) \\
&amp; = \mathbb E(\hat \theta - \mathbb E(\hat \theta))^2 + (\mathbb E( \hat \theta) - \theta)^2 \\
&amp; = \text{Var}(\hat \theta) + \text{Bias}(\hat \theta)^2 \end{aligned}\]</span></p>
</div>
<p>Recall that we defined two different estimators for <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[\hat \sigma_{OLS}^2 = \frac{1}{n-1} \sum_{i=1}^n (y_i - \hat \mu)^2\]</span></p>
<p><span class="math display">\[ \hat \sigma^2_{MLE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat \mu)^2.\]</span></p>
<p>We mentioned that <span class="math inline">\(\hat \sigma^2_{OLS}\)</span> is unbiased and <span class="math inline">\(\hat \sigma^2_{MLE}\)</span> is biased, i.e.</p>
<p><span class="math display">\[\mathbb E(\hat \sigma^2_{OLS}) = \sigma^2 \quad \quad \mathbb E(\hat \sigma^2_{MLE})= \left(
\frac{n-1}{n}
\right) \sigma^2.\]</span></p>
<p>It can also be shown that <span class="math display">\[\text{Var}(\hat \sigma^2_{OLS}) = \frac{2 \sigma^4}{n-1} \quad \quad \text{Var}(\hat \sigma^2_{MLE}) = \frac{2(n-1)\sigma^4}{n^2}.\]</span></p>
<p>Use the formula above to find <span class="math inline">\(MSE(\hat \sigma^2_{OLS})\)</span> and <span class="math inline">\(MSE(\hat \sigma^2_{MLE})\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
MSE(\hat \sigma^2_{OLS}) &amp; = \text{Var}(\hat \sigma^2_{OLS}) + \text{Bias}(\hat \sigma^2_{OLS})^2 \\
&amp; = \frac{2\sigma^4}{n-1} + (\mathbb E(\hat \sigma^2_{OLS}) - \sigma^2)^2 \\
&amp; = \frac{2\sigma^4}{n-1} + (\sigma^2 - \sigma^2)^2 \\
&amp; = \frac{2\sigma^4}{n-1}\\
\end{aligned}\]</span></p>
<p>While</p>
<p><span class="math display">\[\begin{aligned}
MSE(\hat \sigma^2_{MLE}) &amp; = \text{Var}(\hat \sigma^2_{MLE}) + \text{Bias}(\hat \sigma^2_{MLE})^2 \\
&amp; = \frac{2(n-1)\sigma^4}{n^2} + (\mathbb E(\hat \sigma^2_{MLE}) - \sigma^2)^2 \\
&amp; = \frac{2(n-1)\sigma^4}{n^2} + (\frac{n-1}{n} \sigma^2 - \sigma^2)^2 \\
&amp; = \frac{2(n-1)\sigma^4}{n^2} + \frac{\sigma^4}{n^2}\\
&amp; = \frac{2n\sigma^4}{n^2}\\
\end{aligned}\]</span></p>
<p>Determine conditions for when <span class="math inline">\(MSE(\hat \sigma^2_{OLS}) &gt; MSE(\hat \sigma^2_{MLE})\)</span> for <span class="math inline">\(n \geq 2\)</span>. In these cases, trading off some bias for a reduction in variance, the MSE of <span class="math inline">\(\hat \sigma^2_{MLE}\)</span> is improved.</p>
<p>So when is <span class="math display">\[\left( \frac{2}{n-1}\right) \cancel{\sigma^4} &gt;
\frac{2n-1}{n^2} \cancel{\sigma^4}\]</span></p>
<p><span class="math display">\[\frac{2}{n-1} &gt; \frac{2}{n}\]</span></p>
</section>
<section id="penalized-regression" class="level2">
<h2 class="anchored" data-anchor-id="penalized-regression">Penalized Regression</h2>
<p>The most common procedure for penalization is LASSO, popularized by Robert Tibshirani in 1996. The objective of LASSO is to minimize the residual sum of squares subject to a constraint on the size of the parameters.</p>
<p><span class="math display">\[\sum_{i=1}^n (Y_i - x_i^T \beta)^2 \, \text{ subject to } \sum_{j=1}^p |\beta_j| \leq s,\]</span></p>
<p>for some <span class="math inline">\(s\)</span>. (Why should we care about the budget if we show this is equivalent to the optimization with a penalty later? Well it puts the problem in the language of convex optimization). Generally we center and standardize the <span class="math inline">\(Y_i\)</span> values before running LASSO, and we drop the intercept term from the formula.</p>
<p>We can equivalently write as minimizing:</p>
<p><span class="math display">\[\sum_{i=1}^n (Y_i - x_i^T \beta)^2 + \lambda \sum_{j=1}^p |\beta_j|.\]</span></p>
<p>What is the bias of our regression parameter estimates if <span class="math inline">\(\lambda = \infty\)</span>?</p>
<p>Well the <span class="math inline">\(\hat \beta\)</span> values get set to 0s, so the bias ends up being <span class="math inline">\(\mathbb E(0 - \beta) = -\beta\)</span>.</p>
<p>The ridge penalty can be rewritten in matrix notation as <span class="math display">\[(Y-X\beta)'(Y-X\beta) + \lambda \beta' D\beta,\]</span> where <span class="math inline">\(D = diag(0,1,...1).\)</span> Minimize this expression to show <span class="math inline">\(\hat \beta_{\lambda} = (X'X + \lambda D)^{-1} X'Y.\)</span></p>
<p>Remember <span class="math inline">\(\frac{\partial AZ}{\partial Z} = A, \, \frac{\partial Z'B}{\partial Z} = B'\)</span>, and if <span class="math inline">\(C\)</span> is symmetric <span class="math inline">\(\frac{\partial Z'CZ}{\partial Z} = 2Z'C.\)</span></p>
<p><span class="math display">\[\begin{aligned}
\frac{\partial}{\partial \beta} R &amp; = \frac{\partial}{\partial \beta}
(Y^TY - Y^T X \beta - \beta^T X^T Y + \beta^T X^T X \beta + \lambda \beta^T D
\beta) \quad \text{foil...} \\
&amp; = -Y^TX - Y^TX + 2 \beta^T X^TX + 2\lambda \beta^T D \\
&amp; \Rightarrow -2Y^TX + 2 \beta^T X^TX + 2\lambda \beta^T D \stackrel{set}{=} 0_{(p+1) \times 1} \\
&amp; \Leftrightarrow -X^TY + X^T \hat \beta + \lambda D\beta = 0 \\
&amp; \Leftrightarrow (X^TX + \lambda D)\hat \beta = X^T Y \\
&amp; \Leftrightarrow \hat \beta = (X^T X + \lambda D)^{-1} X^T Y,
\end{aligned}\]</span></p>
<p>which is a nice closed form solution.</p>
<p>Find the expectation of <span class="math inline">\(\hat \beta_{\lambda}\)</span>. Determine when it is a biased estimator of <span class="math inline">\(\beta\)</span>.</p>
<p><span class="math display">\[\mathbb E(\hat \beta) = \mathbb E((X^T + \lambda D)^{-1} X^TY) = (X^T X + \lambda D)^{-1} X^T \mathbb E(Y) \\
= (\X^TX + \lambda D)^{-1} X^T X \beta\]</span></p>
<p>If we set <span class="math inline">\(\lambda = 0\)</span>, then we recover <span class="math inline">\(\beta_{OLS}\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: carData</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'car'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:dplyr':

    recode</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:purrr':

    some</code></pre>
</div>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: Matrix</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'Matrix'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:tidyr':

    expand, pack, unpack</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loaded glmnet 4.1-8</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(here<span class="sc">::</span><span class="fu">here</span>(<span class="st">"week5/data/nhanes_sample.csv"</span>)) <span class="sc">|&gt;</span> </span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span><span class="dv">1</span>) <span class="sc">|&gt;</span> <span class="fu">scale</span>() <span class="sc">|&gt;</span> <span class="fu">as.data.frame</span>() </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>New names:
• `` -&gt; `...1`</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Rows: 50 Columns: 26
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (26): ...1, blood_pressure, bmi, waist, weight, DR1TMFAT, DR1TM161, DR1T...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(blood_pressure <span class="sc">~</span> ., <span class="at">data =</span> df)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>jtools<span class="sc">::</span><span class="fu">summ</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;">
<tbody>
  <tr>
   <td style="text-align:left;font-weight: bold;"> Observations </td>
   <td style="text-align:right;"> 50 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> Dependent variable </td>
   <td style="text-align:right;"> blood_pressure </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> Type </td>
   <td style="text-align:right;"> OLS linear regression </td>
  </tr>
</tbody>
</table> <table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;">
<tbody>
  <tr>
   <td style="text-align:left;font-weight: bold;"> F(24,25) </td>
   <td style="text-align:right;"> 2.80 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> R² </td>
   <td style="text-align:right;"> 0.73 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> Adj. R² </td>
   <td style="text-align:right;"> 0.47 </td>
  </tr>
</tbody>
</table> <table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;border-bottom: 0;">
 <thead>
  <tr>
   <th style="text-align:left;">   </th>
   <th style="text-align:right;"> Est. </th>
   <th style="text-align:right;"> S.E. </th>
   <th style="text-align:right;"> t val. </th>
   <th style="text-align:right;"> p </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;font-weight: bold;"> (Intercept) </td>
   <td style="text-align:right;"> 0.00 </td>
   <td style="text-align:right;"> 0.10 </td>
   <td style="text-align:right;"> 0.00 </td>
   <td style="text-align:right;"> 1.00 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> bmi </td>
   <td style="text-align:right;"> -1.01 </td>
   <td style="text-align:right;"> 0.40 </td>
   <td style="text-align:right;"> -2.53 </td>
   <td style="text-align:right;"> 0.02 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> waist </td>
   <td style="text-align:right;"> 0.79 </td>
   <td style="text-align:right;"> 0.47 </td>
   <td style="text-align:right;"> 1.66 </td>
   <td style="text-align:right;"> 0.11 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> weight </td>
   <td style="text-align:right;"> 0.71 </td>
   <td style="text-align:right;"> 0.40 </td>
   <td style="text-align:right;"> 1.77 </td>
   <td style="text-align:right;"> 0.09 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TMFAT </td>
   <td style="text-align:right;"> 3.17 </td>
   <td style="text-align:right;"> 1.21 </td>
   <td style="text-align:right;"> 2.62 </td>
   <td style="text-align:right;"> 0.01 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TM161 </td>
   <td style="text-align:right;"> -0.68 </td>
   <td style="text-align:right;"> 0.46 </td>
   <td style="text-align:right;"> -1.50 </td>
   <td style="text-align:right;"> 0.15 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TM181 </td>
   <td style="text-align:right;"> -1.62 </td>
   <td style="text-align:right;"> 1.16 </td>
   <td style="text-align:right;"> -1.40 </td>
   <td style="text-align:right;"> 0.17 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TM201 </td>
   <td style="text-align:right;"> -0.01 </td>
   <td style="text-align:right;"> 0.31 </td>
   <td style="text-align:right;"> -0.05 </td>
   <td style="text-align:right;"> 0.96 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TM221 </td>
   <td style="text-align:right;"> -0.25 </td>
   <td style="text-align:right;"> 0.15 </td>
   <td style="text-align:right;"> -1.65 </td>
   <td style="text-align:right;"> 0.11 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TPFAT </td>
   <td style="text-align:right;"> -7.52 </td>
   <td style="text-align:right;"> 7.11 </td>
   <td style="text-align:right;"> -1.06 </td>
   <td style="text-align:right;"> 0.30 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TP182 </td>
   <td style="text-align:right;"> 6.00 </td>
   <td style="text-align:right;"> 6.46 </td>
   <td style="text-align:right;"> 0.93 </td>
   <td style="text-align:right;"> 0.36 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TP183 </td>
   <td style="text-align:right;"> 0.65 </td>
   <td style="text-align:right;"> 0.71 </td>
   <td style="text-align:right;"> 0.92 </td>
   <td style="text-align:right;"> 0.37 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TP184 </td>
   <td style="text-align:right;"> 0.01 </td>
   <td style="text-align:right;"> 0.15 </td>
   <td style="text-align:right;"> 0.08 </td>
   <td style="text-align:right;"> 0.93 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TP204 </td>
   <td style="text-align:right;"> 0.39 </td>
   <td style="text-align:right;"> 0.41 </td>
   <td style="text-align:right;"> 0.94 </td>
   <td style="text-align:right;"> 0.36 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TP205 </td>
   <td style="text-align:right;"> 0.40 </td>
   <td style="text-align:right;"> 0.34 </td>
   <td style="text-align:right;"> 1.18 </td>
   <td style="text-align:right;"> 0.25 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TP225 </td>
   <td style="text-align:right;"> 0.25 </td>
   <td style="text-align:right;"> 0.36 </td>
   <td style="text-align:right;"> 0.71 </td>
   <td style="text-align:right;"> 0.48 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TP226 </td>
   <td style="text-align:right;"> -0.21 </td>
   <td style="text-align:right;"> 0.51 </td>
   <td style="text-align:right;"> -0.41 </td>
   <td style="text-align:right;"> 0.68 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TS040 </td>
   <td style="text-align:right;"> 0.16 </td>
   <td style="text-align:right;"> 0.66 </td>
   <td style="text-align:right;"> 0.24 </td>
   <td style="text-align:right;"> 0.81 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TS060 </td>
   <td style="text-align:right;"> 0.77 </td>
   <td style="text-align:right;"> 1.65 </td>
   <td style="text-align:right;"> 0.47 </td>
   <td style="text-align:right;"> 0.64 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TS080 </td>
   <td style="text-align:right;"> -0.21 </td>
   <td style="text-align:right;"> 0.79 </td>
   <td style="text-align:right;"> -0.27 </td>
   <td style="text-align:right;"> 0.79 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TS100 </td>
   <td style="text-align:right;"> -0.99 </td>
   <td style="text-align:right;"> 1.48 </td>
   <td style="text-align:right;"> -0.67 </td>
   <td style="text-align:right;"> 0.51 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TS120 </td>
   <td style="text-align:right;"> 0.16 </td>
   <td style="text-align:right;"> 0.38 </td>
   <td style="text-align:right;"> 0.42 </td>
   <td style="text-align:right;"> 0.68 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TS140 </td>
   <td style="text-align:right;"> -0.00 </td>
   <td style="text-align:right;"> 0.97 </td>
   <td style="text-align:right;"> -0.00 </td>
   <td style="text-align:right;"> 1.00 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TS160 </td>
   <td style="text-align:right;"> 0.11 </td>
   <td style="text-align:right;"> 0.61 </td>
   <td style="text-align:right;"> 0.18 </td>
   <td style="text-align:right;"> 0.86 </td>
  </tr>
  <tr>
   <td style="text-align:left;font-weight: bold;"> DR1TS180 </td>
   <td style="text-align:right;"> -0.61 </td>
   <td style="text-align:right;"> 0.63 </td>
   <td style="text-align:right;"> -0.97 </td>
   <td style="text-align:right;"> 0.34 </td>
  </tr>
</tbody>
<tfoot><tr><td style="padding: 0; " colspan="100%">
<sup></sup> Standard errors: OLS</td></tr></tfoot>
</table>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Variance Inflation Factors:"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Variance Inflation Factors:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">vif</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        bmi       waist      weight    DR1TMFAT    DR1TM161    DR1TM181 
  14.828728   20.577296   14.583985  134.444554   19.190784  123.855958 
   DR1TM201    DR1TM221    DR1TPFAT    DR1TP182    DR1TP183    DR1TP184 
   8.590510    2.039074 4653.739145 3848.793649   46.404126    2.198201 
   DR1TP204    DR1TP205    DR1TP225    DR1TP226    DR1TS040    DR1TS060 
  15.632184   10.690363   11.711358   24.201637   39.800310  250.835452 
   DR1TS080    DR1TS100    DR1TS120    DR1TS140    DR1TS160    DR1TS180 
  57.219977  201.899797   13.176486   86.407140   33.787848   36.795131 </code></pre>
</div>
</div>
<p>Usually one starts to be concerned around variance inflation factors of about 5 to 10.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>jtools<span class="sc">::</span><span class="fu">summ</span>(<span class="fu">lm</span>(blood_pressure <span class="sc">~</span> DR1TPFAT <span class="sc">+</span> DR1TP182, <span class="at">data =</span> df), <span class="at">model.info =</span> <span class="cn">FALSE</span>, <span class="at">model.fit =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;border-bottom: 0;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Est.
</th>
<th style="text-align:right;">
S.E.
</th>
<th style="text-align:right;">
t val.
</th>
<th style="text-align:right;">
p
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;font-weight: bold;">
(Intercept)
</td>
<td style="text-align:right;">
-0.00
</td>
<td style="text-align:right;">
0.13
</td>
<td style="text-align:right;">
-0.00
</td>
<td style="text-align:right;">
1.00
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
DR1TPFAT
</td>
<td style="text-align:right;">
-3.71
</td>
<td style="text-align:right;">
2.18
</td>
<td style="text-align:right;">
-1.71
</td>
<td style="text-align:right;">
0.09
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
DR1TP182
</td>
<td style="text-align:right;">
3.41
</td>
<td style="text-align:right;">
2.18
</td>
<td style="text-align:right;">
1.57
</td>
<td style="text-align:right;">
0.12
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<sup></sup> Standard errors: OLS
</td>
</tr>
</tfoot>

</table>
</div>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>jtools<span class="sc">::</span><span class="fu">summ</span>(<span class="fu">lm</span>(DR1TPFAT <span class="sc">~</span>  DR1TP182, <span class="at">data =</span> df), <span class="at">model.info =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td style="text-align:left;font-weight: bold;">
F(1,48)
</td>
<td style="text-align:right;">
12403.15
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
R²
</td>
<td style="text-align:right;">
1.00
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Adj. R²
</td>
<td style="text-align:right;">
1.00
</td>
</tr>
</tbody>

</table>
<table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;border-bottom: 0;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Est.
</th>
<th style="text-align:right;">
S.E.
</th>
<th style="text-align:right;">
t val.
</th>
<th style="text-align:right;">
p
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;font-weight: bold;">
(Intercept)
</td>
<td style="text-align:right;">
-0.00
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
-0.00
</td>
<td style="text-align:right;">
1.00
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
DR1TP182
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
111.37
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<sup></sup> Standard errors: OLS
</td>
</tr>
</tfoot>

</table>
</div>
</div>
<p>Notice the <span class="math inline">\(R^2\)</span> values in explaining one variable with the other!</p>
<section id="ridge-regression-application" class="level3">
<h3 class="anchored" data-anchor-id="ridge-regression-application">Ridge Regression Application</h3>
<p>The most common R package for fitting LASSO and ridge regression is <code>glmnet</code>, which can be fit as follows:</p>
<p>The function <code>glmnet</code> takes in a matrix of covariates X and a vector of outcomes y.</p>
<p>The alpha parameter controls the type of regularized regression; alpha=0 corresponds to ridge.</p>
<p>For linear regression, we use <code>family="gaussian"</code>.</p>
<p>Since every variable in our dataframe has been centered, we set <code>intercept=F</code>.</p>
<p>When a ridge regression is fit in <code>glmnet</code>, the model is fit over a grid of <span class="math inline">\(\lambda\)</span> tuning parameters. This can be input manually, but here we will let the package select the grid automatically. Plotting this model outputs a coefficient profile plot (“path plot”) displaying the value of each coefficient as <span class="math inline">\(\log(\lambda)\)</span> increases.</p>
<p>Let’s fit the model:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">select</span>(df, <span class="sc">!</span>blood_pressure))</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> df<span class="sc">$</span>blood_pressure</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>ridge <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">family =</span> <span class="st">"gaussian"</span>, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">intercept =</span> F)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ridge, <span class="at">xvar =</span> <span class="st">"lambda"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="week5_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">232</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>ridge_cv <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(<span class="at">x =</span> X, <span class="at">y =</span> y, <span class="at">family =</span> <span class="st">"gaussian"</span>, <span class="at">alpha =</span> <span class="dv">0</span>,</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>                      <span class="at">type.measure =</span> <span class="st">"mse"</span>, <span class="at">nfolds =</span> <span class="dv">10</span>, <span class="at">intercept =</span> F)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ridge_cv)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="week5_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(ridge_cv, <span class="at">s =</span> <span class="st">"lambda.min"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>25 x 1 sparse Matrix of class "dgCMatrix"
                      s1
(Intercept)  .          
bmi          0.026503073
waist        0.160615077
weight       0.191669340
DR1TMFAT     0.059047751
DR1TM161     0.014958156
DR1TM181     0.012996598
DR1TM201     0.103314647
DR1TM221    -0.053043589
DR1TPFAT    -0.095288004
DR1TP182    -0.086122444
DR1TP183    -0.106757795
DR1TP184     0.014389616
DR1TP204    -0.066057808
DR1TP205     0.045126119
DR1TP225     0.019760863
DR1TP226     0.014125553
DR1TS040    -0.019804464
DR1TS060    -0.029331971
DR1TS080    -0.003447792
DR1TS100    -0.077119005
DR1TS120     0.007502362
DR1TS140    -0.022003168
DR1TS160     0.015513329
DR1TS180     0.004587428</code></pre>
</div>
</div>
</section>
<section id="lasso-application" class="level3">
<h3 class="anchored" data-anchor-id="lasso-application">LASSO Application</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>lasso <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">family =</span> <span class="st">"gaussian"</span>, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">intercept =</span> F)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso, <span class="at">xvar =</span> <span class="st">"lambda"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="week5_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>lasso_cv <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(<span class="at">x =</span> X, <span class="at">y =</span> y, <span class="at">family =</span> <span class="st">"gaussian"</span>, <span class="at">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>                      <span class="at">type.measure =</span> <span class="st">"mse"</span>, <span class="at">nfolds =</span> <span class="dv">10</span>, <span class="at">intercept =</span> F)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso_cv)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="week5_files/figure-html/unnamed-chunk-14-2.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lasso_cv, <span class="at">s =</span> <span class="st">"lambda.min"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>25 x 1 sparse Matrix of class "dgCMatrix"
                     s1
(Intercept)  .         
bmi         -0.96586066
waist        0.65635413
weight       0.78167490
DR1TMFAT     1.38968707
DR1TM161    -0.30338201
DR1TM181    -0.31562573
DR1TM201     0.05306561
DR1TM221    -0.22729935
DR1TPFAT    -0.80671398
DR1TP182     .         
DR1TP183     .         
DR1TP184     0.02880849
DR1TP204    -0.06314280
DR1TP205     0.16115245
DR1TP225     0.05649850
DR1TP226     .         
DR1TS040     0.02281074
DR1TS060     .         
DR1TS080     .         
DR1TS100    -0.21420259
DR1TS120     .         
DR1TS140     .         
DR1TS160     .         
DR1TS180    -0.16941646</code></pre>
</div>
</div>
<p>A few notes on the interpretation of LASSO results:</p>
<ul>
<li><p>Because LASSO is just optimizing for prediction, selection of variables has no implications for the scientific importance of variables or their statistical significance.</p></li>
<li><p>One shouldn’t really use LASSO to select predictive variables and then go back to run an OLS regression on those variables</p></li>
</ul>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../week4/week4.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Week 4</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../week6/week6.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Week 6</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>