<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Methods (BST 232) Notes - 11&nbsp; Week 10</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../week9/week9.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-title">Week 10</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Methods (BST 232) Notes</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Methods (BST 232) Notes</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week1/week1.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 1</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week2/week2.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 2</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week3/week3.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 3</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week4/week4.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 4</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week5/week5.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 5</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week6/week6.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 6</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week7/week7.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 7</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week8/week8.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 8</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week9/week9.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 9</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week10/week10.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Week 10</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#generalized-linear-models" id="toc-generalized-linear-models" class="nav-link active" data-scroll-target="#generalized-linear-models">Generalized Linear Models</a>
  <ul class="collapse">
  <li><a href="#exponential-family" id="toc-exponential-family" class="nav-link" data-scroll-target="#exponential-family">Exponential Family</a>
  <ul class="collapse">
  <li><a href="#bernoulli-in-the-exponential-notation" id="toc-bernoulli-in-the-exponential-notation" class="nav-link" data-scroll-target="#bernoulli-in-the-exponential-notation">Bernoulli in the Exponential Notation</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#recap-of-likelihood-theory" id="toc-recap-of-likelihood-theory" class="nav-link" data-scroll-target="#recap-of-likelihood-theory">Recap of Likelihood Theory</a>
  <ul class="collapse">
  <li><a href="#exponential-family-log-likelihood-and-score" id="toc-exponential-family-log-likelihood-and-score" class="nav-link" data-scroll-target="#exponential-family-log-likelihood-and-score">Exponential Family Log-Likelihood and Score</a></li>
  <li><a href="#variance-in-exponential-families" id="toc-variance-in-exponential-families" class="nav-link" data-scroll-target="#variance-in-exponential-families">Variance in Exponential Families</a>
  <ul class="collapse">
  <li><a href="#bernoulli-example" id="toc-bernoulli-example" class="nav-link" data-scroll-target="#bernoulli-example">Bernoulli Example</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#the-systematic-component" id="toc-the-systematic-component" class="nav-link" data-scroll-target="#the-systematic-component">The Systematic Component</a>
  <ul class="collapse">
  <li><a href="#choosing-g" id="toc-choosing-g" class="nav-link" data-scroll-target="#choosing-g">Choosing <span class="math inline">\(g()\)</span></a></li>
  </ul></li>
  <li><a href="#frequentist-estimation-and-inference" id="toc-frequentist-estimation-and-inference" class="nav-link" data-scroll-target="#frequentist-estimation-and-inference">Frequentist Estimation and Inference</a>
  <ul class="collapse">
  <li><a href="#estimation" id="toc-estimation" class="nav-link" data-scroll-target="#estimation">Estimation</a></li>
  <li><a href="#asymptotic-sampling-distribution" id="toc-asymptotic-sampling-distribution" class="nav-link" data-scroll-target="#asymptotic-sampling-distribution">Asymptotic Sampling Distribution</a></li>
  </ul></li>
  <li><a href="#lab" id="toc-lab" class="nav-link" data-scroll-target="#lab">Lab</a>
  <ul class="collapse">
  <li><a href="#the-score" id="toc-the-score" class="nav-link" data-scroll-target="#the-score">The Score</a></li>
  <li><a href="#wald-test" id="toc-wald-test" class="nav-link" data-scroll-target="#wald-test">Wald Test</a></li>
  <li><a href="#score-test" id="toc-score-test" class="nav-link" data-scroll-target="#score-test">Score Test</a></li>
  <li><a href="#likelihood-ratio-test" id="toc-likelihood-ratio-test" class="nav-link" data-scroll-target="#likelihood-ratio-test">Likelihood Ratio Test</a>
  <ul class="collapse">
  <li><a href="#inference-on-a-single-proportion" id="toc-inference-on-a-single-proportion" class="nav-link" data-scroll-target="#inference-on-a-single-proportion">Inference on a single proportion</a></li>
  </ul></li>
  <li><a href="#asymptotic-inference" id="toc-asymptotic-inference" class="nav-link" data-scroll-target="#asymptotic-inference">Asymptotic Inference</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">Week 10</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>So far we’ve done one-sample tests for proportions, two-sample tests for proportions, contingency tables, and we’ve really allowed for at most looking at one binary outcome and one binary covariate.</p>
<p>Now we ask what if we want to build a regression model to look at a binary outcome with multiple predictors, or adjusted for multiple other covariates.</p>
<p>We’ll work with the following lung surgery example:</p>
<p>Is there an association between time spent in the operating room and post-surgical outcomes?</p>
<p>We could choose from a number of possible outcome variables, including: * Hospital stay of &gt;7 days * Number of major complications during the hospital stay</p>
<p>The first outcome is binary (<span class="math inline">\(Y \in \{ 0, 1 \}\)</span>) and the second is a count variable (<span class="math inline">\(Y \in \{ 0, 1, 2, ... \}\)</span>).</p>
<p>The scientific goal might be to characterize the relationship between such an outcome and a <span class="math inline">\(p\)</span>-vector of covariatets, <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>Why can’t we just use linear regression? We could just specify a mean model where <span class="math inline">\(\mathbb E[Y_i | \mathbf{x}_i] = \mathbf{x}_i' \beta\)</span> and estimate <span class="math inline">\(\beta\)</span> via OLS and perform inference via the CLT (which tells us that <span class="math inline">\(\hat \beta \stackrel{\cdot}{\sim} \mathcal N\)</span>?</p>
<p>OLS has nice properties under mild conditions: * if the mean model is correctly specified, <span class="math inline">\(\hat \beta_{OLS}\)</span> is unbiased * OLS is generally robust to the distribution of the error terms * OLS is BLUE if the error terms are homoscedastic.</p>
<p>The first issue one might run into is heteroscedasticity. If <span class="math inline">\(Y_i\)</span> is binary, then we know that it has to be Bernoulli distributed, such that under the mean model specification,</p>
<p><span class="math display">\[Y_i | \mathbf{x}_i \sim \text{Bernoulli}(\mu_i), \quad \text{ where } \mu = P(Y = 1) = \mathbb E(Y)\]</span> <span class="math display">\[ \mu_i = \mathbb E[Y_i | \mathbf{x}_i = \mathbf{x}_i' \beta]. \]</span></p>
<p>For the Bernoulli distribution, there is an implicit mean-variance relationship:</p>
<p><span class="math display">\[\text{Var}[Y_i | \mathbf{x}_i] = \mu_i (1 - \mu_i)\]</span></p>
<p>as long as <span class="math inline">\(\mu_i \neq \mu \forall i\)</span>, study units will be heteroscedastic (i.e., have non-constant variance).</p>
<p>As we’ve seen, heteroscedasticity isn’t a problem that goes away with large samples. It doesn’t go away with the central limit theorem.</p>
<p>Ignoring hteroscedasticity results in invalid inference, but we’ve seen three ways to remedy the situation: * Transform the response variable * Use OLS and base inference on a valid standard error * Use generalized least squares (GLS)</p>
<p>GLS can be a good option in discrete cases that is often under-appreciated, but it does have some limitations that lead many away from using it.</p>
<p>Recall <span class="math display">\[\hat \beta_{GLS} = (\mathbf{X}'\mathbf{\Sigma}^{-1}\mathbf{X})^{-1} \mathbf{X}' \mathbf{\Sigma}^{-1} \mathbf{y}\]</span> where if <span class="math inline">\(\text{Var}(Y_i | \mathbf{x}_i) = \mu_i (1-\mu_i)\)</span>, then</p>
<p><span class="math display">\[\Sigma = \text{diag}(\mu_1(1-\mu_1), ..., \mu_N(1-\mu_N))\]</span></p>
<p>and <span class="math inline">\(\hat \beta_{GLS}\)</span> is BLUE.</p>
<p>Recall that in the typical OLS setting, the <span class="math inline">\(\Sigma\)</span> matrix is given by <span class="math inline">\(\text{diag}(\sigma_1^2, ..., \sigma_N^2)\)</span>, but here we aren’t in a setting where <span class="math inline">\(Y = X \beta + \varepsilon\)</span> and <span class="math inline">\(\text{Var}(\varepsilon) = \sigma^2\)</span>, but instead we’re in this setting where the variance of <span class="math inline">\(Y\)</span> is informed by the Bernoulli distribution’s variance conditional on how the covariates inform the mean.</p>
<p>Recall that for uncorrelated data <span class="math inline">\(\hat \beta_{GLS}\)</span> is the maximizer of a weighted least squares criterion, i.e., <span class="math inline">\(\sum{i=1}^n w_i (y_i - x_i' \beta)^2\)</span> where <span class="math inline">\(w_i = (\text{Var}(Y_i | x_i))^{-1}\)</span>.</p>
<p>Thus we solve for <span class="math inline">\(\beta\)</span> in</p>
<p><span class="math display">\[ 0 = \frac{\partial}{\partial \beta } \sum{i=1}^n w_i (y_i - x_i' \beta)^2\]</span></p>
<p><span class="math display">\[ 0 = \sum_{i=1}^n x_i w_i (y_i x_i' \beta)\]</span></p>
<p>So when <span class="math inline">\(w_i = (\mu_i (1- \mu_i))^{-1}\)</span>, the weighted least squared equations are <span class="math display">\[0 = \sum_{i=1}^n \frac{x_i}{\mu_i ( 1- \mu_i)} (y_i - \mu_i).\]</span></p>
<p>In practice, we use the IRLS algorithm to estimate <span class="math inline">\(\hat \beta_{GLS}\)</span>.</p>
<p><span class="math inline">\(\hat \beta_{GLS}\)</span> is also the MLE when <span class="math inline">\(Y_i \sim \text{Bernoulli}(\mu_i)\)</span>. The likelihood and log-likelihood are given by</p>
<p><span class="math display">\[\mathcal L(\beta | y) = \prod_{i=1}^n \mu_i^{y_i} (1-\mu_i)^{1-y_i},\]</span> <span class="math display">\[\ell(\beta | y) = \sum_{i=1}^n [y_i \log (\mu_i) + (1-y_i) \log (1-\mu_i)].\]</span></p>
<p>To get the MLE, we take derivatives, set them equal to zero and solve. Following the algebra trail, we find that</p>
<p><span class="math display">\[\frac{\partial}{\partial \beta} \ell (\beta | y ) = \sum_{i=1}^n \frac{x_i}{\mu_i (1-\mu_i)} (y_i - \mu_i) \stackrel{set}{=} 0.\]</span></p>
<p>Thus the derivative of the log likelihood is equivalent to the weighted least squares equations, so <span class="math inline">\(\hat \beta_{GLS}\)</span> is the MLE.</p>
<p>Often it’s fine to use this approach, but we’ll see some shortcomings soon.</p>
<p>GLS can accomodate heteroscedasticity for binary outcomes. If the model is correctly specified, GLS is optimal.</p>
<p>However, when modeling binary or count response data, the linear regression model doesn’t respect the fact that the outcome is bounded. The functional that is being modeled is bounded:</p>
<ul>
<li>in binary outcome settings: <span class="math inline">\(\mathbb E[Y_i | x_i] \in (0,1)\)</span></li>
<li>in count outcome settings: <span class="math inline">\(\mathbb E[Y_i | x_i] \in (0,\infty)\)</span>,</li>
</ul>
<p>but our current specifications of the mean model doesn’t impose any restrictions and only assumes <span class="math inline">\(\mathbb E[Y_i|x_i] = x_i' \beta\)</span>. This means we could get predictions that are outside the region of appropriate outcomes.</p>
<p>Is this a big deal? Only sometimes. The GLS approach works quite well for modeling binary outcomes and the coefficients from linear models are quite interpretable. So it’s a trade-off between coefficient interpretability and this property about restricting predictions to the specified set of possible outcomes.</p>
<section id="generalized-linear-models" class="level1">
<h1>Generalized Linear Models</h1>
<p>Our goal is to develop statistical models to characterize the relationship between some response variable <span class="math inline">\(Y\)</span> and a vector of covariates <span class="math inline">\(x\)</span>.</p>
<p>Statistical models consist of two components:</p>
<ul>
<li>A systematic component</li>
<li>A random component</li>
</ul>
<p>When moving beyond linear regression analysis of continuous and response data, we need to be aware of two key challenges: * Sensible specification of the systematic component * Proper accounting of any implicit mean-variance relationships arising from the random component.</p>
<p><span class="vocab">Definition of a Generalized Linear Model</span></p>
<p>A <em>generalized linear model</em> (GLM) specifies a parametric statistical model for the conditional distribution of a response <span class="math inline">\(Y_i\)</span> given a <span class="math inline">\(p\)</span>-vector of covariates <span class="math inline">\(x_i\)</span>.</p>
<p>Consistes of three elements:</p>
<ol type="1">
<li>A probability distribution for the outcome, <span class="math inline">\(Y_i \sim f_{Y}(y)\)</span></li>
<li>A linear prediction equation, <span class="math inline">\(x_i' \beta\)</span></li>
<li>A link function, <span class="math inline">\(g(\cdot)\)</span>.</li>
</ol>
<p>The first of these elements is the random component, and elements 2 and 3 jointly specify the systematic component.</p>
<p>In practice, we see a wide range of response variables with a wide range of associated (possible) distributions</p>
<table class="table">
<thead>
<tr class="header">
<th>Response Type</th>
<th>Range</th>
<th>Possible Distribution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Continuous</td>
<td><span class="math inline">\((-\infty, \infty)\)</span></td>
<td><span class="math inline">\(\mathcal N(\mu, \sigma^2)\)</span></td>
</tr>
<tr class="even">
<td>Binary</td>
<td><span class="math inline">\(\{0, 1\}\)</span></td>
<td><span class="math inline">\(\text{Bernoulli}(\pi)\)</span></td>
</tr>
<tr class="odd">
<td>Polytomous</td>
<td><span class="math inline">\(\{1, ..., K \}\)</span></td>
<td><span class="math inline">\(\text{Multinomial}(\pi_k)\)</span></td>
</tr>
<tr class="even">
<td>Count</td>
<td><span class="math inline">\(\{0, 1, ..., n \}\)</span></td>
<td><span class="math inline">\(\text{Binomial}(n, \pi)\)</span></td>
</tr>
<tr class="odd">
<td>Count</td>
<td><span class="math inline">\(\{1, 2, ..., \}\)</span></td>
<td><span class="math inline">\(\text{Poisson}(\lambda)\)</span></td>
</tr>
<tr class="even">
<td>Positive</td>
<td><span class="math inline">\((0, \infty)\)</span></td>
<td><span class="math inline">\(\text{Gamma}(\alpha, \beta)\)</span></td>
</tr>
</tbody>
</table>
<p>For a given choice of probability distribution, a GLM specifies a model for the conditional mean:</p>
<p><span class="math display">\[ \mu_i = \mathbb E[Y_i | x_i] \]</span></p>
<p>How do we specify a reasonable model for <span class="math inline">\(\mu_i\)</span> while ensuring that we respect the appropriate range/scale of <span class="math inline">\(\mu_i\)</span>?</p>
<p>Achieved by constructing a linear predictor <span class="math inline">\(x_i' \beta\)</span> and relating it to <span class="math inline">\(\mu_i\)</span> via a link function <span class="math inline">\(g(\cdot)\)</span>:</p>
<p><span class="math display">\[g(\mu_i) = x_i' \beta.\]</span></p>
<p>We often specify <span class="math inline">\(g(\cdot)\)</span> such that <span class="math inline">\(g^{-1}(x_i' \beta) = \mu\)</span> respects the appropriate bounds on <span class="math inline">\(\mu_i\)</span>.</p>
<p>We’ll sometimes use the shorthand <span class="math inline">\(\eta_i = x_i' \beta\)</span>.</p>
<div class="cooltip">
<p>How is it that the regular linear model is a trivial GLM? Isn’t it the case that we only make distributional assumptions on <span class="math inline">\(\varepsilon\)</span> and not on <span class="math inline">\(Y\)</span>? Yes, but because a normal distribution plus a constant is still normally distributed (but this is not true for other distributions, e.g., a Binomial or Bernoulli distribution).</p>
<p>In the linear model we assume <span class="math inline">\(Y_i = X_i \beta + \epsilon_i\)</span>, so we could write <span class="math inline">\(Y \sim \mathcal N(X_i \beta, \sigma^2)\)</span> giving us the distributional assumption on <span class="math inline">\(Y\)</span>.</p>
<p>If we suppose that <span class="math inline">\(X_i \sim \text{Bernoulli}(\pi)\)</span>, we can’t model <span class="math inline">\(X_i \beta + \varepsilon_i\)</span> as another Bernoulli distribution where <span class="math inline">\(\varepsilon\)</span> is Bernoulli distributed.</p>
<p>This is why we’re going to have to write <span class="math inline">\(Y_i \sim f_Y\)</span> and <span class="math inline">\(g(\mu_i) = X_i \beta\)</span>.</p>
</div>
<section id="exponential-family" class="level2">
<h2 class="anchored" data-anchor-id="exponential-family">Exponential Family</h2>
<p>GLMs form a class of statistical models for response variables whose distribution belongs to the <em>exponential dispersion family</em></p>
<p>These are the family of distributions with a pdf/pmf of the form:</p>
<p><span class="math display">\[f_Y(y; \theta, \phi) = \exp \left\{ \frac{y \theta - b(\theta)}{a(\phi)} + c(y, \phi) \right\},\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the <em>canonical parameter</em>, <span class="math inline">\(\phi\)</span> is the <em>dispersion parameter</em>, and <span class="math inline">\(b(\theta)\)</span> is the <em>cumulant function</em>.</p>
<p>We will see that <span class="math inline">\(\theta\)</span> is always a function of the conditional mean, <span class="math inline">\(\mu_i\)</span>.</p>
<p>Note that the <span class="math inline">\(c(y, \phi)\)</span> should not depend on <span class="math inline">\(\theta\)</span>.</p>
<section id="bernoulli-in-the-exponential-notation" class="level3">
<h3 class="anchored" data-anchor-id="bernoulli-in-the-exponential-notation">Bernoulli in the Exponential Notation</h3>
<p>Let <span class="math inline">\(Y \sim \text{Bernoulli}(\mu)\)</span>.</p>
<p>A common first step is to apply a convenient transformation that is equivalent to the identity function: <span class="math inline">\(\exp(\log(\cdot))\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
f_Y(y ; \mu ) &amp; = \mu^y (1- \mu)^{1-y} \\
&amp; = \exp \{ y \log(\mu) + (1-y) \log (1-\mu) \} \\
&amp; = \exp \left\{ y \log \left( \frac{\mu}{1-\mu} \right)  + \log(1-\mu) \right\}
\end{aligned}
\]</span></p>
<p>Let</p>
<p><span class="math display">\[
\begin{aligned}
\theta = \log\left( \frac{\mu}{1-\mu} \right) \quad \quad &amp; b(\theta) = \log (1 + \exp \{ \theta \}) \\
a(\phi) = 1 \quad \quad &amp; c(y, \phi) = 0
\end{aligned}
\]</span></p>
<p>Then <span class="math display">\[
\begin{aligned}
f_Y(y; \theta, \phi) &amp; = \exp \{ y \theta - \log (1 + \exp \{ \theta \} ) \} \\
&amp; = \exp \left\{ \frac{y \theta - b(\theta) }{a(\phi)} + c(y, \phi) \right\}.
\end{aligned}
\]</span></p>
<p>Many other common distributions are members of this faimly. The canonical parameter <span class="math inline">\(\theta\)</span> has key relationships with both <span class="math inline">\(\mathbb E(Y)\)</span> and <span class="math inline">\(\text{Var}(Y)\)</span>. Typically varies across study units since <span class="math inline">\(\mathbb E(Y_i) = \mu_i\)</span>. We also index <span class="math inline">\(\theta\)</span> by <span class="math inline">\(i\)</span>, as in <span class="math inline">\(\theta_i\)</span>.</p>
<p>The dispersion parameter <span class="math inline">\(\phi\)</span> has a key relationship with <span class="math inline">\(\text{Var}(Y)\)</span>. It may, but does not typically, vary across study units. Typically it is not unit-specific, so we just write <span class="math inline">\(\phi\)</span>. In some settings, we may have <span class="math inline">\(a(\cdot)\)</span> vary with <span class="math inline">\(i\)</span>, as in $<span class="math inline">\(a_i(\phi)\)</span>. E.g., <span class="math inline">\(a_i(\phi) = \phi / w_i\)</span> where <span class="math inline">\(w_i\)</span> is the prior weight.</p>
<p>When the dispersion parameter is known, some may say that this distribution is a member of the <em>natural exponential family</em>.</p>
<p>Consider the likelihood function for a single observation</p>
<p><span class="math display">\[\mathcal L(\theta_i, \phi ; y_i) = \exp \left\{ \frac{y_i \theta - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi) \right\}.\]</span></p>
<p>The log-likelihood is</p>
<p><span class="math display">\[\ell(\theta_i, \phi; y_i) = \frac{y_i \theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi).\]</span></p>
<p>The first partial derivative with respect to <span class="math inline">\(\theta_i\)</span> is the score function for <span class="math inline">\(\theta_i\)</span> and is given by</p>
<p><span class="math display">\[\frac{\partial}{\partial \theta_i} \ell (\theta_i, \phi; y_i) = U(\theta_i) = \frac{y_i - b'(\theta_i)}{a_i(\phi)}.\]</span></p>
<p>Note that we consider <span class="math inline">\(\frac{\partial}{\partial \theta_i} \ell (\theta_i, \phi; y_i)\)</span> for the purpose of showing properties of exponential families (not for doing estimation).</p>
<p>We know that (under some regularity conditions)</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb E[U(\theta_i)] &amp; = 0 \\
\text{Var}[U(\theta_i)] &amp; = \mathbb E[U(\theta_i)^2] = \mathbb E\left[ \frac{\partial U(\theta_i)}{\partial \theta_i} \right]
\end{aligned}
\]</span></p>
<p>We will use these properties to get expressions for <span class="math inline">\(\mathbb E(Y_i)\)</span> and <span class="math inline">\(\text{Var}(Y_i)\)</span> in terms of the exponential family parameters.</p>
<p>Since the score has mean zero,</p>
<p><span class="math display">\[\mathbb E(U(\theta_i)) = \mathbb E\left[ \frac{Y_i - b'(\theta_i)}{a_i(\phi)} \right] = 0 \]</span></p>
<p>and, consequently,</p>
<p><span class="math display">\[\mu_i = \mathbb E[Y_i] = b'(\theta_i).\]</span></p>
</section>
</section>
</section>
<section id="recap-of-likelihood-theory" class="level1">
<h1>Recap of Likelihood Theory</h1>
<p>The MLE of <span class="math inline">\(\theta\)</span>, denoted <span class="math inline">\(\hat \theta_{MLE}\)</span> is obtained by solving the score equations with respect to <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[U(\theta | y) = \frac{\partial}{\partial \theta} \ell (\theta | y) = 0\]</span></p>
<p>Under some regularity conditions,</p>
<p><span class="math display">\[\hat \theta_{MLE} \sim \text{MVN}_p (\theta, \mathcal I_n(\theta)^{-1})\]</span></p>
<p>where <span class="math inline">\(\mathcal I_n(\theta)\)</span> is the Fisher information matrix with</p>
<p><span class="math display">\[\mathcal I_n(\theta)_{j,k} = -\mathbb E\left[ \frac{\partial^2 \ell (\theta | y)}{\partial \theta_j \partial \theta_k \right]\]</span></p>
<p>We can estimate <span class="math inline">\(\text{Var}(\hat \theta_{MLE}) \approx \mathcal I_n(\theta)^{-1}\)</span> by plugging in <span class="math inline">\(\hat \theta_{MLE}\)</span>.</p>
<p>The Wald test is measuring the distance between the MLE and the null value.</p>
<p><span class="math display">\[(\hat \theta_{1,MLE} - \theta_{1,0})^{T} \widehat{\text{Var}}(\hat \theta_{1,MLE})^{-1} (\hat \theta_{1,MLE} - \theta_{1,0}) \stackrel{d}{\longrightarrow} \chi_q^2 \]</span></p>
<p>The Score test finds the first derivative of the log likelihood at the null value. If this derivative is large, then that means that this null value is far away from the MLE.</p>
<p><span class="math display">\[U(\hat \theta_{0,MLE} | y)' \mathcal I_n(\hat \theta_{0,MLE})^{-1} U(\hat \theta_{0, MLE} | y)  \stackrel{d}{\longrightarrow} \chi_q^2 \]</span></p>
<p>The Likelihood ratio test</p>
<p><span class="math display">\[2 \left( \ell(\hat \theta_{MLE} | y) - \ell(\hat \theta_{0,MLE}|y) \right)  \stackrel{d}{\longrightarrow} \chi_q^2 \]</span></p>
<p>Asymptotically, these should all give the same results, but in small data settings these may differ.</p>
<p>The Fisher information is essentially the negative second derivative of the likelihood function. Since the variance of <span class="math inline">\(\hat \theta_{MLE}\)</span> can be estimated by <span class="math inline">\(\approx \mathcal I_n(\theta)^{-1}\)</span>, if the likelihood function is a very steep (downward) parabola, its second derivative will have large magnitude, and its inverse will thus be small.</p>
<section id="exponential-family-log-likelihood-and-score" class="level2">
<h2 class="anchored" data-anchor-id="exponential-family-log-likelihood-and-score">Exponential Family Log-Likelihood and Score</h2>
<p>Consider the likelihood function for a single observation</p>
<p><span class="math display">\[\mathcal L(\theta_i, \phi; y) = \exp \{ \frac{y_i \theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi) \}\]</span></p>
<p><span class="math display">\[\frac{partial}{\partial \theta_i} \ell(\theta_i, \phi; y_i) = U(\theta_i) = \frac{y_i - b'(\theta_i)}{a_i(\phi)}\]</span></p>
<p>From previously,</p>
<p><span class="math display">\[\mathbb E[U(\theta_i)] = 0\]</span></p>
<p><span class="math display">\[\text{Var}[U(\theta_i)] = \mathbb E[U(\theta_i)^2] = -\mathbb E\left[ \frac{\partial U(\theta_i)}{\partial \theta_i} \right]\]</span></p>
<p>Since this score has mean zero,</p>
<p><span class="math display">\[\mathbb E(U(\theta_i)) = \mathbb E(\frac{Y_i - b'(\theta_i)}{a_i(\phi)}) = 0\]</span></p>
<p>and hence</p>
<p><span class="math display">\[\mu_i = \mathbb E[Y_i] = b'(\theta_i).\]</span></p>
</section>
<section id="variance-in-exponential-families" class="level2">
<h2 class="anchored" data-anchor-id="variance-in-exponential-families">Variance in Exponential Families</h2>
<p>The second partial derivative is</p>
<p><span class="math display">\[\frac{\partial^2}{\partial \theta_i^2} \ell(\theta_i, \phi; y) = - \frac{b''(\theta_i)}{a_i(\phi)}\]</span></p>
<p>Using the above properties, it follows that</p>
<p><span class="math display">\[\text{Var}(U(\theta_i)) = \text{Var}\left[ \frac{Y_i - b'(\theta_i)}{a_i(\phi)} \right] = \frac{b''(\theta_i)}{a_i(\phi)}\]</span></p>
<p>so that</p>
<p><span class="math display">\[\text{Var}[Y_i] = b''(\theta_i)a_i(\phi)\]</span></p>
<p>The variance of <span class="math inline">\(Y_i\)</span> is therefore a function of both <span class="math inline">\(\theta_i\)</span> and <span class="math inline">\(\phi\)</span>.</p>
<p>Note that the canonical parameter is a function of <span class="math inline">\(\mu_i\)</span></p>
<p><span class="math display">\[\mu_i = b'(\theta_i) \Rightarrow \theta_i = b'^{-1}(\mu_i) = \theta(\mu_i)\]</span></p>
<p>so that we can write</p>
<p><span class="math display">\[\text{Var}[Y_i] = b''(\theta_i)a_i(\phi) = b''(\theta(\mu_i))a_i(\phi)\]</span></p>
<p>The function <span class="math inline">\(V(\mu_i) = b''(\theta(\mu_i))\)</span> is called the <span class="vocab">variance function</span>.</p>
<p>The specific form indicates the nature of the (if any) mean-variance relationship.</p>
<section id="bernoulli-example" class="level3">
<h3 class="anchored" data-anchor-id="bernoulli-example">Bernoulli Example</h3>
<p>Let <span class="math inline">\(Y \sim \text{Bernoulli}(\mu)\)</span>.</p>
<p><span class="math display">\[\theta = \log \left( \frac{\mu}{1-\mu} \right), \quad \quad \text{ the logit of } \mu\]</span></p>
<p><span class="math display">\[a(\phi) = 1\]</span></p>
<p><span class="math display">\[b(\theta) = \log(1 + \exp \theta)\]</span></p>
<p><span class="math display">\[\mathbb E[Y] = b'(\theta) = \frac{\exp \theta }{1 + \exp \theta} = \mu, \quad \quad \text{ the expit of } \theta\]</span></p>
<p><span class="math display">\[\text{Var}[Y] = b''(\theta)a(\phi) = \frac{\exp \theta}{(1 + \exp \theta)^2} = \mu(1 - \mu) = V(\mu)\]</span></p>
</section>
</section>
</section>
<section id="the-systematic-component" class="level1">
<h1>The Systematic Component</h1>
<p>The systematic component is the specification of how the distribution <span class="math inline">\(Y \sim f_Y(y; \theta, \phi)\)</span> (where <span class="math inline">\(f_Y\)</span> is an exponential family distribution) depends on the covariates <span class="math inline">\(x_i\)</span>.</p>
<p>In GLMs we model the conditional mean <span class="math inline">\(\mu_i = \mathbb E[Y_i | x_i].\)</span></p>
<p>This provides a connection between <span class="math inline">\(x_i\)</span> and the distribution of <span class="math inline">\(Y_i\)</span> via the canonical parameter <span class="math inline">\(\theta_i\)</span> and the cumulant function <span class="math inline">\(b(\theta_i)\)</span>.</p>
<p><span class="math display">\[f_Y(y) \longrightarrow \theta_i \longrightarrow \mu_i \longrightarrow x_i'\beta \quad \text{(a.k.a. }\eta_i)\]</span></p>
<p>Typically we link the linear predictor of the distribution of <span class="math inline">\(Y\)</span> via a transformation of <span class="math inline">\(\mu_i\)</span>, <span class="math inline">\(g(\cdot)\)</span>, so that</p>
<p><span class="math display">\[g(\mu_i) = x_i'\beta\]</span></p>
<p>Traditionally this is broken into two parts:</p>
<ol type="1">
<li>the linear predictor, <span class="math inline">\(\eta_i = x_i' \beta\)</span></li>
<li>the link function <span class="math inline">\(g(\mu_i) = \eta_i\)</span></li>
</ol>
<p>Sometimes you’ll find the ‘predictor component’ of the model called the ‘systematic component’, as in McCullough and Nelder (1989). In practice, one cannot consider one without the other since the relationship between <span class="math inline">\(\mu_i\)</span> and <span class="math inline">\(x_i\)</span> are jointly determined by <span class="math inline">\(\beta\)</span> and <span class="math inline">\(g(\cdot)\)</span>.</p>
<p>Constructing the linear predictor for a GLM follows the same process one uses for linear regression.</p>
<p>Given a set of covariates <span class="math inline">\(x_i\)</span>, there are two decisions.</p>
<ul>
<li>which covariates to include?</li>
<li>how to include them in the model?</li>
</ul>
<p>For the most part, decisions to include covariates should be driven by scientific considerations</p>
<ul>
<li>is the goal estimation or prediction?</li>
<li>is there a primary exposure of interest?</li>
<li>are any of the covariates effect modifiers? confounders?</li>
</ul>
<p>We will see that there are some differences (from multiple linear regression) when it comes to identification of confounders.</p>
<p>In the GLM framework, we’ll want to note that the inverse of the link function provides the specification of the model on the scale of <span class="math inline">\(\mu_i\)</span></p>
<p><span class="math display">\[\mu_i = g^{-1}(x_i' \beta)\]</span></p>
<p>so we often choose <span class="math inline">\(g()\)</span> such that <span class="math inline">\(g^{-1}(x_i' \beta)\)</span> respects any bounds on <span class="math inline">\(\mu_i\)</span>.</p>
<p>We interpret the link function as specifying a transformation of the conditional mean, <span class="math inline">\(\mu_i\)</span>.</p>
<p>We are not specifying a transformation of the response <span class="math inline">\(Y_i\)</span>.</p>
<p>We are writing <span class="math inline">\(g(\mathbb E(Y_i|x_i))\)</span>, <strong>not</strong> <span class="math inline">\(\mathbb E(g(Y_i) | x_i) = x_i'\beta\)</span>.</p>
<p>Recall that the mean and the canonical parameter are linked via the derivative of the cumulant function.</p>
<p><span class="math display">\[\mathbb E[Y_i] = \mu_i = b'(\theta_i)\]</span></p>
<p>An important link function is the canonical link:</p>
<p><span class="math display">\[g(\mu_i) = \theta(\mu_i) = b'^{-1}(\mu_i),\]</span></p>
<p>i.e., the function that results by viewing the canonical parameter <span class="math inline">\(\theta_i\)</span> as a function of <span class="math inline">\(\mu_i\)</span>.</p>
<p>We’ll see later that this choice results in some mathematical convenience.</p>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Linear regression</th>
<th>Logistic regression</th>
<th>Poisson regression</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Outcome type</td>
<td>Continuous</td>
<td>Binary</td>
<td>Count</td>
</tr>
<tr class="even">
<td>Link</td>
<td>Identity</td>
<td>Logit</td>
<td></td>
</tr>
<tr class="odd">
<td>Systematic component</td>
<td><span class="math inline">\(\mu_i = x_i'\beta\)</span></td>
<td><span class="math inline">\(\log(\mu_i / (1-\mu_i) = x_i' \beta)\)</span> or <span class="math inline">\(\mu_i = \exp(x_i'\beta)/(1+\exp (x_i'\beta))\)</span></td>
<td><span class="math inline">\(\log(\mu_i) = x_i'\beta\)</span> or <span class="math inline">\(\mu_i = \exp(x_i' \beta)\)</span></td>
</tr>
<tr class="even">
<td>Inv. link</td>
<td>Identity</td>
<td>Expit</td>
<td>Exponential</td>
</tr>
</tbody>
</table>
<section id="choosing-g" class="level3">
<h3 class="anchored" data-anchor-id="choosing-g">Choosing <span class="math inline">\(g()\)</span></h3>
<p>Common link functions include:</p>
<ul>
<li>identity</li>
<li>log</li>
<li>logit, <span class="math inline">\(g(\mu_i) = \log \left( \frac{\mu_i}{1-\mu_i} \right)\)</span></li>
<li><a href="https://en.wikipedia.org/wiki/Probit">probit</a></li>
<li>complementary log-log <span class="math inline">\(g(\mu_i) = \log ( - \log(1-\mu_i))\)</span></li>
</ul>
<p>We typically choose a <span class="math inline">\(g()\)</span> via consideration of two issues:</p>
<ol type="1">
<li>Respect the range of values that <span class="math inline">\(\mu_i\)</span> can take</li>
<li>Impact on the interpretability of <span class="math inline">\(\beta\)</span></li>
</ol>
</section>
</section>
<section id="frequentist-estimation-and-inference" class="level1">
<h1>Frequentist Estimation and Inference</h1>
<p>Given an i.i.d. sample of size <span class="math inline">\(n\)</span>, the log-likelihood is</p>
<p><span class="math display">\[\ell(\beta, \phi; y) = \sum_{i=1}^n \left[ \frac{y_i \theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi)\right]\]</span></p>
<section id="estimation" class="level2">
<h2 class="anchored" data-anchor-id="estimation">Estimation</h2>
<p>There are <span class="math inline">\((p+2)\)</span> unknown parameters: <span class="math inline">\((\beta, \phi)\)</span>.</p>
<p>To obtain the MLE we need to solve the score equations:</p>
<p><span class="math display">\[U(\beta, \phi | y) = \left( \frac{\partial \ell}{\parital \beta_0}, ..., \frac{\partial \ell}{\partial \beta_p}, \frac{\partial \ell}{\partial \phi} \right)' = 0\]</span></p>
<p>This is a system of <span class="math inline">\(p+2\)</span> equations. The contribution to the score for <span class="math inline">\(\phi\)</span> is the <span class="math inline">\(i^{\mathrm{th}}\)</span> unit is <span class="math display">\[\frac{\partial \ell}{\partial \phi} = - \frac{a_i'(\phi)}{a_i(\phi)^2}(y_i \theta_i - b(\theta_i)) + c'(y_i, \phi)\]</span></p>
<p>We can use the chain rule to obtain a convenient expression for the <span class="math inline">\(i^{\mathrm{th}}\)</span> contribution to the score function for <span class="math inline">\(\beta_j\)</span>:</p>
<p><span class="math display">\[\frac{\partial \ell}{\partial \beta_j} = \frac{\partial \ell}{\partial \theta_i} \times \frac{\partial \theta_i}{\partial \mu_i} \times \frac{\partial \mu_i}{\partial \eta_i} \times \frac{\partial \eta_i}{\partial \beta_j}\]</span></p>
<p>Note the following results:</p>
<p><span class="math display">\[\frac{\partial \ell}{\partial \theta_i} = \frac{y_i - b'(\theta_i)}{a_i(\phi)} = \frac{y_i-\mu_i}{a_i(\phi)}\]</span></p>
<p><span class="math display">\[\frac{\partial \theta_i}{\partial \mu_i} = \left( \frac{\partial \mu_i}{\partial \theta_i} \right)^{-1} = (b''(\theta_i))^{-1} = (V(\mu_i))^{-1}\]</span></p>
<p><span class="math display">\[\frac{\partial \eta_i}{\partial \beta_j} = x_{ij}\]</span></p>
<p>The score function for <span class="math inline">\(\beta_j\)</span> can therefore be written as <span class="math display">\[\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \frac{\partial \mu_i}{\partial \eta_i} \frac{x_{ij}}{V(\mu_i)a_i(\phi)} (y_i - \mu_i)\]</span></p>
<p>Suppose <span class="math inline">\(a_i(\phi) = \phi / w_i\)</span>. The score equations become</p>
<p><span class="math display">\[\frac{\partial \ell}{\partial \phi} = \sum_{i=1}^n - \frac{w_i ( y_i \theta_i - b(\theta_i))}{\phi^2} + c' (y_i, \phi) = 0\]</span></p>
<p><span class="math display">\[\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n w_i \frac{\partial \mu_i}{\partial \eta_i} \frac{x_{ij}}{V(\mu_i)} (y_i - \mu_i) = 0\]</span></p>
<p>Notice that the <span class="math inline">\((p+1)\)</span> score equations for <span class="math inline">\(\beta\)</span> do not depend on <span class="math inline">\(\phi\)</span>.</p>
<p>Consequently, obtaining the MLE of <span class="math inline">\(\beta\)</span> doesn’t require knowledge of <span class="math inline">\(\phi\)</span>.</p>
<p><span class="math inline">\(\phi\)</span> isn’t required to be known or estimated (if unknown).</p>
<p>For exmaple, in linear regression, we don’t need <span class="math inline">\(\sigma^2\)</span> (or <span class="math inline">\(\hat \sigma^2\)</span>) to obtain <span class="math display">\[\hat \beta_{MLE} = (X^T X)^{-1} X^T Y\]</span></p>
<p>Inference does require an estimate of <span class="math inline">\(\phi\)</span>.</p>
</section>
<section id="asymptotic-sampling-distribution" class="level2">
<h2 class="anchored" data-anchor-id="asymptotic-sampling-distribution">Asymptotic Sampling Distribution</h2>
<p>Subject to appropriate regularity conditions,</p>
<p><span class="math display">\[
\begin{bmatrix}
\hat \beta_{MLE} \\
\hat \phi_{MLE}
\end{bmatrix} \stackrel{\cdot}{\sim}
\text{MVN} \left(
  \begin{bmatrix}
  \beta \\
  \phi
  \end{bmatrix},
\mathcal I_n(\beta, \phi)^{-1}
\right)
\]</span></p>
<p>Now we can compute the components of <span class="math inline">\(\mathcal I_n(\beta, \phi):\)</span></p>
<p><span class="math display">\[\begin{aligned}
\frac{\partial^2 \ell(\beta, \phi; y_i)}{\partial \beta_j \partial \beta_k} &amp; =
\frac{\partial}{\partial \beta_k} \overbrace{\left\{ \frac{\partial \mu_i}{\partial \eta_i}
\frac{x_{ij}}{V(\mu_i)a_i(\phi)} (y_i - \mu_i)
\right\}}^{\text{score for } \beta_j} \\
&amp; = (y_i-\mu_i) \frac{\partial}{\partial \beta_k} \left\{ \frac{\partial \mu_i}{\partial \eta_i} \frac{x_{ij}}{V(\mu_i) a_i(\phi)} \right\} - \left( \frac{\partial \mu_i}{\partial \eta_i} \right)^2 \frac{x_{ij} x_{ik}}{V(\mu_i)a_i(\phi)}
\end{aligned}\]</span></p>
<p>And hence</p>
<p><span class="math display">\[-\mathbb E\left[ \frac{\partial^2 \ell}{\partial \beta_j \partial \beta_k} \right] = \sum_{i=1}^n \left(
  \frac{\partial \mu_i}{\partial \eta_i}
  \right)^2
  \frac{x_{ij}x_{ik}}{V(\mu_i)a_i(\phi)}
\]</span></p>
<p><span class="math display">\[ \begin{aligned}
\frac{\partial^2 \ell(\beta, \phi; y_i)}{\partial \phi \partial \phi} &amp; =
\frac{\partial}{\partial \phi} \overbrace{\left\{
  - \frac{a_i'(\phi)}{a_i(\phi)^2} (y_i \theta_i - b(\theta_i)) + c'(y_i, \phi)
\right\}}^{\text{score for } \phi}  \\
&amp; = - \left\{ \frac{a_i(\phi)^2 a_i'\right\}
\end{aligned} \]</span></p>
</section>
</section>
<section id="lab" class="level1">
<h1>Lab</h1>
<p>Throughout this course, we’ve discussed “asymptotic” properties of estimators, including consistency <span class="math inline">\((\hat \theta \stackrel{p}{\to} \theta)\)</span>. We’ve also discussed asymptotic tests — for example, F-tests which converge to a <span class="math inline">\(\chi^2\)</span> distribution in large samples. However, previous discussions have been limited to models involving normally distributed rnadom variables.</p>
<p>Asymptotic likelihood theory provides a framework by which we can construct estimators with optimal properties for arbitrary distributions, as well as asymptotic tests based on those estimators. Consider an analysis of a random sample such that</p>
<p><span class="math display">\[Y_1, ..., Y_n \stackrel{iid}{\sim} f(y_i \mid \theta)\]</span></p>
<p>and we are interested in drawing inference <span class="math inline">\(\theta\)</span> (which may be a vector of parameters). Let <span class="math inline">\(\mathbf{Y} = (Y_1, ..., Y_n)\)</span>. Then we can define several components of a likelihood model:</p>
<p>The likelihood:</p>
<p><span class="math display">\[\mathcal L(\theta \mid \mathbf{Y}) = f(y \mid \theta) = \prod_{i=1}^n f(y_i \mid \theta)\]</span></p>
<p>Log-likelihood:</p>
<p><span class="math display">\[\ell(\theta \mid y) = \log \mathcal L(\theta \mid y) = \sum_{i=1}^n \log f(y_i \mid \theta)\]</span></p>
<p>Score:</p>
<p><span class="math display">\[U(\theta \mid y) = \frac{\partial}{\partial \theta} \ell (\theta \mid y) = \left( \frac{\partial}{\partial \theta_1} \ell(\theta \mid y), ..., \frac{\partial}{\partial \theta_1} \ell(\theta \mid y) \right)\]</span></p>
<section id="the-score" class="level2">
<h2 class="anchored" data-anchor-id="the-score">The Score</h2>
<p>Information Matrix</p>
<p><span class="math display">\[\mathcal I(\theta) = \mathbb E( U(\theta | y) \otimes U(\theta \mid y)^T) \]</span></p>
<p>Since <span class="math inline">\(\mathbb E[U(\theta \mid y)]^2 = 0\)</span>.</p>
<p>This is the variance of the score. Under regularity conditions, the <span class="math inline">\((i,j)\)</span>th entry is equal to <span class="math display">\[- \mathbb E( \frac{\partial^2}{\partial \theta_i \partial \theta_j} \ell ( \theta \mid y)).\]</span></p>
<p>As we’ve seen previously, the MLE is the solution to <span class="math inline">\(U(\theta \mid y) = 0\)</span>.</p>
<p>The nice property of this estimator is that, for distributions with reasonably well-behaved likelihood functions, <span class="math inline">\(\hat \theta_{MLE}\)</span> is consistent and asymptotically normal (CAN). Therefore</p>
<p><span class="math display">\[\sqrt{n} (\hat \theta_{MLE} - \theta) \stackrel{\mathcal D}{\to} \mathcal N(0, \mathcal I(\theta)^{-1}).\]</span></p>
<p>This identity is from where “asymptotic tests” come from — no matter the true distribution of <span class="math inline">\(\mathbf Y\)</span>, <span class="math inline">\(\hat \theta_{MLE}\)</span> will be asymptotically normal (or <span class="math inline">\(\chi^2\)</span> if we square the estimator). There are three different ways, denoted in the figure below, to construct asymptotic tests of <span class="math inline">\(H_0 : \theta = \theta_0\)</span> — each is obtained by using the above identity in a different way.</p>
<p><img src="images/asymptotic_tests.png" class="img-fluid"></p>
</section>
<section id="wald-test" class="level2">
<h2 class="anchored" data-anchor-id="wald-test">Wald Test</h2>
<p>Directly evaluates whether <span class="math inline">\(\hat \theta_{MLE}\)</span> is consistent with <span class="math inline">\(H_0 : \theta = \theta_0\)</span>.</p>
<p><span class="math display">\[W_n = (\hat \theta_{MLE} - \hat \theta_0)^T \mathcal I(\hat \theta_{MLE}) (\hat \theta_{MLE} - \theta_0) \approx \chi^2(p)\]</span></p>
<p>Note that if <span class="math inline">\(\theta\)</span> is one dimensional this simplifies to</p>
<p><span class="math display">\[W_n = (\hat \theta_{MLE} - \theta_0) \mathcal I(\hat \theta_{MLE})^{1/2} \approx \mathcal N(0,1)\]</span></p>
<ul>
<li>Advantages: Simple to compute, easy to construct confidence interval</li>
<li>Disadvantages: requires MLE, approximation not as accurate in small samples</li>
</ul>
</section>
<section id="score-test" class="level2">
<h2 class="anchored" data-anchor-id="score-test">Score Test</h2>
<p>Asks “how consistent is the observed data with the null hypothesis?” and answers this by considering the gradient of the log-likelihood at the value specified by <span class="math inline">\(H_0\)</span> (should be close to 0).</p>
<p><span class="math display">\[S_n = U(\theta_0 \mid y)^{T} \mathcal I(\theta_0)^{-1} U(\theta_0 \mid y) \approx \chi^2(p)\]</span></p>
<p>Note thta if <span class="math inline">\(\theta\)</span> is 1-dimensional, we can simplify this expression to</p>
<p><span class="math display">\[S_n = U(\theta_0 \mid y) \mathcal I(\theta_0)^{-1/2} \approx \mathcal N(0, 1)\]</span></p>
<ul>
<li>Advantages: Do not need to compute MLE, more computationally efficient.</li>
<li>Disadvatage: Hard to construct confidence intervals since inverting this equation is difficult.</li>
</ul>
<p>Note to remember: For all of the above, if our null hypothesis is a function of multiple parameters, we can construct a <span class="math inline">\(C\)</span> matrix and test <span class="math inline">\(C \theta\)</span> just as we did for the linear model in the first half of the course.</p>
</section>
<section id="likelihood-ratio-test" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-ratio-test">Likelihood Ratio Test</h2>
<p>Considers the relative likelihood of the “best fitting model” under no restrictions to the model under <span class="math inline">\(H_0\)</span>.</p>
<p><span class="math display">\[Q_n = 2 \left( \ell(\hat\theta_{MLE} - \ell(\theta_0) \right) \approx \chi^2(p)\]</span></p>
<ul>
<li>Advantages: No derivatives needed, most accurate approximation</li>
<li>Disdvantage: Requires both MLE and knowledge of log-likelihood under <span class="math inline">\(H_0\)</span>.</li>
</ul>
<section id="inference-on-a-single-proportion" class="level3">
<h3 class="anchored" data-anchor-id="inference-on-a-single-proportion">Inference on a single proportion</h3>
<p>Suppose we have data on the number of independent successes in a population of <span class="math inline">\(n\)</span> study units. This gives rise to the model</p>
<p><span class="math display">\[Y \sim \text{Binom}(n, \pi)\]</span></p>
<p>Suppose our goal is to estimate <span class="math inline">\(\pi\)</span> and test the null hypothesis <span class="math inline">\(H_0 : \pi = \pi_0\)</span>. There are two ways to do this: we can either conduct an asymptotic <span class="math inline">\(\chi^2\)</span> test using one of the three previously discussed options, or we can perform an “exact” test by comparing observed values against the Binomial distribution directly.</p>
</section>
</section>
<section id="asymptotic-inference" class="level2">
<h2 class="anchored" data-anchor-id="asymptotic-inference">Asymptotic Inference</h2>
<p>For any test we use, we’ll need the components of the likelihood. Recall that the pmf of the Binomial distribution is</p>
<p><span class="math display">\[{n \choose y} \pi^y (1-\pi)^{n-y}\]</span></p>
<p>Start by writing out the log-likelihood, score, and information for this model. Note that the information matrix will only be a single value since we only have one parameter <span class="math inline">\(\pi\)</span> and that we will only have one observation <span class="math inline">\(Y\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
\ell(\pi \mid y) &amp; = \log \left( {n \choose y} \pi^y (1-\pi)^{n-y} \right) \\
&amp; = \log({n \choose y}) + y \log \pi + (n - y) \log (1-\pi) \\
&amp; \propto y \log( \pi / (1-\pi)) + n \log (1-\pi).\end{aligned} \]</span></p>
<p>$$U(y) = y() ( \frac{1-- (-)}{ )</p>
<p> (y) = $$</p>
<p>$$I() = -</p>
<p>Find the MLE of <span class="math inline">\(\pi\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
\frac{n \pi(1-2\pi)}{\pi^2(1-\pi)^2} + \frac{n \pi }{\pi (1-\pi)^2} &amp; =
\frac{n - 2n \pi + n \pi}{\pi (1-\pi)^2} \\
&amp; = \frac{n}{\pi (1-\pi)}
\end{aligned}
\]</span></p>
<p><span class="math display">\[U(\pi) = 0 \Rightarrow \frac{y}{n(1-\pi)} = \frac{n}{1-\pi} ...\]</span></p>
<p>Derive the Wald statistic using the definition.</p>
<p>Now invert this test statistic to obtain a <span class="math inline">\(1-\alpha\)</span> percentile Wald-style confidence interval.</p>
<p>What potential issue might arise when using a Wald test statistic?</p>
<p>Derive the Score test statistic using the definition.</p>
<p>Prove that inverting this test statistic to obtain a <span class="math inline">\(1-\alpha\)</span> percentile Score-style confidence interval yields …</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../week9/week9.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Week 9</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->



</body></html>