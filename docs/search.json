[
  {
    "objectID": "week1/week1.html#il-famoso-smoking-ra-fisher",
    "href": "week1/week1.html#il-famoso-smoking-ra-fisher",
    "title": "2  Week 1",
    "section": "il famoso Smoking RA Fisher",
    "text": "il famoso Smoking RA Fisher\nWe’ll talk about a lot of the methods that Ronald A. Fisher developed. Already in the 1900s it was being observed that there was a strong association between smoking and lung cancer. However, Fisher was a smoker himself and posited that the association between lung cancer and smoking could be explained away by some genetic or biological difference between the smoking and non-smoking population (positing some genes that caused people to desire to smoke).\n\n\n\n\n\nRonald Fisher’s unsupported theory of genetics confounding the smoking-lung cancer relationship\n\n\n\n\nWe’re pretty sure that this was driven not by any substance matter expertise, but rather by Fisher’s love of smoking."
  },
  {
    "objectID": "week1/week1.html#hormone-replacement-therapy",
    "href": "week1/week1.html#hormone-replacement-therapy",
    "title": "2  Week 1",
    "section": "Hormone Replacement Therapy",
    "text": "Hormone Replacement Therapy\nIn the mid- to late- 20th century there were a ton of studies linking hormone replacement therapy for older women to better cardiovascular outcomes (lack of coronary heart disease).\nHowever, thankfully due to the Heart and Estrogen/Progestin Study (HERS, in the early 90s) we now know that a lot of those studies were not controlling for socioeconomic status. It turns out that socioeconomic status was highly associated with HRT usage, and associated at least in the US with a lot of better health outcomes across the board.\nIt turned out that HRT when applied at certain times for some people can actually be harmful — but the point is the picture is much muddier than was initially thought and recommendations were rolled back. Later randomized studies were performed that produced reliable bodies of evidence demonstrating either no effect or in some cases harmful effects.\nWe’ll use the baseline data from HERS (not so much interested in the HRT treatment effect), but to investigate the research question:\n\nHow is systolic blood pressure related to age, independently of other well-known cardiovascular risk factors? (Age, diabetes, smoking, etc.)"
  },
  {
    "objectID": "week1/week1.html#prediction-studies",
    "href": "week1/week1.html#prediction-studies",
    "title": "2  Week 1",
    "section": "Prediction Studies",
    "text": "Prediction Studies\nTypically in prediction settings, there’s no single exposure of particular interest; mechanisms and confounding is treated as less of a concern (if at all), and the main challenge is that we need to take care to not overfit the data.\nA major theme of this class will be that different tasks require different analysis strategies and diffrent statistical tools."
  },
  {
    "objectID": "week1/week1.html#quantifying-uncertainty",
    "href": "week1/week1.html#quantifying-uncertainty",
    "title": "2  Week 1",
    "section": "Quantifying Uncertainty",
    "text": "Quantifying Uncertainty\nTypically standard statistical models have nice theoretical properties because years-and-years ago, we didn’t have much data so people spent their time studying theory instead of data. As a result, we have a lot of nice theories about the uncertainty represented in statistical models.\nAn example of the kind of uncertainty we might be interested in is shown in this figure relating Alzheimer’s disease rates and exposure to PM2.5.\n\n\n\n\n\n\n\n\n\nThis figure is taken from the article Long-term effects of PM2·5 on neurological disorders in the American Medicare population: a longitudinal cohort study by Shi et al, Lancet Planetary Health (2020)."
  },
  {
    "objectID": "week1/week1.html#why-learn-methods-before-study-design",
    "href": "week1/week1.html#why-learn-methods-before-study-design",
    "title": "2  Week 1",
    "section": "Why Learn Methods Before Study Design",
    "text": "Why Learn Methods Before Study Design\nAn interesting point made is that it’s important to understand the limitations, strengths of methods, what they can and can’t do, and how to use them before designing a study."
  },
  {
    "objectID": "week1/week1.html#recommended-reading",
    "href": "week1/week1.html#recommended-reading",
    "title": "2  Week 1",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nKutner M, Nachtsheim C, Neter J, Li W. Applied Linear Statistical Model. 5th edition. chapters 1-3\nShmueli, G. (2010). To explain or to predict? Statistical Science. https://www.stat.berkeley.edu/~aldous/157/Papers/shmueli.pdf"
  },
  {
    "objectID": "week1/week1.html",
    "href": "week1/week1.html",
    "title": "2  Week 1",
    "section": "",
    "text": "Linear Regression\nWe will generally be intereseted in the relationship between an outcome \\(Y\\) and \\(p\\) covariates or predictors denoted \\((x_1, ..., x_p)\\).\nWe generally say that a statistical model has a systematic component and a random component.\nWe often hypothesize that the “real” relationship between our outcome and predictors might be “super-complex”. Like cancer and environmental exposures might have complex dependencies on genetics, etc.. So sometimes instead of having a systematic component that captures variables in their full complexity, we might prioritize interpretability and we might be satisfied with an imperfect (but more intuitive, simpler) model that might give us some intuition about reality.\nThe random component may provide both a means to explain everything uncaptured by our predictors, as well as “real” randomness. For example, we might theorize that every time cells divide, there’s some small chance due to genetic drift that cells become metastatic and cancerous which is just random (did a mutation happen that was harmful in the right way in the right spot?)."
  },
  {
    "objectID": "week1/week1.html#hers-study-sbp-in-post-menopausal-women",
    "href": "week1/week1.html#hers-study-sbp-in-post-menopausal-women",
    "title": "2  Week 1",
    "section": "HERS Study: SBP in Post-Menopausal Women",
    "text": "HERS Study: SBP in Post-Menopausal Women\nIn a clinical trial of hormone therapy for preventing heart attacks and deaths among 2,763 post-menopausal women with existing coronary heart disease:\n\nthe outcome variable is systolic blood pressure\nthe data collected on covariates included age, diabetes diagnosis, smoking status, etc.\nthe research question was how is systolic blood pressure jointly related to age, statin use, and other risk factors in this cohort.\n\nReference: Vittinghoff et al., Regression methods in Biostatistics 2005 https://regression.ucsf.edu/regression-methods-biostatistics-linear-logistic-survival-and-repeated-measures-models"
  },
  {
    "objectID": "week1/week1.html#multiple-linear-regression-scalar-notation",
    "href": "week1/week1.html#multiple-linear-regression-scalar-notation",
    "title": "2  Week 1",
    "section": "Multiple Linear Regression: Scalar Notation",
    "text": "Multiple Linear Regression: Scalar Notation\nWe consider the model\n\\[Y_i = \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_p x_{ip} + \\epsilon_i, \\quad i = 1, ..., n.\\]\n\\[\\mathbb E(\\epsilon_i) = 0, \\, \\underbrace{\\text{Var}(\\epsilon_i) = \\sigma^2}_{\\text{homoscedasticity assumption}}, \\, \\text{ and Cov}(\\epsilon_i, \\epsilon_j) = 0.\\]\nTypically we assume that the predictors \\(x_i\\) are fixed and measured without error.\nWe require that \\(p < n\\).\nTJ notes that \\(\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0\\) doesn’t imply independence. This is the assumption we’re making for the time-being, but independence will be implied after we later assume that the errors are normally distributed.\nRoman asks if we need a conditional mean zero assumption — i.e., \\(\\mathbb E(\\epsilon | x) = 0\\)? Rachel notes that \\(\\epsilon\\) and \\(x\\) are assumed to be independent, so \\(\\mathbb E(\\epsilon | x) = \\epsilon\\). We will make stronger assumptions later, but we aren’t introducing those yet.\nBy taking the expected value of both sides, we can equivalently write that\n\\[\\mathbb E(Y_i) = \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_p x_{ip}.\\]\nThe parameters \\(\\beta_j\\) for \\(j = 1, ..., p\\) represent the change in the expected value \\(\\mathbb E(Y_i)\\) per unit change in \\(x_j\\) holding the remaining predictors \\(x_k \\, (k \\neq j)\\) constant.\nOne can see this by observing:\n\\[ \\mathbb E(Y_i | x_{i1} = x^* + 1) = \\beta_0 + \\beta_1 (x^* + 1) + ...\\] \\[ \\mathbb E(Y_i | x_{i1} = x^*) = \\beta_0 + \\beta_1 x^* + ...\\] \\[ \\mathbb E(Y_i | x_{i1} = x^* + 1) - \\mathbb E(Y_i | x_{i1} = x^*) = \\beta_1.\\]\nWe most often interested in testing whether \\(\\beta_j = 0\\), interpreted as a test of whether there’s an association between \\(x_j\\) and \\(Y\\)."
  },
  {
    "objectID": "week1/week1.html#vector-notation",
    "href": "week1/week1.html#vector-notation",
    "title": "2  Week 1",
    "section": "Vector Notation",
    "text": "Vector Notation\nWe can write this model more succinctly by writing:\n\\[ Y_i = x_i' \\beta + \\epsilon_i, \\quad i = 1, ..., n,\\]\nwhere \\(x_i = (1, x_{i1}, ..., x_{ip})',\\) and \\(\\beta = (\\beta_0, \\beta_1, ..., \\beta_p)'\\), \\(\\mathbb E(\\epsilon_i) = 0\\), \\(\\text{Var}(\\epsilon_i) = \\sigma^2\\), and \\(\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0\\)."
  },
  {
    "objectID": "week1/week1.html#examples",
    "href": "week1/week1.html#examples",
    "title": "2  Week 1",
    "section": "Examples",
    "text": "Examples\n\nlibrary(here)\n\nhere() starts at /Users/cht180/Documents/2023/MethodsBST232\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nhers <- readr::read_csv(here(\"data/hers.csv\"))\n\nRows: 2763 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): HT, raceth, nonwhite, smoking, drinkany, exercise, physact, globra...\ndbl (24): age, medcond, weight, BMI, waist, WHR, glucose, weight1, BMI1, wai...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nplt1 <- ggplot(hers %>% dplyr::sample_frac(.1), aes(x = age, y = SBP)) + \n  geom_point() + \n  theme_bw() + \n  ggtitle(\"Scatterplot of a 10% sample of our data\") \nplt1 \n\n\n\nplt2 <- plt1 + \n  stat_summary_bin(bins = 10, breaks = quantile(hers$age, seq(0,1, 0.1)), geom = 'line', fun = mean, color = 'cadetblue', size = 1.5) + \n  stat_summary_bin(bins = 10, breaks = quantile(hers$age, seq(0,1, 0.1)), geom = 'point', shape = 23, fun = mean, color = 'black', size = 3.5, fill = 'white') + \n  ggtitle(\"Now with overlain means for decile groups by age\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nplt2 \n\n\n\nplt3 <- \n  plt2 + \n  geom_smooth(method = 'lm', se = FALSE) + \n  ggtitle(\"Now with a linear model\") \nplt3 \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nlm.sbp.age <- lm(SBP ~ age, data = hers)\njtools::summ(lm.sbp.age)\n\n\n\n\n  \n    Observations \n    2763 \n  \n  \n    Dependent variable \n    SBP \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(1,2761) \n    77.21 \n  \n  \n    R² \n    0.03 \n  \n  \n    Adj. R² \n    0.03 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    103.63 \n    3.60 \n    28.82 \n    0.00 \n  \n  \n    age \n    0.47 \n    0.05 \n    8.79 \n    0.00 \n  \n\n\n Standard errors: OLS\n\n\n\nSo now we would say that \\(\\hat{\\mathbb E}(SBP_i) = 103.63 + 103.63 Age_i\\). Though, the intercept is kind of useless since our model doesn’t have any observations for systolic blood pressure for age 0 infants. We might want to fit an age centered model.\n\nlm.sbp.agec <- lm(SBP ~ I(age - mean(age)), data = hers)\njtools::summ(lm.sbp.agec)\n\n\n\n\n  \n    Observations \n    2763 \n  \n  \n    Dependent variable \n    SBP \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(1,2761) \n    77.21 \n  \n  \n    R² \n    0.03 \n  \n  \n    Adj. R² \n    0.03 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    135.07 \n    0.36 \n    378.24 \n    0.00 \n  \n  \n    I(age - mean(age)) \n    0.47 \n    0.05 \n    8.79 \n    0.00 \n  \n\n\n Standard errors: OLS\n\n\n\nIn which case we’d say that \\(\\hat{\\mathbb E}(SBP_i) = 135.07 + 135.07 Age_i\\)."
  },
  {
    "objectID": "week1/week1.html#multiple-linear-regression",
    "href": "week1/week1.html#multiple-linear-regression",
    "title": "2  Week 1",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\n# fitting a p=2 model\nlm.sbp.age.weight <- lm(SBP ~ age + weight, data = hers)\nx <- y <- seq(0, 100, length= 30)\nf <- function(x,y){ z <- x*coef(lm.sbp.age.weight)[2] + y*coef(lm.sbp.age.weight)[3] + coef(lm.sbp.age.weight)[1] }\nz <- outer(x,y,f)\npersp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = \"lightblue\", xlab = \"age\", ylab = \"weight\", zlab = \"expected SBP\", ticktype = 'detailed', nticks = 4)"
  },
  {
    "objectID": "week1/week1.html#indicator-dummy-variables",
    "href": "week1/week1.html#indicator-dummy-variables",
    "title": "2  Week 1",
    "section": "Indicator / Dummy Variables",
    "text": "Indicator / Dummy Variables\nWe might be interested in modeling categorical variables as well. To do so, we would include dummy variables.\nIf a categorical variable has \\(K\\) levels, then we’ll need to compute \\(K-1\\) dummy variables where the omitted variable is called the “reference level”.\n\nunique(hers$physact)\n\n[1] \"much more active\"     \"much less active\"     \"about as active\"     \n[4] \"somewhat less active\" \"somewhat more active\"\n\nhers$physact <- as.factor(hers$physact) \n# the reference category will be \"much more active\" since it's the \n# first factor level — \n# if we wanted to change it, we could run: \n# hers$raceth <- relevel(hers$raceth, \"much less active\") \n# for example. \nlm.sbp.physact <- lm(SBP ~ physact, data = hers)\nhead(model.matrix(lm.sbp.physact))\n\n  (Intercept) physactmuch less active physactmuch more active\n1           1                       0                       1\n2           1                       1                       0\n3           1                       0                       0\n4           1                       1                       0\n5           1                       0                       0\n6           1                       0                       0\n  physactsomewhat less active physactsomewhat more active\n1                           0                           0\n2                           0                           0\n3                           0                           0\n4                           0                           0\n5                           1                           0\n6                           0                           0\n\njtools::summ(lm.sbp.physact)\n\n\n\n\n  \n    Observations \n    2763 \n  \n  \n    Dependent variable \n    SBP \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(4,2758) \n    0.48 \n  \n  \n    R² \n    0.00 \n  \n  \n    Adj. R² \n    -0.00 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    135.00 \n    0.63 \n    215.01 \n    0.00 \n  \n  \n    physactmuch less active \n    0.07 \n    1.49 \n    0.05 \n    0.96 \n  \n  \n    physactmuch more active \n    -0.90 \n    1.26 \n    -0.72 \n    0.47 \n  \n  \n    physactsomewhat less active \n    0.96 \n    1.06 \n    0.91 \n    0.36 \n  \n  \n    physactsomewhat more active \n    -0.05 \n    0.91 \n    -0.05 \n    0.96 \n  \n\n\n Standard errors: OLS\n\n\n\nIn machine learning this is called “one-hot” encoding."
  },
  {
    "objectID": "week1/week1.html#polynomial-regressions",
    "href": "week1/week1.html#polynomial-regressions",
    "title": "2  Week 1",
    "section": "Polynomial Regressions",
    "text": "Polynomial Regressions\n\nlm.sbp.agepolynomial <- lm(SBP ~ poly(age), data = hers)\n\n# we could plug in coef(lm.sbp.agepolynomial)[1] through \n# coef(lm.sbp.agepolynomial)[4] into the following equation, \n# but it's kind of boring — instead here's a more fun \n# looking cubic polynomial we could imagine being the result of\n# a polynomial regression:\ncurve(1 - 2.8*x + 4*(x^2) - .65*(x^3), from = 0, to = 5,\n      xlab = expression(x[1]),\n      ylab = \"E(Y)\")\n\n\n\n\nIt’s worth emphasizing that usually we prefer splines or generalized additive models to polynomial regression these days since polynomial regression can act strangely."
  },
  {
    "objectID": "week1/week1.html#marginal-associations",
    "href": "week1/week1.html#marginal-associations",
    "title": "2  Week 1",
    "section": "Marginal Associations",
    "text": "Marginal Associations\nIf we have one binary predictor \\(x_1\\) and one continuous \\(x_2\\), we might be interested in the marginal association between \\(Y\\) and \\(x_2\\). This would look like a model fit with lm(Y ~ x2) and \\(x_1\\) is not included."
  },
  {
    "objectID": "week1/week1.html#conditional-association",
    "href": "week1/week1.html#conditional-association",
    "title": "2  Week 1",
    "section": "Conditional Association",
    "text": "Conditional Association\nWe might also want to fit models that include \\(x_1\\), so those could be fit with lm(Y ~ x1 + x2) and this would include an effect for \\(x_2\\) (one effect, not multiple) that is applied after considering an effect for \\(x1\\)."
  }
]