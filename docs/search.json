[
  {
    "objectID": "week12/week12.html#continuing-on-link-functions",
    "href": "week12/week12.html#continuing-on-link-functions",
    "title": "Week 12",
    "section": "Continuing on Link Functions",
    "text": "Continuing on Link Functions\n\nLinear (identity) link function\nWhen we use the identity link function with binary outcomes, we call this a linear probability model.\n\\[ \\mu_i = \\beta_0 + \\beta_1 x_i \\]\nThe contrast modeled is the risk difference (RD).\n\\(\\beta_0\\) is the probability of response when \\(x = 0\\); \\(\\beta_1\\) is the change in probability of response when \\(x\\) differs by 1 unit.\nWe’ve noted before that this doesn’t respect the \\([0,1]\\) bounds on the response probability.\nWe could also write \\(\\beta_1 = \\mu[x^\\star + 1] - \\mu[x^\\star]\\).\n\n\nLog link function\nSuppose we choose the log link. The systematic component of the model is\n\\[\\log(\\mu_i) = \\beta_0 + \\beta_1 x_i\\]\nWe interpret \\(\\beta_0\\) as the log of the probability of response when \\(x = 0\\). \\(\\exp(\\beta_0)\\) is the probability of response when \\(x=0\\).\nWe interpret \\(\\beta_1\\) as the change in the log of the probability of response, comparing two populations whose values of \\(x\\) differs by 1 unit.\n\\(\\exp(\\beta_1)\\) is the ratio of the probability of response when comparing \\(x^\\star + 1\\) to \\(x^\\star\\).\n\\[\\beta_1 = \\log(\\mu[x^\\star + 1]) - \\log(\\mu[x^\\star]) = \\log(\\frac{\\mu[x^\\star + 1]}{\\mu[x^\\star]}),\\] \\[\\exp(\\beta_1) = \\frac{\\mu[x^\\star + 1]}{\\mu[x^\\star]}.\\]\nThe contrast we’re modeling here is the risk ratio.\nThe log link doesn’t necessarily respect the fact that the true response probability has an upper bound of 1. We can see this by considering the inverse of the link function: \\[\\mu_i = \\exp\\{ \\beta_0 + \\beta_1 x_i \\}\\] which takes values on \\((0, \\infty)\\).\n\n\nLogit link function\nThe logit link is the canonical link function for binary outcomes, and the GLM for Bernoulli outcomes using the logit link is known as logistic regression. The systematic component is:\n\\[\\text{logit}(\\mu_i) = \\log \\left( \\frac{\\mu_i}{1-\\mu_i} \\right) = \\beta_0 + \\beta_1 x_i. \\]\nThe functional \\[\\frac{\\mu_i}{1-\\mu_i} = \\frac{P(Y_i = 1 \\mid x_i)}{P(Y_i = 0 \\mid x_i)}\\]\nis the odds of response.\nWe interpret \\(\\beta_0\\) as the log of the odds of response when \\(x = 0\\). \\(\\exp(\\beta_0)\\) is the odds of response when \\(x = 0\\).\n\\[\\beta_1 = \\log\\left( \\frac{\\mu[x^\\star + 1]}{1-\\mu[x^\\star+1]}\\right) - \\log \\left( \\frac{\\mu[x^\\star]}{1 - \\mu[x^\\star]} \\right).\\]\n\\[\\beta_1 = \\log \\left( \\frac{\\text{odds}(x^\\star+1)}{\\text{odds}(x^\\star)} \\right)\\]\nWe typically report \\(\\exp(\\beta_1)\\), which is the ratio of the odds when \\(x = x^\\star + 1\\) to when \\(x = x^\\star\\).\nThe inverse of the logit link function is called the expit function:\n\\[\\mu_i = \\frac{\\exp \\{ \\beta_0 + \\beta_1 x_i \\} }{1 + \\exp \\{ \\beta_0 + \\beta_1 x_i \\} }.\\]\n\n\nInverse CDFs as link functions for binary outcomes\nThe logit function (the link for logistic regression) is the inverse CDF of the standard logistic distribution.\nThe CDF (of any distribution) provides a mapping from th support of the random variable to the \\((0,1)\\) interval.\n\\[\\mathbb{F}_X(\\cdot) : (-\\infty, \\infty) \\to (0,1)\\]\nIt’s relatively common to use inverse CDFs as link functions.\n\\[\\mathbb{F}^{-1}_X(\\cdot) : (0,1) \\to (-\\infty, \\infty)\\]\n\\(g(\\cdot) \\equiv \\mathbb F^{-1}(\\cdot)\\) maps \\(\\mu \\in (0,1)\\) to \\(\\eta \\in (-\\infty, \\infty)\\).\n\n\nProbit Link Function\n\\[\\text{probit}(\\mu_i) = \\Phi^{-1}(\\mu_i) = \\beta_0 + \\beta_1 x_i\\]\nwhere \\(\\Phi(\\cdot)\\) is the CDF of the standard normal distribution.\nInterpret \\(\\beta_0\\) as the probit of probability of response when \\(x = 0\\).\nInterpret \\(\\beta_1\\) as the change in the probit of the probability of response, comparing two populations whose values of \\(x\\) differs by 1 unit.\nInterpretation is tricky:\n\nContrast is in terms of the inverse CDF of a standard normal distribution.\nThere is no easy way of relating this contrast to more intuitive measures.\n\n\\(\\beta_1\\) can be viewed as changes in \\(z\\)-scores in the response probability.\n\n\nComplementary log-log\n\\[\\log(-\\log(1-\\mu_i)) = \\beta_0 + \\beta_1 x_i\\]\nInverse CDF of the extreme value (or log-Weibull) distribution.\nAs with the probit function, there isn’t any intuitive way of interpreting regression parameters based on this link function.\nHas the distinction that it is asymmetric.\n\n\nComparing Links\n\nModels using logit and probit links should give very similar predictions.\nShown by Amemiya (1981) there is an approximate relationship between the coefficients from models using the logit and probit links:\n\\[\\beta_1^{\\text{logit}} \\approx 1.6 \\beta_1^{\\text{probit}}\\]\nAmemiya (1981) also shows that when \\(\\mu \\in (0.3,0.7)\\), there is an approximate relationship between the coefficients from models using the logit and linear links:\n\\[\\beta_1^{\\text{logit}} \\approx \\frac{\\beta_1^{\\text{\\linear}}}{4}.\\]\nThis might come from a relatively simple Taylor expansion of the expit function.\n\nWhen working with rare outcomes, the log, logit, and complementary log-log links give us pretty much the same things. When working with more common outcomes, the different links give us quite different outcomes.\nFrom the figures, differences across these link functions manifest primarily in the tails when probability of response is small or large.\nAlso the logit and probit functions are almost linearly related.\nFor small values of \\(\\mu_i\\), the complementary log-log, logit, and log functions are close to each other.\n\nEqually good for rare events\nFor \\(\\mu_i \\leq 0.1\\) \\[\\log \\left( \\frac{\\mu_i}{1-\\mu_i} \\right) \\approx \\log(\\mu_i) \\]\nlog link has the best interpretation\nOR and RR are close numerically\n\nWhat people often do is fit logistic regression, estimate an odds ratio, and then say because the OR is a rare outcome, it’s also approximately a rate ratio/relative risks. Why not just fit the log-link model? It may be computationally challenging/tricky."
  },
  {
    "objectID": "week10/week10.html",
    "href": "week10/week10.html",
    "title": "Week 10",
    "section": "",
    "text": "Generalized Linear Models\nOur goal is to develop statistical models to characterize the relationship between some response variable \\(Y\\) and a vector of covariates \\(x\\).\nStatistical models consist of two components:\nWhen moving beyond linear regression analysis of continuous and response data, we need to be aware of two key challenges: * Sensible specification of the systematic component * Proper accounting of any implicit mean-variance relationships arising from the random component.\nDefinition of a Generalized Linear Model\nA generalized linear model (GLM) specifies a parametric statistical model for the conditional distribution of a response \\(Y_i\\) given a \\(p\\)-vector of covariates \\(x_i\\).\nConsistes of three elements:\nThe first of these elements is the random component, and elements 2 and 3 jointly specify the systematic component.\nIn practice, we see a wide range of response variables with a wide range of associated (possible) distributions\nFor a given choice of probability distribution, a GLM specifies a model for the conditional mean:\n\\[ \\mu_i = \\mathbb E[Y_i | x_i] \\]\nHow do we specify a reasonable model for \\(\\mu_i\\) while ensuring that we respect the appropriate range/scale of \\(\\mu_i\\)?\nAchieved by constructing a linear predictor \\(x_i' \\beta\\) and relating it to \\(\\mu_i\\) via a link function \\(g(\\cdot)\\):\n\\[g(\\mu_i) = x_i' \\beta.\\]\nWe often specify \\(g(\\cdot)\\) such that \\(g^{-1}(x_i' \\beta) = \\mu\\) respects the appropriate bounds on \\(\\mu_i\\).\nWe’ll sometimes use the shorthand \\(\\eta_i = x_i' \\beta\\).\nThe MLE of \\(\\theta\\), denoted \\(\\hat \\theta_{MLE}\\) is obtained by solving the score equations with respect to \\(\\theta\\):\n\\[U(\\theta | y) = \\frac{\\partial}{\\partial \\theta} \\ell (\\theta | y) = 0\\]\nUnder some regularity conditions,\n\\[\\hat \\theta_{MLE} \\sim \\text{MVN}_p (\\theta, \\mathcal I_n(\\theta)^{-1})\\]\nwhere \\(\\mathcal I_n(\\theta)\\) is the Fisher information matrix with\n\\[\\mathcal I_n(\\theta)_{j,k} = -\\mathbb E\\left[ \\frac{\\partial^2 \\ell (\\theta | y)}{\\partial \\theta_j \\partial \\theta_k \\right]\\]\nWe can estimate \\(\\text{Var}(\\hat \\theta_{MLE}) \\approx \\mathcal I_n(\\theta)^{-1}\\) by plugging in \\(\\hat \\theta_{MLE}\\).\nThe Wald test is measuring the distance between the MLE and the null value.\n\\[(\\hat \\theta_{1,MLE} - \\theta_{1,0})^{T} \\widehat{\\text{Var}}(\\hat \\theta_{1,MLE})^{-1} (\\hat \\theta_{1,MLE} - \\theta_{1,0}) \\stackrel{d}{\\longrightarrow} \\chi_q^2 \\]\nThe Score test finds the first derivative of the log likelihood at the null value. If this derivative is large, then that means that this null value is far away from the MLE.\n\\[U(\\hat \\theta_{0,MLE} | y)' \\mathcal I_n(\\hat \\theta_{0,MLE})^{-1} U(\\hat \\theta_{0, MLE} | y)  \\stackrel{d}{\\longrightarrow} \\chi_q^2 \\]\nThe Likelihood ratio test\n\\[2 \\left( \\ell(\\hat \\theta_{MLE} | y) - \\ell(\\hat \\theta_{0,MLE}|y) \\right)  \\stackrel{d}{\\longrightarrow} \\chi_q^2 \\]\nAsymptotically, these should all give the same results, but in small data settings these may differ.\nThe Fisher information is essentially the negative second derivative of the likelihood function. Since the variance of \\(\\hat \\theta_{MLE}\\) can be estimated by \\(\\approx \\mathcal I_n(\\theta)^{-1}\\), if the likelihood function is a very steep (downward) parabola, its second derivative will have large magnitude, and its inverse will thus be small.\nThe systematic component is the specification of how the distribution \\(Y \\sim f_Y(y; \\theta, \\phi)\\) (where \\(f_Y\\) is an exponential family distribution) depends on the covariates \\(x_i\\).\nIn GLMs we model the conditional mean \\(\\mu_i = \\mathbb E[Y_i | x_i].\\)\nThis provides a connection between \\(x_i\\) and the distribution of \\(Y_i\\) via the canonical parameter \\(\\theta_i\\) and the cumulant function \\(b(\\theta_i)\\).\n\\[f_Y(y) \\longrightarrow \\theta_i \\longrightarrow \\mu_i \\longrightarrow x_i'\\beta \\quad \\text{(a.k.a. }\\eta_i)\\]\nTypically we link the linear predictor of the distribution of \\(Y\\) via a transformation of \\(\\mu_i\\), \\(g(\\cdot)\\), so that\n\\[g(\\mu_i) = x_i'\\beta\\]\nTraditionally this is broken into two parts:\nSometimes you’ll find the ‘predictor component’ of the model called the ‘systematic component’, as in McCullough and Nelder (1989). In practice, one cannot consider one without the other since the relationship between \\(\\mu_i\\) and \\(x_i\\) are jointly determined by \\(\\beta\\) and \\(g(\\cdot)\\).\nConstructing the linear predictor for a GLM follows the same process one uses for linear regression.\nGiven a set of covariates \\(x_i\\), there are two decisions.\nFor the most part, decisions to include covariates should be driven by scientific considerations\nWe will see that there are some differences (from multiple linear regression) when it comes to identification of confounders.\nIn the GLM framework, we’ll want to note that the inverse of the link function provides the specification of the model on the scale of \\(\\mu_i\\)\n\\[\\mu_i = g^{-1}(x_i' \\beta)\\]\nso we often choose \\(g()\\) such that \\(g^{-1}(x_i' \\beta)\\) respects any bounds on \\(\\mu_i\\).\nWe interpret the link function as specifying a transformation of the conditional mean, \\(\\mu_i\\).\nWe are not specifying a transformation of the response \\(Y_i\\).\nWe are writing \\(g(\\mathbb E(Y_i|x_i))\\), not \\(\\mathbb E(g(Y_i) | x_i) = x_i'\\beta\\).\nRecall that the mean and the canonical parameter are linked via the derivative of the cumulant function.\n\\[\\mathbb E[Y_i] = \\mu_i = b'(\\theta_i)\\]\nAn important link function is the canonical link:\n\\[g(\\mu_i) = \\theta(\\mu_i) = b'^{-1}(\\mu_i),\\]\ni.e., the function that results by viewing the canonical parameter \\(\\theta_i\\) as a function of \\(\\mu_i\\).\nWe’ll see later that this choice results in some mathematical convenience.\nGiven an i.i.d. sample of size \\(n\\), the log-likelihood is\n\\[\\ell(\\beta, \\phi; y) = \\sum_{i=1}^n \\left[ \\frac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c(y_i, \\phi)\\right]\\]\nThroughout this course, we’ve discussed “asymptotic” properties of estimators, including consistency \\((\\hat \\theta \\stackrel{p}{\\to} \\theta)\\). We’ve also discussed asymptotic tests — for example, F-tests which converge to a \\(\\chi^2\\) distribution in large samples. However, previous discussions have been limited to models involving normally distributed rnadom variables.\nAsymptotic likelihood theory provides a framework by which we can construct estimators with optimal properties for arbitrary distributions, as well as asymptotic tests based on those estimators. Consider an analysis of a random sample such that\n\\[Y_1, ..., Y_n \\stackrel{iid}{\\sim} f(y_i \\mid \\theta)\\]\nand we are interested in drawing inference \\(\\theta\\) (which may be a vector of parameters). Let \\(\\mathbf{Y} = (Y_1, ..., Y_n)\\). Then we can define several components of a likelihood model:\nThe likelihood:\n\\[\\mathcal L(\\theta \\mid \\mathbf{Y}) = f(y \\mid \\theta) = \\prod_{i=1}^n f(y_i \\mid \\theta)\\]\nLog-likelihood:\n\\[\\ell(\\theta \\mid y) = \\log \\mathcal L(\\theta \\mid y) = \\sum_{i=1}^n \\log f(y_i \\mid \\theta)\\]\nScore:\n\\[U(\\theta \\mid y) = \\frac{\\partial}{\\partial \\theta} \\ell (\\theta \\mid y) = \\left( \\frac{\\partial}{\\partial \\theta_1} \\ell(\\theta \\mid y), ..., \\frac{\\partial}{\\partial \\theta_1} \\ell(\\theta \\mid y) \\right)\\]"
  },
  {
    "objectID": "week10/week10.html#exponential-family",
    "href": "week10/week10.html#exponential-family",
    "title": "Week 10",
    "section": "Exponential Family",
    "text": "Exponential Family\nGLMs form a class of statistical models for response variables whose distribution belongs to the exponential dispersion family\nThese are the family of distributions with a pdf/pmf of the form:\n\\[f_Y(y; \\theta, \\phi) = \\exp \\left\\{ \\frac{y \\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi) \\right\\},\\]\nwhere \\(\\theta\\) is the canonical parameter, \\(\\phi\\) is the dispersion parameter, and \\(b(\\theta)\\) is the cumulant function.\nWe will see that \\(\\theta\\) is always a function of the conditional mean, \\(\\mu_i\\).\nNote that the \\(c(y, \\phi)\\) should not depend on \\(\\theta\\).\n\nBernoulli in the Exponential Notation\nLet \\(Y \\sim \\text{Bernoulli}(\\mu)\\).\nA common first step is to apply a convenient transformation that is equivalent to the identity function: \\(\\exp(\\log(\\cdot))\\).\n\\[\n\\begin{aligned}\nf_Y(y ; \\mu ) & = \\mu^y (1- \\mu)^{1-y} \\\\\n& = \\exp \\{ y \\log(\\mu) + (1-y) \\log (1-\\mu) \\} \\\\\n& = \\exp \\left\\{ y \\log \\left( \\frac{\\mu}{1-\\mu} \\right)  + \\log(1-\\mu) \\right\\}\n\\end{aligned}\n\\]\nLet\n\\[\n\\begin{aligned}\n\\theta = \\log\\left( \\frac{\\mu}{1-\\mu} \\right) \\quad \\quad & b(\\theta) = \\log (1 + \\exp \\{ \\theta \\}) \\\\\na(\\phi) = 1 \\quad \\quad & c(y, \\phi) = 0\n\\end{aligned}\n\\]\nThen \\[\n\\begin{aligned}\nf_Y(y; \\theta, \\phi) & = \\exp \\{ y \\theta - \\log (1 + \\exp \\{ \\theta \\} ) \\} \\\\\n& = \\exp \\left\\{ \\frac{y \\theta - b(\\theta) }{a(\\phi)} + c(y, \\phi) \\right\\}.\n\\end{aligned}\n\\]\nMany other common distributions are members of this faimly. The canonical parameter \\(\\theta\\) has key relationships with both \\(\\mathbb E(Y)\\) and \\(\\text{Var}(Y)\\). Typically varies across study units since \\(\\mathbb E(Y_i) = \\mu_i\\). We also index \\(\\theta\\) by \\(i\\), as in \\(\\theta_i\\).\nThe dispersion parameter \\(\\phi\\) has a key relationship with \\(\\text{Var}(Y)\\). It may, but does not typically, vary across study units. Typically it is not unit-specific, so we just write \\(\\phi\\). In some settings, we may have \\(a(\\cdot)\\) vary with \\(i\\), as in $\\(a_i(\\phi)\\). E.g., \\(a_i(\\phi) = \\phi / w_i\\) where \\(w_i\\) is the prior weight.\nWhen the dispersion parameter is known, some may say that this distribution is a member of the natural exponential family.\nConsider the likelihood function for a single observation\n\\[\\mathcal L(\\theta_i, \\phi ; y_i) = \\exp \\left\\{ \\frac{y_i \\theta - b(\\theta_i)}{a_i(\\phi)} + c(y_i, \\phi) \\right\\}.\\]\nThe log-likelihood is\n\\[\\ell(\\theta_i, \\phi; y_i) = \\frac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c(y_i, \\phi).\\]\nThe first partial derivative with respect to \\(\\theta_i\\) is the score function for \\(\\theta_i\\) and is given by\n\\[\\frac{\\partial}{\\partial \\theta_i} \\ell (\\theta_i, \\phi; y_i) = U(\\theta_i) = \\frac{y_i - b'(\\theta_i)}{a_i(\\phi)}.\\]\nNote that we consider \\(\\frac{\\partial}{\\partial \\theta_i} \\ell (\\theta_i, \\phi; y_i)\\) for the purpose of showing properties of exponential families (not for doing estimation).\nWe know that (under some regularity conditions)\n\\[\n\\begin{aligned}\n\\mathbb E[U(\\theta_i)] & = 0 \\\\\n\\text{Var}[U(\\theta_i)] & = \\mathbb E[U(\\theta_i)^2] = \\mathbb E\\left[ \\frac{\\partial U(\\theta_i)}{\\partial \\theta_i} \\right]\n\\end{aligned}\n\\]\nWe will use these properties to get expressions for \\(\\mathbb E(Y_i)\\) and \\(\\text{Var}(Y_i)\\) in terms of the exponential family parameters.\nSince the score has mean zero,\n\\[\\mathbb E(U(\\theta_i)) = \\mathbb E\\left[ \\frac{Y_i - b'(\\theta_i)}{a_i(\\phi)} \\right] = 0 \\]\nand, consequently,\n\\[\\mu_i = \\mathbb E[Y_i] = b'(\\theta_i).\\]"
  },
  {
    "objectID": "week10/week10.html#exponential-family-log-likelihood-and-score",
    "href": "week10/week10.html#exponential-family-log-likelihood-and-score",
    "title": "Week 10",
    "section": "Exponential Family Log-Likelihood and Score",
    "text": "Exponential Family Log-Likelihood and Score\nConsider the likelihood function for a single observation\n\\[\\mathcal L(\\theta_i, \\phi; y) = \\exp \\{ \\frac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c(y_i, \\phi) \\}\\]\n\\[\\frac{partial}{\\partial \\theta_i} \\ell(\\theta_i, \\phi; y_i) = U(\\theta_i) = \\frac{y_i - b'(\\theta_i)}{a_i(\\phi)}\\]\nFrom previously,\n\\[\\mathbb E[U(\\theta_i)] = 0\\]\n\\[\\text{Var}[U(\\theta_i)] = \\mathbb E[U(\\theta_i)^2] = -\\mathbb E\\left[ \\frac{\\partial U(\\theta_i)}{\\partial \\theta_i} \\right]\\]\nSince this score has mean zero,\n\\[\\mathbb E(U(\\theta_i)) = \\mathbb E(\\frac{Y_i - b'(\\theta_i)}{a_i(\\phi)}) = 0\\]\nand hence\n\\[\\mu_i = \\mathbb E[Y_i] = b'(\\theta_i).\\]"
  },
  {
    "objectID": "week10/week10.html#variance-in-exponential-families",
    "href": "week10/week10.html#variance-in-exponential-families",
    "title": "Week 10",
    "section": "Variance in Exponential Families",
    "text": "Variance in Exponential Families\nThe second partial derivative is\n\\[\\frac{\\partial^2}{\\partial \\theta_i^2} \\ell(\\theta_i, \\phi; y) = - \\frac{b''(\\theta_i)}{a_i(\\phi)}\\]\nUsing the above properties, it follows that\n\\[\\text{Var}(U(\\theta_i)) = \\text{Var}\\left[ \\frac{Y_i - b'(\\theta_i)}{a_i(\\phi)} \\right] = \\frac{b''(\\theta_i)}{a_i(\\phi)}\\]\nso that\n\\[\\text{Var}[Y_i] = b''(\\theta_i)a_i(\\phi)\\]\nThe variance of \\(Y_i\\) is therefore a function of both \\(\\theta_i\\) and \\(\\phi\\).\nNote that the canonical parameter is a function of \\(\\mu_i\\)\n\\[\\mu_i = b'(\\theta_i) \\Rightarrow \\theta_i = b'^{-1}(\\mu_i) = \\theta(\\mu_i)\\]\nso that we can write\n\\[\\text{Var}[Y_i] = b''(\\theta_i)a_i(\\phi) = b''(\\theta(\\mu_i))a_i(\\phi)\\]\nThe function \\(V(\\mu_i) = b''(\\theta(\\mu_i))\\) is called the variance function.\nThe specific form indicates the nature of the (if any) mean-variance relationship.\n\nBernoulli Example\nLet \\(Y \\sim \\text{Bernoulli}(\\mu)\\).\n\\[\\theta = \\log \\left( \\frac{\\mu}{1-\\mu} \\right), \\quad \\quad \\text{ the logit of } \\mu\\]\n\\[a(\\phi) = 1\\]\n\\[b(\\theta) = \\log(1 + \\exp \\theta)\\]\n\\[\\mathbb E[Y] = b'(\\theta) = \\frac{\\exp \\theta }{1 + \\exp \\theta} = \\mu, \\quad \\quad \\text{ the expit of } \\theta\\]\n\\[\\text{Var}[Y] = b''(\\theta)a(\\phi) = \\frac{\\exp \\theta}{(1 + \\exp \\theta)^2} = \\mu(1 - \\mu) = V(\\mu)\\]"
  },
  {
    "objectID": "week10/week10.html#estimation",
    "href": "week10/week10.html#estimation",
    "title": "Week 10",
    "section": "Estimation",
    "text": "Estimation\nThere are \\((p+2)\\) unknown parameters: \\((\\beta, \\phi)\\).\nTo obtain the MLE we need to solve the score equations:\n\\[U(\\beta, \\phi | y) = \\left( \\frac{\\partial \\ell}{\\parital \\beta_0}, ..., \\frac{\\partial \\ell}{\\partial \\beta_p}, \\frac{\\partial \\ell}{\\partial \\phi} \\right)' = 0\\]\nThis is a system of \\(p+2\\) equations. The contribution to the score for \\(\\phi\\) is the \\(i^{\\mathrm{th}}\\) unit is \\[\\frac{\\partial \\ell}{\\partial \\phi} = - \\frac{a_i'(\\phi)}{a_i(\\phi)^2}(y_i \\theta_i - b(\\theta_i)) + c'(y_i, \\phi)\\]\nWe can use the chain rule to obtain a convenient expression for the \\(i^{\\mathrm{th}}\\) contribution to the score function for \\(\\beta_j\\):\n\\[\\frac{\\partial \\ell}{\\partial \\beta_j} = \\frac{\\partial \\ell}{\\partial \\theta_i} \\times \\frac{\\partial \\theta_i}{\\partial \\mu_i} \\times \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\times \\frac{\\partial \\eta_i}{\\partial \\beta_j}\\]\nNote the following results:\n\\[\\frac{\\partial \\ell}{\\partial \\theta_i} = \\frac{y_i - b'(\\theta_i)}{a_i(\\phi)} = \\frac{y_i-\\mu_i}{a_i(\\phi)}\\]\n\\[\\frac{\\partial \\theta_i}{\\partial \\mu_i} = \\left( \\frac{\\partial \\mu_i}{\\partial \\theta_i} \\right)^{-1} = (b''(\\theta_i))^{-1} = (V(\\mu_i))^{-1}\\]\n\\[\\frac{\\partial \\eta_i}{\\partial \\beta_j} = x_{ij}\\]\nThe score function for \\(\\beta_j\\) can therefore be written as \\[\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{x_{ij}}{V(\\mu_i)a_i(\\phi)} (y_i - \\mu_i)\\]\nSuppose \\(a_i(\\phi) = \\phi / w_i\\). The score equations become\n\\[\\frac{\\partial \\ell}{\\partial \\phi} = \\sum_{i=1}^n - \\frac{w_i ( y_i \\theta_i - b(\\theta_i))}{\\phi^2} + c' (y_i, \\phi) = 0\\]\n\\[\\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n w_i \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{x_{ij}}{V(\\mu_i)} (y_i - \\mu_i) = 0\\]\nNotice that the \\((p+1)\\) score equations for \\(\\beta\\) do not depend on \\(\\phi\\).\nConsequently, obtaining the MLE of \\(\\beta\\) doesn’t require knowledge of \\(\\phi\\).\n\\(\\phi\\) isn’t required to be known or estimated (if unknown).\nFor exmaple, in linear regression, we don’t need \\(\\sigma^2\\) (or \\(\\hat \\sigma^2\\)) to obtain \\[\\hat \\beta_{MLE} = (X^T X)^{-1} X^T Y\\]\nInference does require an estimate of \\(\\phi\\)."
  },
  {
    "objectID": "week10/week10.html#asymptotic-sampling-distribution",
    "href": "week10/week10.html#asymptotic-sampling-distribution",
    "title": "Week 10",
    "section": "Asymptotic Sampling Distribution",
    "text": "Asymptotic Sampling Distribution\nSubject to appropriate regularity conditions,\n\\[\n\\begin{bmatrix}\n\\hat \\beta_{MLE} \\\\\n\\hat \\phi_{MLE}\n\\end{bmatrix} \\stackrel{\\cdot}{\\sim}\n\\text{MVN} \\left(\n  \\begin{bmatrix}\n  \\beta \\\\\n  \\phi\n  \\end{bmatrix},\n\\mathcal I_n(\\beta, \\phi)^{-1}\n\\right)\n\\]\nNow we can compute the components of \\(\\mathcal I_n(\\beta, \\phi):\\)\n\\[\\begin{aligned}\n\\frac{\\partial^2 \\ell(\\beta, \\phi; y_i)}{\\partial \\beta_j \\partial \\beta_k} & =\n\\frac{\\partial}{\\partial \\beta_k} \\overbrace{\\left\\{ \\frac{\\partial \\mu_i}{\\partial \\eta_i}\n\\frac{x_{ij}}{V(\\mu_i)a_i(\\phi)} (y_i - \\mu_i)\n\\right\\}}^{\\text{score for } \\beta_j} \\\\\n& = (y_i-\\mu_i) \\frac{\\partial}{\\partial \\beta_k} \\left\\{ \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{x_{ij}}{V(\\mu_i) a_i(\\phi)} \\right\\} - \\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right)^2 \\frac{x_{ij} x_{ik}}{V(\\mu_i)a_i(\\phi)}\n\\end{aligned}\\]\nAnd hence\n\\[-\\mathbb E\\left[ \\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k} \\right] = \\sum_{i=1}^n \\left(\n  \\frac{\\partial \\mu_i}{\\partial \\eta_i}\n  \\right)^2\n  \\frac{x_{ij}x_{ik}}{V(\\mu_i)a_i(\\phi)}\n\\]\n\\[ \\begin{aligned}\n\\frac{\\partial^2 \\ell(\\beta, \\phi; y_i)}{\\partial \\phi \\partial \\phi} & =\n\\frac{\\partial}{\\partial \\phi} \\overbrace{\\left\\{\n  - \\frac{a_i'(\\phi)}{a_i(\\phi)^2} (y_i \\theta_i - b(\\theta_i)) + c'(y_i, \\phi)\n\\right\\}}^{\\text{score for } \\phi}  \\\\\n& = - \\left\\{ \\frac{a_i(\\phi)^2 a_i'\\right\\}\n\\end{aligned} \\]"
  },
  {
    "objectID": "week10/week10.html#the-score",
    "href": "week10/week10.html#the-score",
    "title": "Week 10",
    "section": "The Score",
    "text": "The Score\nInformation Matrix\n\\[\\mathcal I(\\theta) = \\mathbb E( U(\\theta | y) \\otimes U(\\theta \\mid y)^T) \\]\nSince \\(\\mathbb E[U(\\theta \\mid y)]^2 = 0\\).\nThis is the variance of the score. Under regularity conditions, the \\((i,j)\\)th entry is equal to \\[- \\mathbb E( \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\ell ( \\theta \\mid y)).\\]\nAs we’ve seen previously, the MLE is the solution to \\(U(\\theta \\mid y) = 0\\).\nThe nice property of this estimator is that, for distributions with reasonably well-behaved likelihood functions, \\(\\hat \\theta_{MLE}\\) is consistent and asymptotically normal (CAN). Therefore\n\\[\\sqrt{n} (\\hat \\theta_{MLE} - \\theta) \\stackrel{\\mathcal D}{\\to} \\mathcal N(0, \\mathcal I(\\theta)^{-1}).\\]\nThis identity is from where “asymptotic tests” come from — no matter the true distribution of \\(\\mathbf Y\\), \\(\\hat \\theta_{MLE}\\) will be asymptotically normal (or \\(\\chi^2\\) if we square the estimator). There are three different ways, denoted in the figure below, to construct asymptotic tests of \\(H_0 : \\theta = \\theta_0\\) — each is obtained by using the above identity in a different way."
  },
  {
    "objectID": "week10/week10.html#wald-test",
    "href": "week10/week10.html#wald-test",
    "title": "Week 10",
    "section": "Wald Test",
    "text": "Wald Test\nDirectly evaluates whether \\(\\hat \\theta_{MLE}\\) is consistent with \\(H_0 : \\theta = \\theta_0\\).\n\\[W_n = (\\hat \\theta_{MLE} - \\hat \\theta_0)^T \\mathcal I(\\hat \\theta_{MLE}) (\\hat \\theta_{MLE} - \\theta_0) \\approx \\chi^2(p)\\]\nNote that if \\(\\theta\\) is one dimensional this simplifies to\n\\[W_n = (\\hat \\theta_{MLE} - \\theta_0) \\mathcal I(\\hat \\theta_{MLE})^{1/2} \\approx \\mathcal N(0,1)\\]\n\nAdvantages: Simple to compute, easy to construct confidence interval\nDisadvantages: requires MLE, approximation not as accurate in small samples"
  },
  {
    "objectID": "week10/week10.html#score-test",
    "href": "week10/week10.html#score-test",
    "title": "Week 10",
    "section": "Score Test",
    "text": "Score Test\nAsks “how consistent is the observed data with the null hypothesis?” and answers this by considering the gradient of the log-likelihood at the value specified by \\(H_0\\) (should be close to 0).\n\\[S_n = U(\\theta_0 \\mid y)^{T} \\mathcal I(\\theta_0)^{-1} U(\\theta_0 \\mid y) \\approx \\chi^2(p)\\]\nNote thta if \\(\\theta\\) is 1-dimensional, we can simplify this expression to\n\\[S_n = U(\\theta_0 \\mid y) \\mathcal I(\\theta_0)^{-1/2} \\approx \\mathcal N(0, 1)\\]\n\nAdvantages: Do not need to compute MLE, more computationally efficient.\nDisadvatage: Hard to construct confidence intervals since inverting this equation is difficult.\n\nNote to remember: For all of the above, if our null hypothesis is a function of multiple parameters, we can construct a \\(C\\) matrix and test \\(C \\theta\\) just as we did for the linear model in the first half of the course."
  },
  {
    "objectID": "week10/week10.html#likelihood-ratio-test",
    "href": "week10/week10.html#likelihood-ratio-test",
    "title": "Week 10",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\nConsiders the relative likelihood of the “best fitting model” under no restrictions to the model under \\(H_0\\).\n\\[Q_n = 2 \\left( \\ell(\\hat\\theta_{MLE} - \\ell(\\theta_0) \\right) \\approx \\chi^2(p)\\]\n\nAdvantages: No derivatives needed, most accurate approximation\nDisdvantage: Requires both MLE and knowledge of log-likelihood under \\(H_0\\).\n\n\nInference on a single proportion\nSuppose we have data on the number of independent successes in a population of \\(n\\) study units. This gives rise to the model\n\\[Y \\sim \\text{Binom}(n, \\pi)\\]\nSuppose our goal is to estimate \\(\\pi\\) and test the null hypothesis \\(H_0 : \\pi = \\pi_0\\). There are two ways to do this: we can either conduct an asymptotic \\(\\chi^2\\) test using one of the three previously discussed options, or we can perform an “exact” test by comparing observed values against the Binomial distribution directly."
  },
  {
    "objectID": "week10/week10.html#asymptotic-inference",
    "href": "week10/week10.html#asymptotic-inference",
    "title": "Week 10",
    "section": "Asymptotic Inference",
    "text": "Asymptotic Inference\nFor any test we use, we’ll need the components of the likelihood. Recall that the pmf of the Binomial distribution is\n\\[{n \\choose y} \\pi^y (1-\\pi)^{n-y}\\]\nStart by writing out the log-likelihood, score, and information for this model. Note that the information matrix will only be a single value since we only have one parameter \\(\\pi\\) and that we will only have one observation \\(Y\\).\n\\[\\begin{aligned}\n\\ell(\\pi \\mid y) & = \\log \\left( {n \\choose y} \\pi^y (1-\\pi)^{n-y} \\right) \\\\\n& = \\log({n \\choose y}) + y \\log \\pi + (n - y) \\log (1-\\pi) \\\\\n& \\propto y \\log( \\pi / (1-\\pi)) + n \\log (1-\\pi).\\end{aligned} \\]\n$$U(y) = y() ( \\frac{1-- (-)}{ )\n (y) = $$\n$$I() = -\nFind the MLE of \\(\\pi\\).\n\\[\\begin{aligned}\n\\frac{n \\pi(1-2\\pi)}{\\pi^2(1-\\pi)^2} + \\frac{n \\pi }{\\pi (1-\\pi)^2} & =\n\\frac{n - 2n \\pi + n \\pi}{\\pi (1-\\pi)^2} \\\\\n& = \\frac{n}{\\pi (1-\\pi)}\n\\end{aligned}\n\\]\n\\[U(\\pi) = 0 \\Rightarrow \\frac{y}{n(1-\\pi)} = \\frac{n}{1-\\pi} ...\\]\nDerive the Wald statistic using the definition.\nNow invert this test statistic to obtain a \\(1-\\alpha\\) percentile Wald-style confidence interval.\nWhat potential issue might arise when using a Wald test statistic?\nDerive the Score test statistic using the definition.\nProve that inverting this test statistic to obtain a \\(1-\\alpha\\) percentile Score-style confidence interval yields …"
  },
  {
    "objectID": "week9/week9.html",
    "href": "week9/week9.html",
    "title": "Week 9",
    "section": "",
    "text": "Recap\nSo far we’ve compared differences in probabilities on the absolute scale (i.e., via risk differences), and then we looked at relative measures (odds ratios, log odds ratios).\nNow we’ll look at contingency table methods.\nThe way we’ll be approaching this is to condition on the marginal totals, treating those as fixed and known.\nOne can show (Agresti, CDA Section 3.5) that\n\\[P(O_{11} = o_{11} | o_{1.}, o_{.1}, o_{..}, \\psi) =\n\\frac{\n  {o_{.1} \\choose o_{11}} {o_{..} - o_{.1} \\choose {o_{1.} - o_{11}} }\\psi^{o_{11}}\n  }{\n    \\sum_{\\ell=0}^{o_{.1}} { o_{.1} \\choose \\ell} { o_{..} - o_{.1} \\choose o_{1.} - \\ell } \\psi^{\\ell}\n  }\n\\]\nThis is a known distribution (non-central hypergeoemtric), but it depends on the unknown \\(\\psi\\), the odds-ratio.\nThe null hypothesis of no association between row and column variables for a \\(2\\times 2\\) contingency table holds if and only if \\(\\psi = 1\\).\nSo under the null hypothesis \\(H_0 : \\psi = 1\\) there are no unknown parameters and \\(O_{11} |o_{1.}, o_{.1}, o_{..}\\) follows a (central) hypergeometric distribution that we can use to calculate \\(p\\)-values.\nOnce we’ve conditioned on the marginals, knowing \\(O_{11}\\) tells us that all the other cell counts in a \\(2 \\times 2\\) table. So testing whether the observed \\(O_{11}\\) is extreme relative to its conditional distribution under the null is equivalent to testing whether any of the cell counts are extreme.\nWe could also construct exact tests for \\(H_0: \\psi = 2.5\\) for example, using the non-central hypergeometric distribution."
  },
  {
    "objectID": "week9/week9.html#pearson-chi-square-test-statistic",
    "href": "week9/week9.html#pearson-chi-square-test-statistic",
    "title": "Week 9",
    "section": "Pearson Chi-Square Test Statistic",
    "text": "Pearson Chi-Square Test Statistic\n\n\n\n\nPht\nCbz\nPb\n\n\n\n\nYes\n\\(O_{11} = 9\\)\n\\(O_{12} = 0\\)\n\\(O_{13} = 5\\)\n\n\nNo\n\\(O_{21} = 65\\)\n\\(O_{22} = 46\\)\n\\(O_{23} = 47\\)\n\n\n\nWhere the rows correspond to having digit hypoplasia and the columns are different drugs used in a study.\nWe will denote \\(O_{.1} = 74\\) to be the total in the first row, etc., and \\(\\hat \\pi_{i.} = O_{i.}/O{..}\\), \\(\\hat \\pi_{.j} = O_{.j}/O_{..}\\).\n$H_0 : $ There is no association between row (outcome) and column (exposure) variables.\n$H_A : $ There is an association between row and column.\n\nSometimes \\(H_0\\) is also written as “There is independence between the row and column variables.”\n\nThe Pearson Chi-square test statistic is\n\\[\\chi_P^2  \\sum_{i,j} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}} \\stackrel{H_0}\\sim \\chi_{(R-1) \\times (C-1)}^2\n\\]\nwhere \\(E_{ij}\\) is the expected count in cell \\(i,j\\) under the null, i.e.,\n\\[E_{ij} = O_{..} \\times \\hat \\pi_{i.} \\times \\hat \\pi_{.j}.\n\\]\nBecause under the null of no association we should have that \\(\\pi_{ij} = \\pi_{i.} \\times \\pi_{.j}.\\)"
  },
  {
    "objectID": "week9/week9.html#pearson-chi-square-test-as-a-score-test-2-times-2-tables",
    "href": "week9/week9.html#pearson-chi-square-test-as-a-score-test-2-times-2-tables",
    "title": "Week 9",
    "section": "Pearson Chi-Square Test as a Score Test: \\(2 \\times 2\\) Tables",
    "text": "Pearson Chi-Square Test as a Score Test: \\(2 \\times 2\\) Tables\n\n\n\n\nMajor Malformation: Yes\nNo\n\n\n\n\n\nAED Exposed: Yes\n\\(y_1 = 18\\)\n\\(n_1 - y_1 = 298\\)\n\\(n_1 = 316\\)\n\n\nNo\n\\(y_0 = 9\\)\n\\(n_0 - y_0 = 597\\)\n\\(n_0 = 606\\)\n\n\n\n\\(y = 27\\)\n\\(n- y = 895\\)\n\\(n = 922\\)\n\n\n\nCompare the following:\nchisq.test(xtabs(~aed+major, data=newborn.dat),correct=F)\n\n    Pearson’s Chi-squared test\n\ndata: xtabs(~aed + major, data = newborn.dat)\nX-squared = 12.9564, df = 1, p-value = 0.0003188\n\n\nprop.test(y,n,alternative=\"two.sided\",correct=F)\n\n    2-sample test for equality of proportions\n    without continuity correction\n\ndata: y out of n\nX-squared = 12.9564, df = 1, p-value = 0.0003188\nalternative hypothesis: two.sided\n\n95 percent confidence interval:\n  -0.06941918 -0.01480190\n\nsample estimates:\n    prop 1     prop 2\n0.01485149 0.05696203"
  },
  {
    "objectID": "week9/week9.html#validity-of-pearson-chi-square-in-small-samples",
    "href": "week9/week9.html#validity-of-pearson-chi-square-in-small-samples",
    "title": "Week 9",
    "section": "Validity of Pearson Chi-Square in Small Samples",
    "text": "Validity of Pearson Chi-Square in Small Samples\nThe Pearson chi-square test statistic is only asymptotically chi-square distributed. Inferences constructed using Pearson’s chi-square test statistic are usually considered valid when all of the \\(R \\times C\\) table have expected values greater than or equal to five (Fisher and van Belle, 1993, page 250).\nThere are other rules of thumb on this:\n\nThe test might still be valid if all of the expected values, except one, are five or greater (Fisher and van Belle, 1993, page 250).\nFor Pearson chi-square test with more than one degree of freedom, many statisticians use the rule that a minimum expected value of one is acceptable as long as no more than 20% of cells have expected values less than 5 (Agresti, 2013, page 77).\n\nchisq.test(xtabs(~nails + brand, data=monodrug.dat),correct=F)\n\n    Pearson’s Chi-squared test\n\ndata: xtabs(~nails + brand, data = monodrug.dat)\nX-squared = 5.8289, df = 2, p-value = 0.05423\n\nWarning message:\nChi-squared approximation may be incorrect in:\nchisq.test(xtabs(~nails + brand, data = monodrug.dat),\ncorrect = F)\nR prints this warning if any of the cells has an observed value less than 5.\nAn alternative is ot use an exact test that avoids the asymptotic approximation to the distribution of the Pearson statistic."
  },
  {
    "objectID": "week9/week9.html#test-statistics-for-h_0-psi-1.",
    "href": "week9/week9.html#test-statistics-for-h_0-psi-1.",
    "title": "Week 9",
    "section": "Test Statistics for \\(H_0 : \\psi = 1\\).",
    "text": "Test Statistics for \\(H_0 : \\psi = 1\\).\nUnder the null, \\(H_0 : \\psi = 1\\), the target cell count \\(O_{11}\\) follows the central hypergeometric distribution.\n\\[\n\\begin{aligned}\nPr_C(O_{11} = o_{11} | \\psi = 1) & =\n\\frac{\\displaystyle\n  {o_{.1} \\choose o_{11}} {o_{..} - o_{.1} \\choose {o_{1.} - o_{11}}}\n  }{\\displaystyle\n    \\sum_{\\ell=0}^{o_{.1}} { o_{.1} \\choose \\ell} { o_{..} - o_{.1} \\choose o_{1.} - \\ell }\n  } \\\\\n& = \\frac{\\displaystyle\n  {o_{.1} \\choose o_{11}} {o_{.2} \\choose o_{12}}\n  }{\\displaystyle\n    { o_{..} \\choose o_{1.}}\n  },\n\\end{aligned}\n  \\]\nsince \\(\\displaystyle \\sum_{\\ell = 0}^{o_{.1}} { o_{.1} \\choose \\ell} { o_{..} - o_{.1} \\choose o_{1.} - \\ell } = { o_{..} \\choose o_{1.}}\\)."
  },
  {
    "objectID": "week9/week9.html#one-sided-p-value",
    "href": "week9/week9.html#one-sided-p-value",
    "title": "Week 9",
    "section": "One-sided \\(p\\)-value",
    "text": "One-sided \\(p\\)-value\nSuppose we want to test\n\\[H_0 : \\psi = 1, \\quad \\quad H_A : \\psi > 1. \\]\nThe exact (right-tail) \\(p\\)-value is\n\\[\\begin{aligned}\np & = Pr(O_{11} \\geq o_{11} \\mid H_0 \\colon \\psi = 1)\\\\\n& = \\sum_{\\ell = o_{11}^z } \\frac{\\displaystyle\n  {o_{.1} \\choose \\ell } {o_{.2} \\choose o_{1.} - \\ell }\n  }{\\displaystyle\n    { o_{..} \\choose o_{1.}}\n  }\n\\end{aligned}\n\\]\nwhere \\(z = \\min (o_{.1, o_{1.}})\\).\nSuppose we want to test\n\\(H_0 : \\psi = 1, \\quad \\quad H_A : \\psi < 1\\)$\nThe exact (left-tail) \\(p\\)-value is\n\\[\\begin{aligned}\np & = Pr(O_{11} \\leq o_{11} \\mid H_0 : \\psi = 1) \\\\\n& = \\sum_{\\ell = z}^{o_{11}} \\frac{\\displaystyle\n  {o_{.1} \\choose \\ell } {o_{.2} \\choose o_{1.} - \\ell }\n  }{\\displaystyle\n    { o_{..} \\choose o_{1.}}\n  }\n\\end{aligned}\n\\]\nwhere \\(z = \\max(0, o_{1.} + o_{.1} - o_{..})\\)."
  },
  {
    "objectID": "week9/week9.html#two-sided-p-value",
    "href": "week9/week9.html#two-sided-p-value",
    "title": "Week 9",
    "section": "Two-sided \\(p\\)-value",
    "text": "Two-sided \\(p\\)-value\nSuppose we want to test \\[H_0 : \\psi = 1$, \\quad \\quad H_A : \\psi \\neq 1\\]\nThe possible values for \\(O_{11}\\) are\n\\[\\max (0, o_{1.} + o_{.1} - o_{..}) \\leq o_{11} \\leq \\min(o_{1.}, o_{.1}),\\]\n\nCalculate the probabilities of all of these values,\n\n\\[P_{\\ell} = Pr(O_{11} = \\ell \\mid H_0 : \\psi = \\ell) \\]\n\nSum the probabilities \\(P_{\\ell}\\) that are less than or equal to the observed probability \\(P\\) in the first equation above.\n\n\\[\np = \\sum_{\\ell = z_1}^{z_2} P_\\ell I(P_\\ell \\leq P) \\]\nwhere \\(z_1 = \\max(0, o_{1.} + o_{.1} - o_{..})\\) and \\(z_2 = \\min(o_{1.}, o_{.1})\\).\nIn general if we have a \\(2\\times 2\\) table, we can compute the odds ratio as follows:\n\nGeneric \\(2 \\times 2\\) table\n\n\n\n\n\n\n\n\n1\n0\n\n\n\n\n1\na\nb\n\n\n0\nc\nd\n\n\n\nThen the \\(\\mathrm{OR} = \\frac{ad}{bc}\\).\n# Function: SuppDists::dghyper(x, n, r, N, log=FALSE)\n# arguments: x is o_11 cell, n is o_{.1},\n# r is o_{1.}, N is total\n\nlibrary(SuppDists)\n\nhyper.probs <- dghyper(seq(0,12,1),12,58,566)\n\ncbind(seq(0,12,1), round(hyper.probs,4))\n\n    [,1]  [,2]\n[1,]   0 0.2696\n[2,]   1 0.3775\n[3,]   2 0.2377\n[4,]   3 0.0889\n[5,]   4 0.0220\n[6,]   5 0.0038\n[7,]   6 0.0005\n[8,]   7 0.0000\n[9,]   8 0.0000\n[10,]  9 0.0000\n[11,] 10 0.0000\n[12,] 11 0.0000\n[13,] 12 0.0000\n\nfisher.pvalue <- sum(hyper.probs[4:13])\nfisher.pvalue\n[1] 0.1152\nThe function fisher.test() runs a Fisher’s exact test directly.\nxtabs(~MONOCBZ + major, data=monocbz.dat)\n        major\nMONOCBZ   0 1\n      0 499 9\n      1 55  3\n\nfisher.test(xtabs(~MONOCBZ + major, data=monocbz.dat))\n      Fisher’s Exact Test for Count Data\n\ndata: xtabs(~MONOCBZ + major, data = monocbz.dat)\np-value = 0.1152\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.5101941 12.5603706\nsample estimates:\nodds ratio\n  3.015342\nOne can compute an exact CI for the OR by inverting Fisher’s exact test."
  },
  {
    "objectID": "week11/week11.html",
    "href": "week11/week11.html",
    "title": "Week 11",
    "section": "",
    "text": "Asymptotic Sampling Distribution\nSubject to appropriate regularity conditions,\n\\[\\begin{bmatrix} \\hat \\beta_{MLE} \\\\ \\hat \\phi_{MLE} \\end{bmatrix} \\sim \\text{MVN} \\left(\n    \\begin{bmatrix} \\beta \\\\ \\phi \\end{bmatrix}, \\mathcal I_n (\\beta, \\phi)^{-1}\n    \\right).\\]\nNow we compute the components of \\(\\mathcal I_n(\\beta, \\phi):\\)\n\\[\\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k} = \\frac{\\partial}{\\partial \\beta_k} \\underbrace{\\left\\{ \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{x_{ij}}{V(\\mu_i) a_i(\\phi)} (y_i - \\mu_i) \\right\\}}_{\\text{score for } \\beta_j}.\\]\n\\[ = (y_i - \\mu_i) \\frac{\\partial}{\\partial \\beta_k} \\left\\{ \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{x_{ij}}{V(\\mu_i) a_i(\\phi)} \\right\\} - \\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right)^2 \\frac{x_{ij} x_{ik}}{V(\\mu_i) a_i(\\phi)}.\\]\nHence \\[-\\mathbb E\\left[\\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k} \\right] =\n\\sum_{i=1}^n \\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right)^2 \\frac{x_{ij}x_{ik}}{V(\\mu_i) a_i(\\phi)}.\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\phi} & =\n\\frac{\\partial}{\\partial \\phi} \\left\\{\n    \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{x_{ij}}{V(\\mu_i)a_i(\\phi)} (y_i - \\mu_i)\n\\right\\} \\\\\n& = - \\frac{a_i'(\\phi) \\partial \\mu_i x_{ij}}{a_i(\\phi)^2 \\partial \\eta_i V(\\mu_i)} (y_i - \\mu_i)\n\\end{aligned}\n\\]\nThus\n\\[-E \\left[ \\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\phi} \\right] = 0\\]\n\\[\\begin{aligned}\\frac{\\partial^2 \\ell}{\\partial \\phi \\partial \\phi} & = \\frac{\\partial}{\\partial \\phi}\n\\left\\{\n    - \\frac{a_i'(\\phi)}{a_i(\\phi)^2} (y_i\\theta_i - b(\\theta_i)) + c'(y_i, \\phi)\n\\right\\} \\\\\n& = - \\left\\{ \\frac{a_i(\\phi)^2 a_i''(\\phi) - 2a_i(\\phi) a_i'(\\phi)^2}{a_i(\\phi)^4}\\right\\} [y_i \\theta_i - b(\\theta_i)] + c''(y_i, \\phi) \\\\\n& = -K(\\phi)[y_i\\theta_i - b(\\theta_i)] + c''(y_i, \\phi)\n\\end{aligned}\\]\nand thus \\[-\\mathbb E\\left[ \\frac{\\partial^2 \\ell}{\\partial \\phi \\partial \\phi} \\right] =\n\\sum_{i=1}^n K(\\phi)[b'(\\theta_i)\\theta_i - b(\\theta_i)] - \\mathbb E[c''(Y_i, \\phi)].\\]\nFor the linear predictor \\(x_i' \\beta\\), suppose we partition \\(\\beta = (\\beta_1, \\beta_2)\\) and we are interested in testing:\n\\[H_0 : \\beta_1 = \\beta_{1,0} \\quad \\text{vs.} \\quad H_a : \\beta_1 \\neq \\beta_{1,0}\\]\nThe length of \\(\\beta_1\\) is \\(q \\leq (p+1)\\) and \\(\\beta_2\\) is left arbitrary.\nIn most settings \\(\\beta_{1,0} = 0\\) which represents some form of ‘no effect’\nFollowing our review of asymptotic theory, there are three common hypothesis testing frameworks: Wald, score, and likelihood ratio testing.\nWe saw that the score equation for \\(\\beta_j\\) is\n\\[\\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{x_{ij}}{V(\\mu_i) a_i(\\phi)} (y_i - \\mu_i) = 0\\]\nEstimation of \\(\\beta\\) requires solving \\((p+1)\\) of these equations simultaneously. This is tricky because \\(\\beta\\) appears in several places (implicitly anywhere a \\(\\mu\\) appears).\nNo closed form solution in general is available (though there is one for linear settings), so we need an iterative method. A commonly used general purpose optimization routine is the Newton-Raphson algorithm.\nGeneral idea of Newton-Raphson and Fisher Scoring\nGeneral idea of Newton-Raphson\n\\[\\beta^{(r+1)} = \\beta^{(r)} - \\left[ \\mathcal{I}_{\\beta \\beta}^{(r)}\\right]^{-1} U^{(r)}_{\\beta}\\]\nFisher scoring is an adaptation of the Newton-Raphson algorithm that uses the expected information, \\(\\mathcal I_{\\beta \\beta}\\), rather than observed information \\(\\mathcal I_{\\beta \\beta}\\) for the update.\nThere’s a close relationship between Fisher scoring and IRLS. Often this is exploited for computation in software packages, many of which use IRLS by default (including R’s glm()).\nThe idea:\nDefine\n\\[Z_i = \\overbrace{g(\\mu_i) + (Y_i - \\mu_i)g'(\\mu_i)}^{\\text{First order Taylor approx of }g(Y_i)}\\]\nto be an “adjusted response variable” to show that\n\\[\\text{Var}(Z_i) = (W_i)^{-1}, \\text{ where }W_i = \\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right)^2 \\frac{1}{V(\\mu_i)a_i(\\phi)}\\]\nThen note that\n\\[\\mathbb E[Z_i] = x_i'\\beta\\]\nso we can consider a linear model for \\(Z\\) to estimate \\(\\beta\\), i.e., using the GLS estimator \\(\\hat \\beta_{GLS} = (X'WX)^{-1} X'WZ\\).\nTwo immediate challenges to estimating \\(\\hat \\beta_{GLS}\\) in practice are\nHowever, we can use IRLS to do estimation.\nSuppose the current estimate of \\(\\beta\\) is \\(\\hat \\beta^{(r)}\\). Compute\n\\[\\eta_i^{(r)} = x_i'\\hat \\beta^{(r)}\\] \\[\\mu_i^{(r)} = g^{-1} (\\eta_i^{(r)})\\] \\[W_i^{(r)} = \\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\biggr \\mid_{\\eta_i^{(r)}} \\right)^2 \\frac{1}{V(\\mu_i^{(r)})}\\] \\[z_i^{(r)} = \\eta_i^{(r)} + (y_i - \\mu_i^{(r)}) \\frac{\\partial \\eta_i}{\\partial \\mu_i} \\mid_{\\mu_i^{(r)}}\\]\n\\(W_i\\) is called the ‘working weight’\nCheck the McCullagh and Nelder (1989) book which shows that using IRLS for estimation here is equivalent to Fisher scoring. For more detail, see Agresti 4.6.3 or https://grodri.github.io/glms/notes/\nThe updated version of \\(\\hat \\beta\\) is obtained as the WLS estimate to the regression of \\(Z\\) on \\(X\\):\n\\[\\hat \\beta^{(r+1)} = (X' W^{(r)} X)^{-1} (X' W^{(r)} Z^{(r)})\\]\n\\(X\\) is the \\(n \\times (p+1)\\) design matrix from the initial specification of the model. \\(W^{(r)}\\) is the diagonal \\(n \\times n\\) matrix with entries \\(\\{ W_1^{(r)}, ..., W_n^{(r)}}\\). \\(Z^{(r)}\\) is the \\(n\\)-vector \\((z_1^{(r)}, ..., z_n^{(r)})\\).\nIterate until the \\(\\hat \\beta\\) values converges.\nA generic call to glm() is given by\nfit0 <- glm(formula, family, data, ...)\nformula specifies the structure of the linear predictor \\(\\eta_i = x_i' \\beta\\).\nfamily jointly specifies the probability distribution \\(f_Y()\\), link function \\(g()\\) and variance function \\(V()\\).\nE.g.,\nglm(Y ~ X_1 + X2, family = binomial(), data = df)\nMost common family objects are binomial() or poisson().\nThe residuals inside a glm() are called working residuals and used internally for iteratively reweighted least squares.\nIn linear models, we used residual diagnostics to examine the adequacy of model fit and investigate potential data issues such as outliers. Adequacy of model fit included functional form for terms in the linear predictor and the homoscedasticity assumption.\nIn GLMs, residual diagnostics are much more complex and visual inspection of residual plots is typically less informative.\nIn general, residuals are meant to represent variation in the outcome that is not explained by the model.\nVariation after the systematic component is accounted for, and therefore residuals are therefore model-specific.\nPearson residuals account for the heteroscedasticity via standardization.\n\\[r_i^p = \\frac{y_i - \\hat \\mu_i}{\\sqrt{\\text{Var}(Y_i)}}\\]\nWe’ll see that Pearson residuals can be useful for diagnosing departures from our model’s variance assumption.\nThe deviance residual (sometimes called the “preferred residual”) is defined as\n\\[r_i^d = \\text{sign}(y_i - \\hat \\mu_i)\\sqrt{d_i}\\]\nwhere, letting \\(\\ell(\\hat \\mu_i, \\phi; y_i)\\) be the log-likelihood contribution of unit \\(i\\) evaluated as \\(\\hat \\mu_i\\) and \\(\\phi\\), then \\[d_i = 2 \\phi(\\ell(y_i, \\phi; y_i) - \\ell(\\hat \\mu_i, \\phi; y_i))\\]\nWe can think about this as the distance between \\(\\hat \\mu_i\\) and \\(y_i\\) on the log-likelihood scale.\nPierce and Shafer (JASA, 1986) examined various residuals for GLMs. https://www.jstor.org/stable/2289071\nOne can get both deviance and Pearson residuals from glm() in R:\nA prospective study of coronary heart disease (CHD) with \\(n = 3,154\\) male participants aged 39-54 at risk for CHD, employed at 10 companies in California with a baseline survey and measurements taken at intake (1960-61) along with annual surveys until December 1969 to assess incident CHD.\nOur primary goal is to investigate the relationship between ‘behavior pattern’ and risk of CHD.\nParticipants were categorized into one of two behavior pattern groups.\nType A: characterized by enhanced aggressiveness, ambitiousness, competitive drive, and chronic sense of urgency.\nType B: characterized by more relaxed and non-competitive.\nFor now, we will ignore any loss to follow-up and consider the binary outcome of\n\\[Y = \\left\\{ \\begin{array}\n1 \\quad & \\text{occurrence of CHD during follow-up} \\\\\n0 & \\text{otherwise}\n\\end{array} \\right. \\]\n8.1% of participants developed CHD during follow-up, while 50.4 of participants were classified as Type A at baseline.\n\\[\\hat P(\\text{CHD} = 1 \\mid \\text{Type} = A) = 0.112\\] \\[\\hat P(\\text{CHD} = 1 \\mid \\text{Type} = B) = 0.050\\]\nI.e., 11.2% of Type A men and 5% of Type B men developed CHD during follow-up.\nOften we will use the generic term ‘risk,’ e.g., “risk of CHD” rather than probability of CHD during follow-up."
  },
  {
    "objectID": "week11/week11.html#form-of-the-fisher-matrix",
    "href": "week11/week11.html#form-of-the-fisher-matrix",
    "title": "Week 11",
    "section": "Form of the Fisher Matrix",
    "text": "Form of the Fisher Matrix\nWe can write the expected information matrix in block-diagonal form:\n\\[\\mathcal I_n(\\beta, \\phi) = \\begin{bmatrix}\n    \\mathcal I_{\\beta \\beta} & 0 \\\\\n    0 & \\mathcal I_{\\phi \\phi}\n\\end{bmatrix}.\\]\nThe inverse of the information matrix is the asymptotic variance\n\\[\\text{Var}[\\hat \\beta_{MLE}, \\hat \\phi_{MLE}] = \\mathcal I_n(\\beta, \\phi)^{-1} =\n\\begin{bmatrix} \\mathcal I^{-1}_{\\beta \\beta} & 0 \\\\ 0 & \\mathcal I_{\\phi \\phi}^{-1} \\end{bmatrix}.\\]\nThe block diagonal structure of \\(\\text{Var}[\\hat \\beta_{MLE}, \\hat \\phi_{MLE}]\\) implies that for GLMs, valid characterization of the uncertainty in \\(\\hat \\beta_{MLE}\\) does not require the propagation of uncertainty in the estimate of \\(\\phi\\).\nFor example, for linear regression on Normally distributed outcomes, we plug in an estimate of \\(\\sigma^2\\) into \\[\\text{Var}[\\hat \\beta_{MLE}] = \\sigma^2 (X^T X)^{-1}\\] without worrying about uncertainty in estimation of \\(\\sigma^2\\).\nFor GLMs, therefore, estimation of \\(\\text{Var}(\\hat \\beta_{MLE})\\) proceeds by plugging in the values of \\((\\hat \\beta_{MLE}, \\hat \\phi)\\) into \\(\\mathcal I_{\\beta \\beta}^{-1}\\), i.e., \\[\\widehat{\\text{Var}}[\\hat \\beta_{MLE}] = \\hat{\\mathcal{I}}_{\\beta \\beta}^{-1}\\] where \\(\\hat \\phi\\) is any consistent estimator of \\(\\phi\\).\n\nMatrix notation\nRecall that the \\((j,k)^{\\text{th}}\\) element of \\(\\mathcal I_{\\beta \\beta}\\) has the form\n\\[(\\mathcal I_{\\beta \\beta})_{jk} = \\sum_{i=1}^n \\underbrace{\\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right)^2 \\frac{1}{V(\\mu_i) a_i(\\phi)}}_{\\coloneqq W_i} x_{ij}x_{jk},\\]\nso we can write\n\\[(\\mathcal I_{\\beta \\beta})_{jk} = \\sum_{i=1}^n W_i x_{ij}x_{jk}.\\]\n\\(W_i\\) sort of looks like the inverse of the variance of \\(Y_i\\).\nWe can therefore write:\n\\[\\mathcal I_{\\beta \\beta} = X'WX \\quad \\text{ and } \\quad \\text{Var}(\\hat \\beta_{MLE}) = \\mathcal I_{\\beta \\beta}^{-1} = (X'WX)^{-1},\\]\nwhere \\(W = \\text{diag}(W_1, ..., W_n)\\) and \\(X\\) is the design matrix.\nNote that \\((X'WX)^{-1}\\) looks a lot like \\(\\text{Var}(\\hat \\beta_{GLS})\\), just instead of \\(W\\) we used \\(\\Sigma^{-1}\\)."
  },
  {
    "objectID": "week11/week11.html#wald-test",
    "href": "week11/week11.html#wald-test",
    "title": "Week 11",
    "section": "Wald Test",
    "text": "Wald Test\nLet \\(\\hat \\beta_{MLE} = (\\hat \\beta_{1,MLE}, \\hat \\beta_{MLE})\\).\nUnder \\(H_0\\)\n\\[(\\hat \\beta_{1,MLE} - \\beta_{1,0})' \\widehat{\\text{Var}}[\\hat \\beta_{1,MLE}]^{-1}(\\hat\\beta_{1,MLE} - \\beta_{1,0}) \\longrightarrow_d \\chi^2_q\\]\nwhere \\(\\widehat{\\text{Var}}[\\hat \\beta_{1,MLE}]\\) is the \\(q\\times q\\) sub-matrix of \\(\\mathcal{I}_{\\beta\\beta}^{-1}\\) corresponding to \\(\\beta_1\\), evaluated at \\(\\left( \\hat \\beta_{MLE}, \\hat \\phi_{MLE} \\right).\\)\nThis is the multivariate analog of \\(((\\text{Est} - \\text{Null})/\\text{SE})^2\\).\nThe Wald test is just looking at how far our estimate of \\(\\beta\\) is from the null and seeing if that distance is large relative to the uncertainty in our estimate."
  },
  {
    "objectID": "week11/week11.html#score-test",
    "href": "week11/week11.html#score-test",
    "title": "Week 11",
    "section": "Score Test",
    "text": "Score Test\nLet \\(\\hat \\beta_{0,MLE} = (\\beta_{1,0}, \\hat \\beta_{2,MLE})\\) and \\(\\hat \\phi_{0,MLE}\\) denote the MLEs under \\(H_0\\).\nUnder \\(H_0\\),\n\\[U(\\hat \\beta_{0,MLE}, \\hat \\phi_{0,MLE}; y)' \\mathcal I_n(\\hat \\beta_{0,MLE}; \\hat \\phi_{0, MLE})^{-1} U(\\hat \\beta_{0,MLE}, \\hat \\phi_{0,MLE}; y) \\longrightarrow_d \\chi^2_q.\\]"
  },
  {
    "objectID": "week11/week11.html#likelihood-ratio-test",
    "href": "week11/week11.html#likelihood-ratio-test",
    "title": "Week 11",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\nObtain unrestricted MLEs: \\((\\hat \\beta_{MLE}, \\hat \\phi_{MLE})\\), and obtain MLEs under \\(H_0:\\) \\((\\hat \\beta_{0, MLE}, \\hat \\phi_{0,MLE})\\).\nUnder \\(H_0\\),\n\\[2(\\ell(\\hat \\beta_{MLE}, \\hat \\phi_{MLE}; y) - \\ell(\\hat \\beta_{0,MLE}, \\hat \\phi_{0, MLE} ; y)) \\longrightarrow_d \\chi^2_q.\\]"
  },
  {
    "objectID": "week11/week11.html#types-of-contrasts",
    "href": "week11/week11.html#types-of-contrasts",
    "title": "Week 11",
    "section": "Types of Contrasts",
    "text": "Types of Contrasts\nWe need to pick a contrast —\nRisk difference:\n\\[RD = P(Y = 1 \\mid x = 1) - P(Y = 1 \\mid x = 0)\\] \\[\\widehat{RD} = \\hat P(Y = 1 \\mid x = 1) - \\hat P(Y=1 \\mid x = 0) = .112 - .050 = .062\\]\nThe difference in the estimated risk of CHD during follow-up between type A and type B men is 0.062 (or 6.2%). This characterizes the way in which the additional risk of CHD of being a type A person manifests through an absolute increase.\nWe’ve already seen the OR as a relative-scale contrast. A more interpretable option is the relative risk:\n\\[RR = \\frac{P(Y = 1 \\mid x = 1)}{P(Y = 1 \\mid x = 0)}\\]\n\\[\\widehat{RR} = \\frac{\\hat P(Y = 1 \\mid x = 1)}{\\hat P(Y = 1 \\mid x = 0)} = \\frac{0.112}{0.050} = 2.24 \\]\n\nLog-likelihood for GLMs with binary outcomes\n\\[\\ell(\\beta; y) = \\sum_{i=1}^n y_i \\theta_i - b(\\theta_i)\\]\n\\[ = \\sum_{i=1}^n y_i \\theta_i - \\log(1 + \\exp\\{\\theta_i\\})\\]\nWhere \\(\\theta_i\\) is a function of \\(\\beta\\) via\n\\[\\mu_i = b'(\\theta_i) = \\frac{\\exp \\theta_i}{1 + \\exp \\theta_i}\\]\nand\n\\[\\mu_i = g^{-1}(x_i'\\beta)\\]"
  },
  {
    "objectID": "week11/week11.html#score-and-information-for-glms-with-binary-outcomes",
    "href": "week11/week11.html#score-and-information-for-glms-with-binary-outcomes",
    "title": "Week 11",
    "section": "Score and information for GLMs with Binary outcomes",
    "text": "Score and information for GLMs with Binary outcomes\nThe score function for \\(\\beta_j\\) is\n\\[\\frac{\\partial \\ell }{\\partial \\beta_j} = \\sum_{i=1}^n \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{x_{ij}}{\\mu_i (1-\\mu_i)} (y_i - \\mu_i)\\]\nwhere the expression for \\(\\partial \\mu_i / \\partial \\eta_i\\) depends on the choice of \\(g()\\).\nSince \\(\\phi\\) is fixed and known, the expected information matrix is just \\[\\mathcal I_{\\beta \\beta} = X' W X\\]\nwhere \\(X\\) is the design matrix for the model and \\(W\\) is a diagonal matrix with \\(i^{\\text{th}}\\) diagonal element\n\\[W_i = \\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right)^2 \\frac{1}{\\mu_i (1-\\mu_i)},\\]\nwhich we’ve said previously looks like an inverse variance of \\(Y\\)."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. Introduction to Statistical Learning [Internet]. [cited 2023 Nov 8]. Available from: https://trevorhastie.github.io/ISLR/\nAllen Akinkunle. The Bias-Variance Decomposition Demystified [Internet]. [cited 2023 Nov 8]. Available from: https://allenkunle.me/bias-variance-decomposition\nEfron B, Tibshirani RJ. An Introduction to the Bootstrap. CRC Press; 1994. 453 p. \nJulian J Faraway. Routledge & CRC Press. [cited 2023 Nov 8]. Linear Models with R. Available from: https://www.routledge.com/Linear-Models-with-R/Faraway/p/book/9781439887332\nTrevor Hastie, Junyang Qian, Kenneth Tay. An Introduction to glmnet [Internet]. [cited 2023 Nov 8]. Available from: https://glmnet.stanford.edu/articles/glmnet.html\nFrank Harrell. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis [Internet]. Cham: Springer International Publishing; 2015 [cited 2023 Nov 8]. (Springer Series in Statistics). Available from: https://link.springer.com/10.1007/978-3-319-19425-7\nEric Vittinghoff, David V. Glidden, Stephen C. Shiboski, Charles E. McCulloch. Regression Methods in Biostatistics. [cited 2023 Nov 8]. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. Available from: https://regression.ucsf.edu/regression-methods-biostatistics-linear-logistic-survival-and-repeated-measures-models\nAgresti A, Coull BA. Approximate Is Better than “Exact” for Interval Estimation of Binomial Proportions. The American Statistician. 1998;52(2):119–26. https://www.jstor.org/stable/2685469\nErnst MD. Permutation Methods: A Basis for Exact Inference. Statistical Science. 2004 Nov;19(4):676–85. https://www.jstor.org/stable/4144438\nAgresti A. Categorical Data Analysis. John Wiley & Sons; 2012. 756 p. \nShmueli G. To Explain or to Predict? Statist Sci [Internet]. 2010 Aug 1 [cited 2023 Aug 28];25(3). Available from: https://projecteuclid.org/journals/statistical-science/volume-25/issue-3/To-Explain-or-to-Predict/10.1214/10-STS330.full\nPierce DA, Schafer DW. Residuals in Generalized Linear Models. Journal of the American Statistical Association. 1986;81(396):977–86.\nKutner MH, Neter J. Applied Linear Regression Models. McGraw-Hill/Irwin; 2004. 728 p."
  },
  {
    "objectID": "week12/week12.html",
    "href": "week12/week12.html",
    "title": "Week 12",
    "section": "",
    "text": "Modeling Wester Collaborative Group Study (WCGS)\nReturning to the WCGS, the dataset has a number of covariates that we might consider including in a model for CHD risk:\nHow do we approach making decisions about what to include in the model?\nWe discussed model selection based on a priori knowledge previously.\nOne strategy is to fit and report the following three models:\nThen we would report analysis 2 as the main analysis, reporting analysis 1 and 3 as sensitivity analyses.\nAdd core adjustment variables:\nWe think the \\(\\chi^2\\) reported in the model.fit statistics is the global \\(\\chi^2\\) statistic for a likelihood ratio test, but we should double-check.\nThe coefficients’ z-values and \\(p\\)-values are related to univariate Wald tests.\nInterpretation of \\(\\widehat{OR} = \\exp(\\beta_{\\text{behave}}) = 1.99\\) from the core model:\nIt’s hard to make much of the residuals… Recall that the deviance residuals are the preferred residuals in GLM settings.\nSo far, we’ve only considered the logit link function:\n\\[g(\\mu_i) = \\log \\left( \\frac{\\mu_i}{1-\\mu_i} \\right) = x_i' \\beta\\]\nBy far this is the most common link function used for GLMs of binary data.\nWhat about other link functions?\nPotential choices include:\nFor the goal of characterizing the association between behavior type and risk of CHD, interpretabililty is crucial.\nIn R we use the family= argument to change the link. Other components of the GLM that are functions of the link are appropriately adjusted.\nFirst, consider changing the link in the unadjusted analysis:\nThe main difference between using link = 'identity' vs. without inside the binomial() is that it’s changing the variance to use the binomial variance.\nNote for thought: Think more about how this is not GLS: is it because it’s using Newton Raphson (?)\nInterpretation of \\(\\widehat{RR} = \\exp(\\hat \\beta_{\\text{behave}}) = 2.21\\):\nThe fitted values are the same from the unadjusted logit, identity, and log link models because these are saturated models.\nA saturated model is a model that estimates a separate parameter for all unique values of \\(x_i\\).\nThere are numerous equivalent ways to define saturated models, so you may encounter other (equivalent) definitions elsewhere.\nA model with a single binary predictor is saturated because there are two possible values of \\(x_i\\) and two parameters \\((\\beta_0, \\beta_1)\\).\nBecause saturated models contain a separate parameter corresponding to each possible level of \\(\\mathbb E[Y_i \\mid x_i] = \\mu_i\\), they can perfectly fit the expected value in each \\(x\\) group.\nI.e., \\(\\hat{\\mathbb E}[Y_i \\mid x_i = x^\\star]\\) will be the sample mean of units with \\(x = x^\\star\\).\nSaturated models will thus give the same fitted values, \\(\\hat \\mu_i\\), regardless of the link function.\nChanging the link function from the default can cause IRLS to have trouble finding starting values.\nFor example, if we try to fit glm(chd ~ behave + age + wt + sbp + chol + smoker, family = binomial(link='identity'), data = wcgs), we will get an error.\nThe problem is that it can’t find a set of starting coefficients where all of the predicted values are in the \\((0,1)\\) range.\nOne option is to pass your own starting values in:\nWhat people often do is to use the output from another model that didn’t fail. One option is to fit the logistic regression, get the fitted values from that model, and then specify those as the starting fitted values for any of these models using alternative link functions.\nIf at any point during the IRLS algorithm, one of the fitted values is outside \\((0,1)\\), then \\(\\text{Var}(Y_i) = \\mu_i ( 1 - \\mu_i)\\) will be negative. This makes it likely that the model will either error or be unlikely to converge.\nAn alternative computational option in the identity link setting is to use OLS with an appropriate variance estimator to account for the heteroscedasticity induced by the mean-variance relationship.\nIn R:\nWe might also get similar errors about not finding valid starting values using a log link in a binomial outcome regression.\nIf we use the log-link, we get a relative risk. The interpretation of \\(\\exp(\\hat \\beta_{\\text{behave}}) = 1.78\\) is that it is the relative risk for Type A men is 78% higher than for Type B men, conditional on age, weight, SBP, cholesterol, and smoking.\nFor a continuous response variable, consider two models that could be used to assess the \\(Y - X\\) association:\n\\[\\mathbb E[Y \\mid X, Z] = \\beta_0^c + \\beta_X^c X + \\beta_Z^c Z \\label{eqn:1} \\tag{1}\\] \\[\\mathbb E[Y \\mid X] = \\beta_0^m + \\beta_X^m X \\label{eqn:2} \\tag{2} \\]\nIn model \\(\\ref{eqn:1}\\), \\(\\beta_X^c\\) is a conditinoal parameter, where contrasts condition on the value of \\(Z\\).\nIn model \\(\\ref{eqn:2}\\), \\(\\beta_X^m\\) is a marginal parameter, and contrasts using it do not condition on anything.\nThe relationship between the two parameters is clarified as follows:\n\\[\\mathbb E[Y\\mid X ] = \\mathbb E_Z[\\mathbb E_Y[Y \\mid X,Z] \\mid X ] \\tiny{\\tag{ Law of iterated expectations}}\\] \\[ = \\mathbb E_Z[\\beta_0^c + \\beta_X^c X + \\beta_Z^c Z ] \\] \\[ = \\beta_0^c + \\beta_X^c X + \\beta_Z^c \\mathbb E_Z[Z \\mid X] \\]\nSo the marginal contrast equals:\n\\[\\beta_X^m = \\mathbb E[Y \\mid X = (x+1)] - \\mathbb E[Y \\mid X = x]\\] \\[ = \\beta_X^c + \\beta_Z^c \\underbrace{\\{ \\mathbb E[Z \\mid X = (x+1) - \\mathbb E[Z \\mid X = x]] \\}}_{\\text{slope from a linear regression of } Z \\sim X } \\]\nConsidering the model \\(\\mathbb E[Z \\mid X] = \\gamma_0 + \\gamma_X X\\),\n\\[\\beta_X^m = \\beta_X^c + \\beta_Z^c \\gamma_X.\\]\nThe marginal contrast is the conditional plus a bias term.\nThe bias term (\\(\\beta_Z^c \\gamma_X\\)) is non-zero if both\nIf \\(Z\\) is a confounder and we don’t adjust for it, the marginal association we estimate will be biased.\nThe direction of the bias depends on the interplay between \\(\\beta_Z^c\\)\nIf \\(Z\\) isn’t a confounder, then one or both of \\(\\{ \\beta_Z^c, \\gamma_X \\}\\) are zero, so the bias term is zero and \\[\\beta_X^m = \\beta_X^c.\\]\nSo the marginal and conditional parameters are equivalent when the variables conditioned on are not confounders.\nThis is a result of the collapsibility of parameters from a linear regression (or any GLM using the identity link).\nIf \\(Z\\) is not a confounder, conditioning on it won’t do any harm since doing so does not change the quantity we’re estimating.\nWe call \\(Z\\) a precision variable if it is associated with \\(Y\\) but not \\(X\\).\nIf \\(Z\\) is a precision variable, the standard error of \\(\\beta_X^c\\) will be smaller than the standard error of \\(\\beta_X^m\\).\nIf \\(Z\\) explains a substantial amount of variability in \\(Y\\) then the conditional model SSE will be smaller, so \\(\\hat \\sigma^2\\) will also generally be smaller for the conditional model.\nBecause linear regression parameters are collapsible and including precision variables in the model leads to smaller uncertainties, it’s generally good practice to condition on precision variables.\nIs the same true for logistic regression?\nFor a binary outcome, consider two models:\n\\[\\text{logit} \\mathbb E[Y \\mid X, Z] = \\beta_0^c + \\beta_X^c X + \\beta_Z^c Z \\label{eqn:3} \\tag{3}\\] \\[\\text{logit} \\mathbb E[Y \\mid X] = \\beta_0^m + \\beta_X^m X \\label{eqn:4} \\tag{4}\\]\nThe conditional odds ratio for a binary \\(X\\) is\n\\[\\theta_X^c = \\exp (\\beta_X^c) = \\frac{\\mathbb E[Y \\mid X = 1, Z]}{1 - \\mathbb E[Y \\mid X = 1, Z]}\n\\bigg / \\frac{\\mathbb E[Y \\mid X = 0, Z]}{1 - \\mathbb E[Y \\mid X = 0, Z]} \\]\nThe marginal odds ratio for \\(X\\) is\n\\[\\theta_X^m = \\exp (\\beta_X^m) = \\frac{\\mathbb E[Y \\mid X = 1]}{1 - \\mathbb E[Y \\mid X = 1]}\n\\bigg / \\frac{\\mathbb E[Y \\mid X = 0]}{1 - \\mathbb E[Y \\mid X = 0]}.\\]\nAnalagous to what we did for linear regression, we’d like to try to write the marginal OR as a function of the conditional.\nRecall that to do this, we\n\\[\n\\begin{aligned}\n\\mathbb E[Y \\mid X] & = \\mathbb E_Z[\\mathbb E_Y[Y \\mid X, Z] \\mid X] \\\\\n& = \\mathbb E_Z\\left[ \\frac{\\exp\\{ \\beta_0^c + \\beta_X^c X + \\beta_Z^c Z \\} }{1 + \\exp\\{\n  \\beta_0^c + \\beta_X^c X + \\beta_Z^c Z\n\\}} \\bigg\\vert X \\right] \\\\\n& = \\int_Z \\left( \\frac{\\exp\\{ \\beta_0^c + \\beta_X^c X + \\beta_Z^c Z \\} }{1 + \\exp\\{\n  \\beta_0^c + \\beta_X^c X + \\beta_Z^c Z\n\\}} \\right) f_{Z|X}(Z = z \\mid X) \\partial z \\\\\n%  & = \\int_Z \\left( \\frac{\\theta_X^c \\exp\\{ \\beta_0^c+ \\beta_Z^c Z \\} }{1 + \\theta_X^c \\exp\\{\n%  \\beta_0^c + \\beta_Z^c Z\n% \\}} \\right) f_{Z|X}(Z = z \\mid X) \\partial z.\n\\end{aligned}\n\\]\nIf we consider plugging this expression for \\(\\mathbb E[Y\\mid X]\\) into\n\\[\\theta_X^m = \\exp (\\beta_X^m) = \\frac{\\mathbb E[Y \\mid X = 1]}{1 - \\mathbb E[Y \\mid X = 1]}\n\\bigg / \\frac{\\mathbb E[Y \\mid X = 0]}{1 - \\mathbb E[Y \\mid X = 0]},\\]\nwe can see that the relationship between the conditional OR \\(\\theta_X^m\\) and marginal OR \\(\\theta_X^m\\) is not straightforward.\nThere is no simple, closed-form expression for \\(\\theta_X^m\\) as a function of \\(\\theta_X^c\\). Unlike in linear regression, they are not linearly related.\nHowever, given \\(\\theta_X^c\\) we can calculate \\(\\theta_X^m\\) numerically.\nTo do so, from the expression for \\(\\mathbb E[Y \\mid X]\\) we need to specify:\nFor the purposes of this exercise, we’ll consider binary \\(X\\) and \\(Z\\) that are related via logistic regression\n\\[\\text{logit} \\mathbb E[Z \\mid X] = \\gamma_0 + \\gamma_X X.\\]\nNotationally, let \\(\\phi_{XZ} = \\exp(\\gamma_X)\\) denote the \\(Z \\sim X\\) odds ratio.\nWe consider a range of values of\nand for each scenario we compute \\(\\theta_X^m\\) and the percent difference:\n\\[\\frac{\\theta_X^m - \\theta_X^c}{\\theta_X^c} \\times 100.\\]\nQuick review of GLMs.\nThe exponential dispersion family can be written as\n\\[f(y \\mid \\theta, \\phi) = \\exp \\left\\{ \\frac{y \\theta - b(\\theta)}{a(\\phi)}  + c(y, \\phi)\\right\\}\\]\nwhere \\(\\theta\\) is the canonical parameter and \\(\\phi\\) is the dispersion parameter. We also showed in class that\n\\[\\mathbb E[Y_i] = \\mu_i = b'(\\theta_i)\\] \\[\\text{Var}[Y_i] = b''(\\theta_i) a_i(\\phi)\\]\nWe refer to \\(b''(\\theta_i)\\) as the variance function and sometimes we write it as \\(V(\\mu_i) = b''(\\theta_i) = b''(b'^{-1}(\\mu_i)),\\) i.e., \\(V\\) can be some function of \\(\\mu\\). Then we fit a GLM by assuming the model \\(g(\\mu_i) = \\eta_i = X_i'\\beta\\)."
  },
  {
    "objectID": "week12/week12.html#comparison-of-approaches",
    "href": "week12/week12.html#comparison-of-approaches",
    "title": "Week 12",
    "section": "Comparison of Approaches",
    "text": "Comparison of Approaches\nOften linear models are motivated by the following minimization problem:\n\\[\\min_{\\beta} || \\mathbf{y} - \\mathbf{X} \\beta ||_2^2\\]\nAssuming that \\(\\mathbb E[Y] = X \\beta\\).\nA more statistically driven approach is to assume that\n\\[Y\\mid X \\sim \\mathcal N(X \\beta, \\sigma^2),\\]\nand we want to \\(\\max_{\\beta, \\sigma^2} f_N(y, X, \\beta) \\approx e^{-\\frac{(y-X\\beta)^2}}.\\)\nIt is a happy coincidence that in both the ML and statistically driven approach, we derive that \\(\\hat \\beta = (X^T X)^{-1} X^T y\\), “our favorite equation.”\n\nConsider a Binomial Problem\nConsider the problem where we have a binary random variable \\(Y \\sim \\text{Bernoulli}(\\mu)\\) with pdf\n\\[f_Y(y) = \\mu^y(1-\\mu)^{1-y}.\\]\nIn this case, we assume that \\[Y \\mid X \\sim \\text{Bernoulli}(\\mathbb E(Y|X)),\\]\nand \\(\\mathbb E[Y|X] = \\text{P}(Y = 1)\\), therefore \\(0 \\leq \\mathbb E[Y|X] \\leq 1\\).\nThere are several ways to enforce this bounding. Let \\(\\phi : \\R \\to [0,1]\\).\n\nOne way could be to model \\(\\mathbb E(\\phi(Y) | X)\\), but this has two problems: interpretation, and more importantly, we’re not necessarily interested in the distribution of \\(\\phi(Y)\\).\nAnother option is to transform the \\(X\\) values so that \\(\\mathbb E(Y | X) = \\phi(X) \\beta\\).\nThe last way is to write \\(\\mathbb E(Y) = \\phi(X \\hat \\beta)\\). Another way to write this is to let \\(g = \\phi^{-1}\\) and write \\(g(\\mathbb E(Y|X)) = X \\hat \\beta.\\)\n\nWe have two constraints on \\(g\\): it has to convert values from \\(\\R\\) to \\([0,1]\\) as well as has nice interpretability.\n\n\n\n\nRespects Constraints\nInterpretable\n\n\n\n\n\\(g(y) = y\\)\nNo\nVery\n\n\nprobit\nYes\nNot very\n\n\n\n\n\n\n\n\n(a). Show that this distribution belongs to the exponential dispersion family and find expressions for \\(\\theta\\), \\(b(\\theta)\\), \\(a(\\phi)\\), and \\(c(y, \\phi)\\).\n\\[\n\\begin{aligned}\nf_Y(y) & = \\mu^y (1 - \\mu)^{1-y} \\\\\n& = \\exp \\{ \\log \\{ \\mu^y (1-\\mu)^{1-y} \\} \\} \\\\\n& = \\exp \\{ y \\log(\\mu) + (1-\\mu) \\log(1-\\mu) \\} \\\\\n& = \\exp \\{ y \\log \\left( \\frac{\\mu}{1-\\mu} + \\log(1-\\mu) \\right) \\} \\\\\n& = \\exp \\{ y \\theta - \\log(1 + \\exp(\\theta)) \\},\n\\end{aligned}\n\\]\nwhere \\(\\theta = \\log \\left( \\frac{\\mu}{1-\\mu} \\right)\\), \\(b(\\theta) = \\log(1 + \\exp(\\theta))\\), \\(a(\\phi) = 1\\), and \\(c(y,\\phi) = 0\\).\n\\(\\theta\\) is often a nice candidate for the link function.\n\nThe exponential dispersion family is a generalization of what’s called the natural exponential family.\n\nRecall that \\(\\text{expit}(X \\hat \\beta) = \\frac{e^{X\\beta}}{1-e^{X\\hat\\beta}}\\).\nConsider the simple logistic regression model for a binary outcome \\(y_i\\) and a single covariate \\(x_i\\)\n\\[\\text{\\text{logit}}(\\mu_i) = \\beta_0 + \\beta_1 x_i \\tag{1} \\]\nwhere \\(\\mu_i = P(Y_i = 1 | X_i = x_i) = \\mathbb E(Y_i | X_i = x_i)\\) and \\(\\text{logit}(\\mu_i) = \\log \\left( \\frac{\\mu_i}{1-\\mu_i} \\right)\\). Since we’ve already shown the Bernoulli distribution falls within the exponential dispersion family, it should be clear that model (1) is a GLM with the \\(\\text{logit}\\) link function.\n\\(\\beta_0\\) is the log odds of the outcome \\(Y\\) for subjects with \\(x_i = 0\\).\n\\[\\log \\left( \\frac{\\mu_i}{1-\\mu_i} \\right) = \\text{logit}(\\mu_i) = \\beta_0 + \\beta_1(0) = \\beta_0\\]\n\\(\\beta_1\\)\n\nAdvice for interpreting \\(\\beta_0\\) and \\(\\beta_1\\) for some random GLM on an exam:\nLook at the difference between an observation with \\(x^\\star\\) and \\(x^\\star + 1\\).\n\\[\\log \\left( \\frac{\\mu_i / (1-\\mu_i)}{\\mu_j/(1-\\mu_j)} \\right) =\n\\text{logit}(\\mu_i) - \\text{logit}(\\mu_j) = \\beta_0 + \\beta_1(x + 1) - \\beta_0 - \\beta_1x = \\beta_1\\]\n\nNow suppose we have collected \\(n\\) independent observations. Show that the log-likelihood based on model (1) is\n\\[\\ell(\\beta_0, \\beta_1 | y) = \\sum_{i=1}^n \\{ y_i (\\beta_0 + \\beta_1 x_i) - \\log(1 + \\exp(\\beta_0 + \\beta_1 x_i)) \\}.\\]\nAnswer:\n\\[L(\\theta) = \\prod \\exp \\{ y_i \\theta_i - \\log (1 + \\exp \\theta_i) \\}\\]\nTherefore\n\\[\\ell(\\theta | y) = \\sum y_i \\theta_i - \\log (1 + \\exp \\theta_i)\\] \\[\\ell(\\beta_0, \\beta_1, y) = \\sum y_i ( \\beta_0 + \\beta_1 x_i) - \\log (1 + \\exp(\\beta_0 + \\beta_1 x_i))\\]\nObtain the score equations \\(U(\\beta_0)\\) and \\(U(\\beta_1)\\).\n\\[\n\\begin{aligned}\nU(\\beta_0) = \\frac{\\partial \\ell}{\\partial \\beta_0} & = \\sum_{i=1}^n \\left\\{\n  y_i - \\frac{\\exp(\\beta_0 + \\beta_1 x_i)}{1 + \\exp(\\beta_0 + \\beta_1 x_i)}\n  \\right\\} \\\\\n  & = \\sum (y_i - \\mu_i) \\stackrel{\\text{set}}{=} 0\n  \\end{aligned}\n  \\]\n\\[\n\\begin{aligned}\nU(\\beta_1) = \\frac{\\partial \\ell}{\\partial \\beta_1} & = \\sum_{i=1}^n \\left\\{\n  y_i x_i - \\frac{x_i \\exp(\\beta_0 + \\beta_1 x_i)}{1 + \\exp(\\beta_0 + \\beta_1 x_i)}\n  \\right\\} \\\\\n  & = \\sum x_i (y_i - \\mu_i) \\stackrel{\\text{set}}{=} 0\n  \\end{aligned}\n\\]\nShow that the Fisher Information Matrix \\(\\mathcal I(\\beta)\\) takes the following form:\n\\[\\mathcal I(\\beta) = \\pmatrix{\n\\sum \\mu_i (1- \\mu_i) & \\sum x_i \\mu_i ( 1- \\mu_i) \\\\\n\\sum x_i \\mu_i (1-\\mu_i) & \\sum x_i^2 \\mu_i(1-\\mu_i)\n}.\\]\nWhere \\(\\mu_i = \\text{expit}(\\beta_0 + \\beta_1 x_i)\\) and \\(\\text{expit}(x) = \\frac{\\exp(x)}{1 + \\exp(x)}\\)\nThe Fisher Information Matrix is like the Hessian of the log-likelihood.\nThe higher the curvature, the lower the variance.\nAppendix:\n(Fill out soon…)"
  },
  {
    "objectID": "week13/week13.html",
    "href": "week13/week13.html",
    "title": "Week 13",
    "section": "",
    "text": "Binary Response Data: Case-Control Studies\nOutline:"
  },
  {
    "objectID": "week13/week13.html#study-design",
    "href": "week13/week13.html#study-design",
    "title": "Week 13",
    "section": "Study Design",
    "text": "Study Design\nSo far, we’ve considered estimation and inference based on an independent sample size of size \\(n\\), \\(\\{ (X_i, Y_i) : i = 1, ..., n \\}\\) and the likelihood:\n\\[\\mathcal L = \\prod_{i=1}^n P(Y_i | X_i)\\]\nWe parametrize \\(P(Y|X)\\) in terms of a regression model, \\(\\mu = \\mathbb E[Y|X,\\beta]\\).\nWe want to learn about the regression coefficients, \\(\\beta\\).\nWe’ve often implicitly assumed our data come from cross-sectional sampling where one chooses individuals completely at random and we observe their outcomes and/or covariates.\nIn a cross-sectional sample, \\((Y, X)\\) are jointly random, so that the likelihood is:\n\\[\\mathcal L_{\\text{joint}} = \\prod_{i=1}^n P(Y_i, X_i)\\] \\[ = \\prod_{i=1}^n P(Y_i | X_i) P(X_i)\\]\nHowever, we generally assume that the marginal covariate distribution, \\(P(X_i)\\) does not involve \\(\\beta\\). Thus we can base estimation/inference on\n\\[\\mathcal L = \\prod_{i=1}^n P(Y_i|X_i)\\]\nThis is because\n\\[\\log \\mathcal L = \\sum_i \\log P(Y_i | X_i) + \\sum_i \\log P(X_i).\\]\nIf \\(P(X_i)\\) doesn’t involve any \\(\\beta\\) terms, then when we go to maximize/optimize against \\(\\mathcal L\\) with respect to \\(\\beta\\), the \\(P(X_i)\\) terms will fall out.\nIn binary outcome settings, we often need a surprisingly large sample size to have reasonable power. Recall that \\(\\text{Power} = P(\\text{Reject } H_0 | H_0 \\text{ false})\\).\nPower analyses are commonly used in the design of studies to determine how large \\(n\\) should be in order to allow reasonable power to detect the anticipated effect size.\nFor some very simple analytic approaches (e.g., \\(t\\)-tests), there are closed form formulas for power under a given sample and effect size.\nHere we will conduct a power analysis using a simulation approach to both: 1) demonstrate that random sampling designs often lead to low power with (rare) binary outcomes, and 2) give a sense of how to do power analyses using simulations.\nWe conduct a simulation study to assess power to detect an association between a binary outcome and a binary exposure, conditional on several other variables.\nWe’ll look at power in a range of scenarios:\n\nEffect sizes (conditional ORs): 1.5, 2.0, and 3.0 (all considered large)\nRandom samples of size: 3,000 to 8,000\nFor each scenario (combination of OR and \\(n\\)), we\n\nSimulate many datasets with the given specs\nFit the model to each simulated dataset\nCompute the percent of datasets for which the null is rejected.\n\n\nWe’re going to assume that incidence in the population is about 5%.\n\n\nlibrary(tidyverse) \n\nORs <- c(1.5, 2.0, 3.0)\n\nn <- seq(3000, 8000, length.out = 8)\n\npct_to_odds <- function(pct) { pct / (1 - pct) }\nodds_to_pct <- function(o) { o / (1+o) }\nN_iters <- 100\n\npower_outcomes <- list()\n\nfor (or in ORs) {\n  for (sample_size in n) { \n    for (iteration in 1:N_iters) {\n      # simulate data\n      sample_size_exposed <- round(sample_size / 50)\n      sample_size_unexposed <- round(sample_size / 50 * 49)\n\n      df <- data.frame(\n        Y = c(\n          rbinom(n = sample_size_unexposed, size = 1, prob = .05),\n          rbinom(n = sample_size_exposed, size = 1, prob = odds_to_pct(pct_to_odds(.05) * or))),\n        X = c(rep('cat1', sample_size_unexposed), rep('cat2', sample_size_exposed))\n      )\n\n      # fit model \n      model <- glm(Y ~ X, family = binomial(), data = df)\n\n      # assess significance \n      reject_H0 <- broom::tidy(model)[[2,'p.value']] < 0.05\n\n      # store outcomes\n      power_outcomes[[length(power_outcomes) + 1]] <- c(\n        OR = or, sample_size = sample_size, significant = reject_H0\n      )\n    }\n  }\n}\n\npower_outcomes <- dplyr::bind_rows(power_outcomes)\n\npower_outcomes_summary <- power_outcomes |> \n  group_by(sample_size, OR) |> \n  summarize(pct_significant = sum(significant)/n())\n\nggplot(power_outcomes_summary, \n  aes(x = sample_size, y = pct_significant, color = factor(OR), shape = factor(OR))) + \ngeom_point() + \ngeom_line()\n\nA key reason why power is so low is because the outcome is somewhat rare (5%).\nWe get few events, leading to large standard errors, and therefore low power.\nRepeated simulations increasing the incidence (fixing \\(n=4000\\)), achieved by manipulating \\(\\beta_0\\), here we pick \\(\\beta_0\\) values yielding incidences from \\(0.05\\) to \\(0.25\\).\nConsidering varying incidence rates:\n\nlibrary(gt)\npwr_df <- tibble::tribble(\n  ~`Odds Ratio`, ~`0.05`, ~`0.10`, ~`0.15`, ~`0.20`, ~`0.25`,\n      1.5    ,  18.6 ,  24.5 ,  29.3 ,  30.8 ,  32.8 , \n      2.0    ,  42.9 ,  57.2 ,  65.3 ,  69.1 ,  70.5 \n)\n\ngt(pwr_df) |> \n  tab_header(\n    title = md(\"**Relationship of Incidence, Odds Ratios, and Statistical Power**\")\n  ) |> \n  tab_spanner(\n    columns = c(2:6), \n    label = \"Incidence Rate\"\n  )  \n\n\n\n\n\n  \n    \n      Relationship of Incidence, Odds Ratios, and Statistical Power\n    \n    \n    \n      Odds Ratio\n      \n        Incidence Rate\n      \n    \n    \n      0.05\n      0.10\n      0.15\n      0.20\n      0.25\n    \n  \n  \n    1.5\n18.6\n24.5\n29.3\n30.8\n32.8\n    2.0\n42.9\n57.2\n65.3\n69.1\n70.5\n  \n  \n  \n\n\n\n\n\nIt’s also important if the exposure is relatively rare. That combination of rare exposure and rare outcome is particularly damaging to power.\nAs incidence increases, power increases. But of course, we cannot and usually should not increase incidence in the population. However, we can manipulate the relative number of cases and non-cases that we observe in the data. I.e., we artificially inflate the observed incidence. For example, this can be done via a case-control design, sometimes called “outcome dependent sampling.”\nThe problem is that the sample is no longer representative of the target population. But this non-randomness is by design, under the control of researchers. Such designs can be referred to as biased sampling schemes and we can use statistical techniques to account for the non-random sampling.\nIn a case-control study, we initially stratify the population by outcome status, we know \\(Y = 0/1\\) for everyone a priori.\nWe proceed by sampling, at random, to get \\(n_1\\) cases (i.e., for whom \\(Y = 1\\)) and \\(n_0\\) non-cases or controls (i.e., for whom \\(Y = 0\\)).\nFor all \\(n = n_0 + n_1\\) sampled individuals, ‘observe’ the value of their covariates. It’s crucial that \\(X\\) is random and not \\(Y\\).\nThe appropriate likelihood is\n\\[\\mathcal L_R = \\prod_{i=1}^n P(X_i | Y_i)\\]\n\\[ = \\prod_{i=1}^{n_0} P(X_i | Y_i = 0) \\prod_{i=n_0+1}^{n} P(X_i | Y_i=1)\\]\nThis is often referred to as a retrospective likelihood. However, the scientific goal is typically to learn about the prospective associations. I.e., \\(P(Y|X)\\)\nHow do we learn about prospective associations from the retrospective likelihood?\nAs we’ve noted, case-control sampling is non-random with respect to the target population. We formalize this by introducing a random variable \\(S\\), which is an indicator of selection by the sampling scheme.\n\\[ S = \\left\\{ \\begin{array}{ll} 1 \\quad & \\text{ selected} \\\\ 0 & \\text{ not selected.} \\end{array} \\right.\\]\n\\(S\\) is a binary random variable with some probability \\(P(S = 1)\\).\nCross-sectional sampling is where selection is independent of \\((Y,X)\\), and \\(P(S = 1)\\) is constant. If we’re going to sample \\(n\\) people out of a population of size \\(N\\), then \\(P(S_i = 1) = n/N\\).\nIn case-control sampling, selection depends on outcome status \\(Y\\) and we could write \\(P(S=1 \\mid Y = y)\\).\nApplying Bayes’ rule, we can write the retrospective likelihood as\n\\[\\mathcal L_R = \\prod_{i=1}^n P(X_i | Y_i) = \\prod_{i=1}^n P(X_i | Y_i, S_i = 1)\\] \\[ = \\prod_{i=1}^n P(Y_i | X_i, S_i) \\frac{P(X_i | S_i = 1)}{P(Y_i | S_i = 1)}\\]\nFor now, let’s focus on the \\(P(Y | X, S = 1)\\) term. This looks similar to our quantity of interest, \\(P(Y|X)\\), and is something we can learn about from the case-control data.\nLet’s assume that the true model of the prospective associations in the target population of interest is given by\n\\[\\text{logit}P(Y=1 \\mid X)  = X' \\beta.\\]\nWe can derive an expression for \\(P(Y=1 \\mid X, S=1)\\) in terms of the true prospective association parameters of interest, \\(\\beta\\).\nApplying Bayes’ rule and noting that selection depends solely on \\(Y\\):\n\\[\\begin{aligned}\nP(Y = 1 \\mid X , S = 1) & = \\frac{P(S = 1 \\mid X, Y = 1) P(Y = 1 \\mid X)}{P(S = 1 \\mid X)} \\\\\n& = \\frac{P(S = 1 \\mid X, Y = 1) P(Y = 1 \\mid X)}{\\sum_{y=0}^1 P(S = 1 \\mid X, Y = y) P(Y= y | X)} \\\\\n& = \\frac{P(S = 1 \\mid Y = 1) P(Y = 1 \\mid X)}{\\sum_{y=0}^1 P(S = 1 \\mid Y = y) P(Y= y | X)} \\\\\n& = \\frac{\\pi_1 P(Y = 1 \\mid X)}{\\sum_{y=0}^1 \\pi_y P(Y= y | X)}, \\\\\n\\end{aligned}\\]\nwhere \\(\\pi_y = P(S = 1 \\mid Y = y)\\) (and \\(\\pi_1 = \\pi_{y = 1}\\)).\nSo\n\\[P(Y = 1 \\mid X, S = 1) = \\frac{\\pi_1 P(Y=1 \\mid X)}{\n  \\pi_1 P(Y=1 \\mid X) + \\pi_0 P(Y=0 \\mid X).\n}\\]\nIf we divide the numerator and denominator by \\(\\pi_0 \\times P(Y=0 \\mid X)\\),\n\\[\\begin{aligned}\nP(Y=1 \\mid X, S = 1) & = \\frac{\\frac{\\pi_1}{\\pi_0} \\overbrace{\\frac{P(Y = 1|X)}{P(Y=0|X)}}^{\\text{odds of outcome}}}{\n\\frac{\\pi_1}{\\pi_0}  \\frac{P(Y = 1|X)}{P(Y=0|X)} + \\underbrace{\\frac{\\pi_0}{\\pi_0} \\frac{P(Y = 0|X)}{P(Y=0|X)}}_{=1}\n} \\\\\n& = \\frac{\n  \\frac{\\pi_1}{\\pi_0} \\exp \\{ X' \\beta \\}\n}{\n  1 + \\frac{\\pi_1}{\\pi_0} \\exp \\{ X' \\beta \\}\n} \\\\\n& = \\frac{\n  \\exp \\{ \\log \\left( \\frac{\\pi_1}{\\pi_0} \\right) + X' \\beta \\}\n}{\n  1 + \\exp \\{  \\log \\left( \\frac{\\pi_1}{\\pi_0} \\right) + X' \\beta \\}\n} \\\\\n& = \\frac{\n  \\exp \\{ \\beta_0^* + \\beta_1 X_1 + ... + \\beta_p X_p \\}\n}{\n  1 + \\exp \\{ \\beta_0^* + \\beta_1 X_1 + ... + \\beta_p X_p \\}\n} \\\\\n& = \\text{expit}(X' \\beta^*)\n\\end{aligned}\n\\]\nwhere \\(\\beta_0^* = \\beta_0 + \\log\\left( \\frac{\\pi_1}{\\pi_0} \\right)\\) and the only difference between \\(\\beta\\) and \\(\\beta^*\\) is in the intercept term.\nWe see that \\(P(Y=1\\mid X, S=1)\\) has the same functional form as the desired logistic regression model, with only the intercept differing.\nIf the true \\(P(Y=1 \\mid X)\\) is given by a logistic regression, then so is \\(P(Y=1 \\mid X, S = 1)\\).\nThe odds ratio relationships between \\(X\\) and \\(Y\\) are preserved despite the selection process.\nThe intercept for the two logistic models are different, but usually we aren’t concerned about estimating/interpreting the intercept.\nRecall the retrospective likelihood is\n\\[\\mathcal L_R = \\prod_{i=1}^n P(X_i | Y_i) = \\prod_{i=1}^n P(Y_i | X_i, S_i = 1) \\frac{P(X_i | S_i = 1)}{P(Y_i|S_i=1)}\\]\nwhere we now know the form of \\(P(Y_i \\mid X_i, S_i = 1)\\).\nDo we need to worry about the \\(\\frac{P(X_i | S_i=1)}{P(Y_i|S_i=1)}\\) term? Or can we proceed with estimation of \\(\\beta\\) ignoring that term?\nIn theory, no, we cannot ignore this term.\nConsider that \\(P(Y=0 \\mid S=1)\\) and \\(P(Y=1 \\mid S = 1)\\) are fixed by design. This imposes constraints on \\(P(Y\\mid X, S = 1)\\) and thereby also on \\(\\beta\\), i.e.,\n\\[P(Y = 1 \\mid S = 1) = \\int_{\\mathcal X} P(Y = 1 \\mid X = x, S = 1) P(X = x) \\mathrm d x.\\]\nThis indicates that to obtain an estimate of \\(\\beta\\) via\n\\[\\mathcal L^* = \\prod_{i=1}^n P(Y_i \\mid X_i, S_i = 1),\\]\none must maximize over a constrained parameter space. E.g., using a constrained optimization procedure such as Lagrange multipliers.\nHowever, Prentice and Pyke (1979) showed that it turns out that the constrained MLE is the same as the unconstrained MLE, and the asymptotic variance is also the same.\nSo we can ignore the constraints imposed by the sampling scheme and we can proceed fitting the logistic regression as usual in case-control studies. These results only hold for logistic regression, i.e., binomial regression with the logit link.\nIt has also been shown that ordinal likelihood ratio tests are valid in case-control studies (Scott and Wild 1989).\nCaveats:\n\nWe cannot draw conclusions about \\(\\beta_0\\) without additional information.\nCannot perform prediction without additional information.\nCannot learn about other contrasts such as the relative risk without additional information.\n\nFurther, one must also be aware of a number of non-statistical issues such as issues associated with observational studies, appropriate selection of controls, and recall bias.\nRecall bias refers to the bias that people often don’t do a good job reporting (recalling) exposures that they were exposed to in the past, especially if it’s been a long time since the exposure (whatever “long time” means depends on context)."
  },
  {
    "objectID": "week13/week13.html#esophageal-cancer-example",
    "href": "week13/week13.html#esophageal-cancer-example",
    "title": "Week 13",
    "section": "Esophageal cancer example",
    "text": "Esophageal cancer example\nConsider data from a case-control study on the association between alcohol and tobacco consumption and risk of esophageal cancer conducted in France in the 1970s. Data are available directly in R’s datasets package.\nIn the sample, 200, 1,175 individuals are cases. 17% of the sample had esophageal cancer. Overall incidence in the U.S. is around 5 per 100,000.\nThe data also contain information on 3 covariates categorized in: age in years, tobacco consumption in gm/day, and alcohol consumption in gm/day."
  },
  {
    "objectID": "week14/week14.html",
    "href": "week14/week14.html",
    "title": "Week 14",
    "section": "",
    "text": "Preface\nWe’re continuing on to matched case-control studies and conditional logistic regression.\nReview Moral: Only logistic regression can be applied “same as always” in case control studies to estimate odds ratios as long as one doesn’t care about the intercept (which is biased).\nToday we’ll talk about difficulties with matched case control study.\nThe case-control design assumes the population can be stratified on the basis of \\(Y\\).\nSuppose we also have access to information on certain covariates for everyone: e.g., we know the sex and race of the baby, but we don’t (readily) have information on the mother or the pregnancy. We’ll denote this set of covariates by \\(Z\\).\nWe could impose balance on \\(Z\\) by drawing controls such that the distribution of \\(Z\\) is the same as in cases. E.g., each time you draw a case, only draw from the controls with the same (or very similar) values of \\(Z\\)."
  },
  {
    "objectID": "week14/week14.html#north-carolina-birth-example-data",
    "href": "week14/week14.html#north-carolina-birth-example-data",
    "title": "Week 14",
    "section": "North Carolina Birth Example Data",
    "text": "North Carolina Birth Example Data\nSuppose interest lies in the relationship between gestational age at birth and infant mortality, i.e., death within the first year of life.\nThe infants dataset has information on 225,152 births for all white and African-American births in North Carolina in 2003-04.\nInfant mortality is a rare event: 1,752 events in the available data constituting a rate of 8 per 1,000 births.\nIn the absence of these complete data, we would need to conduct a study."
  },
  {
    "objectID": "week14/week14.html#emulating-a-case-control-study",
    "href": "week14/week14.html#emulating-a-case-control-study",
    "title": "Week 14",
    "section": "Emulating a case-control study",
    "text": "Emulating a case-control study\nSuppose we have sufficient resources to collect information on \\(n=400\\) births.\nSimple random sampling would yield few cases, probably around the average of 3 deaths.\nA case-control design would be much more efficient:\n\nRandomly sample \\(n_1 = 200\\) cases from the 1,752 infant deaths\nRandomly sample \\(n_0 = 200\\) controls from the 223,400 non-deaths\nRetrospectively ‘observe’ their covariate values, \\(X\\), and\nInclude information on a number of potential confounders such as:\nMothers’ age, smoking during pregnancy, weight gained, baby’s race and sex\n\n\nload(here::here(\"data/NorthCarolina_data.dat\"))\n\n## create an indicator of case/control status\ninfants$Y <- as.numeric(infants$dTime < 365)\n\ntable(infants$Y)\n\n\n     0      1 \n223400   1752 \n\n## randomly sample 200 cases and 200 controls\ncases <- sample(c(1:nrow(infants))[infants$Y == 1], 200)\nconts <- sample(c(1:nrow(infants))[infants$Y == 0], 200)\ndataCC <- infants[c(conts, cases),]\n\n## confirming that we have 200 cases/controls in dataCC\ntable(dataCC$Y)\n\n\n  0   1 \n200 200 \n\n## check covariate distribution\ntapply(dataCC$sex, dataCC$Y, FUN = mean)\n\n    0     1 \n0.445 0.580 \n\ntapply(dataCC$race, dataCC$Y, FUN = mean)\n\n   0    1 \n0.20 0.48 \n\ntapply(dataCC$smoker, dataCC$Y, FUN = mean)\n\n    0     1 \n0.100 0.145"
  },
  {
    "objectID": "week14/week14.html#covariate-imbalance-in-case-control-studies",
    "href": "week14/week14.html#covariate-imbalance-in-case-control-studies",
    "title": "Week 14",
    "section": "Covariate imbalance in case-control studies",
    "text": "Covariate imbalance in case-control studies\nThere appears to be substantial imbalance in the distributions of several key covariates between the cases and controls.\nMany of these covariates are also likely to be strongly associated with gestational age at birth, suggesting that:\n\nStrong confounding may be present\nWe may need to adjust for a large number of covariates\nThere may be areas of covariate space (i.e., combinations of covariate values) for which there’s little or no variability in the exposure and/or outcome\n\nIt would be desirable to have greater balance between cases and controls. One way of achieving this is through matching.\n\nUsually in a GLM, we think of the \\(Y\\) as being random; however, in this setting, the \\(X\\) variables are themselves random.\nDo recall that we worked out a retrospective likelihood in logistic regression showing that it gives the same estimates as the prospective likelihood that we’re used to.\nThis means we can proceed treating \\(Y\\) as random conditional on \\(X\\) and get the same results as if we’d treated \\(X\\) as random."
  }
]