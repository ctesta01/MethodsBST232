[
  {
    "objectID": "week3/week3.html#variance-of-hat-beta",
    "href": "week3/week3.html#variance-of-hat-beta",
    "title": "Week 3",
    "section": "Variance of \\(\\hat \\beta\\)",
    "text": "Variance of \\(\\hat \\beta\\)\nThe variance of \\(\\hat \\beta\\) is expressed as the variance-covariance matrix\n\\[\\text{Var}(\\hat \\beta) = (X'X)^{-1} X' \\text{Var}(Y) X (X'X)^{-1}\\] \\[ = \\sigma^2 (X'X)^{-1}\\]\nIf we let \\(D = (X'X)^{-1}\\), the variance of \\(\\hat \\beta_j = \\sigma D_{jj}\\) and the covariance between \\(\\hat \\beta_i\\) and \\(\\hat \\beta_j\\) is \\(\\sigma^2 D_{ij}\\).\nHow would we get to this result?\nWe are using a shorthand where we denote \\((X'X)^TX' = A\\), and now we’re just looking at the \\(\\text{Var}(AY)\\). When we are working with the matrix-variance formula, we can rewrite \\(\\text{Var}(AY) = A\\text{Var}(Y)A'\\).\nPlugging in the formula for \\(A\\), we get to the above.\nRemember we said that \\(\\text{Var}(\\epsilon) = \\sigma^2\\) and \\(Y = X\\beta + \\epsilon\\) where the only randomness comes from \\(\\epsilon\\). In other words\n\\[\\text{Var}(\\hat \\beta) = (X'X)^{-1} X' \\text{Var}(Y) X (X'X)^{-1}\\] \\[ = (X'X)^{-1} X' \\text{Var}(X\\beta + \\epsilon) X (X'X)^{-1}\\] \\[ = (X'X)^{-1} X' \\text{Var}(\\epsilon) X (X'X)^{-1}\\] \\[ = (X'X)^{-1} X' \\sigma^2 I X (X'X)^{-1}\\] \\[ = \\sigma^2 (X'X)^{-1} X' I X (X'X)^{-1}\\] \\[ = \\sigma^2 \\cancel{(X'X)^{-1} X' X }\\underbrace{(X'X)^{-1}}_{\\stackrel{set}{=}D}.\\]\n\nGauss-Markov Theorem\n\nUnder the standard linear assumptions, \\(\\hat \\beta_{OLS}\\) is the best linear unbiased estimator (BLUE) for \\(\\beta\\).\nLinear unbiased estimator: \\(\\hat \\beta_{OLS}\\) is a linear combination of the observed \\(y\\) values (given that \\(\\hat \\beta = (X'X)^{-1}X'y\\) is a matrix of constants times a vector \\(y\\)) and is an unbiased estimator.\nIt’s “best” in the sense that it is the lowest variance (most precise).\n\nSo the Gauss Markov Theorem tells us that among all linear, unbiased estimators of \\(\\beta\\), \\(\\hat \\beta_{OLS}\\) has the lowest variance.\n\n\nSimple Linear Regression as a Special Case\nThe least squares estimators \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) can be expressed as\n\\[\\hat \\beta_0 = \\sum_{i=1}^n l_i y_i, \\quad \\hat \\beta_1 = \\sum_{i=1}^n k_i y_i,\\]\nwhere \\(l_i = \\frac{1}{n} - \\frac{\\bar x(x_i - \\bar x)}{\\sum_{i=1}^n (x_i - \\hat x)^2},\\) and \\(k_i = \\frac{(x_i - \\bar x)}{\\sum_{i=1}^n (x_i - \\bar x)^2}\\).\n\nVariance of LS Estimators\n\\[\\text{Var}(\\hat \\beta_0) = \\sigma \\left\\{ \\frac{1}{n} + \\frac{\\bar x^2}{\\sum_{i=1}^n (x_i-\\bar x)^2}\\right\\},\\] \\[\\text{Var}(\\hat \\beta_1) = \\sigma \\left\\{\\frac{1}{\\sum_{i=1}^n (x_i-\\bar x)^2}\\right\\},\\] \\[\\text{Cov}(\\hat \\beta_0, \\hat \\beta_1) = \\sigma \\left\\{ - \\frac{\\bar x}{\\sum_{i=1}^n (x_i-\\bar x)^2}\\right\\}.\\]\nThe variance-covariance matrix is\n\\[\\text{Var}(\\hat \\beta) = \\sigma(X'X)^{-1} = \\left[ \\begin{array}{cc} \\text{Var}(\\hat \\beta_0) & \\text{Cov}(\\hat \\beta_0, \\hat \\beta_1) \\\\ \\text{Cov}(\\hat \\beta_0, \\hat \\beta_1) & \\text{Var}(\\hat \\beta_1) \\end{array} \\right].\\]\n\n\nEstimation of \\(\\sigma^2\\)\nIn order to estimate \\(\\text{Var}(\\hat \\beta)\\), we need an estimator of \\(\\sigma^2\\):\nWe base this on the sum of squared errors (SSE):\n\\[SSE = (y - X\\hat\\beta)'(y-X\\hat \\beta)\\] \\[ = \\sum_{i=1}^n(y_i-x'_i\\hat\\beta)^2\\] \\[ = \\sum_{i=1}^(\\hat \\epsilon_i)^2\\]\n\\[\\hat \\sigma^2 = MSE = \\frac{SSE}{n - p - 1}.\\]\nThis estimator \\(\\hat \\sigma^2\\) is an unbiased estimator.\nThe \\(n-p-1\\) in the denominator is because we estimate \\(p+1\\) parameters and we divide by the degrees of freedom, which is \\(n - \\text{\\# things we had to estimate}\\). The Kutner book has a more rigorous presentation of why this is the right amount to divide by.\n\n\n\nNormality assumption\nIf we are willing to make the stronger assumption that \\(\\epsilon_i \\stackrel{iid}{\\sim} \\mathcal N(0, \\sigma^2)\\), then we can perform inference on \\(\\beta\\).\nFirst note that \\(\\epsilon_i \\stackrel{iid}{\\sim} \\mathcal N(0, \\sigma^2) \\Longrightarrow Y_i \\stackrel{ind}{\\sim} \\mathcal N(x_i'\\beta, \\sigma^2)\\), such that\n\\[f_Y(y_i|\\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left[ -\\frac{1}{2\\sigma^2} (y_i - x_i'\\beta)^2\\right]\\]\nNotice that the \\(Y_i\\) values are independent but not identically distributed.\nBefore we were just assuming that the \\(\\epsilon\\) values were uncorrelated, which in the special case of the normal distribution implies independence, but this isn’t necessarily so for other distributions.\nWe can then use maximum likelihood techniques to obtain \\[\\hat \\beta_{MLE} \\sim MVN_{p+1} \\left[ \\beta, \\sigma^2 (X'X)^{-1} \\right].\\]\n\n\nJoint Density\nRecap of maximum likelihood estimation.\nIn general, suppose we have data \\(Y_1, ..., Y_n\\), which are independent random variables with \\(Y_i\\) having probability density function \\[f_Y(y_i|\\theta)\\] where \\(\\theta\\) is a vector of unknown parameters.\nThen the joint density function of all the \\(y_i\\) given \\(\\theta\\) is the product of the individual densities\n\\[f(y_1, ..., y_n|\\theta) = \\prod_{i=1}^n f_Y(y_i|\\theta).\\]\n\nLikelihood Functions\nThe likelihood function of \\(\\theta\\) given the data has the same form as the joint pdf:\n\\[\\mathcal L(\\theta|y_1,...,y_n) = f(y_1, ..., y_n|\\theta) = \\prod_{i=1}^n f_Y(y_i|\\theta).\\]\nOf course this looks exactly the same as the joint density of the \\(Y_i\\) values, but instead this is a function of \\(\\theta\\) instead of a function of the \\(y_i\\) values.\nOnce you take a random sample of size \\(n\\), the \\(y_i\\) values are known, and the likelihood is considered as a function of unknown parameter \\(\\theta\\).\nThe likelihood function should still integrate to 1.\nThe MLE of \\(\\theta\\) is the value \\(\\hat \\theta\\) that maximizes the likelihood\n\\[\\mathcal L(\\theta | y_1, ..., y_n)\\]\nas a function of \\(\\theta\\).\nThe value \\(\\hat \\theta\\) that maximizes \\(\\mathcal L(\\theta)\\) also maximizes\n\\[\\ell(\\theta | y_1, ..., y_n) = \\log \\mathcal L(\\theta | y_1, ..., y_n).\\]\n\n\nSolving for MLE\n\\[ \\frac{\\partial \\ell}{\\partial \\theta} \\stackrel{set}{=} 0,\\]\nand technically we’re going to need to check that this is a maximum as opposed to a minimum, and we’ll do so by checking that\n\\[\\left[ \\frac{\\partial^2 \\ell}{\\partial \\theta^2} \\right]_{\\theta = \\hat \\theta} < 0.\\]\nIf we were in a matrix setting instead of a vector setting, we’d need to check that the matrix is negative definite for a maximum, or positive definite for a minimum.\nThe negative of the second derivative,\n\\[\\frac{-\\partial^2 \\ell(\\theta | y_1, ..., y_n)}{\\partial \\theta^2},\\]\nis called the information.\n\n\nReturning to MLE for Regression\nThus in the linear regression setting if we assume \\(\\epsilon_i \\stackrel{iid}{\\sim} \\mathcal N(0, \\sigma^2),\\) then \\(Y_i \\stackrel{ind}{\\sim} \\mathcal N(x'_i\\beta, \\sigma^2)\\) and \\[f_Y(y_i|\\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left[ -\\frac{1}{2\\sigma^2} (y_i - x_i'\\beta)^2\\right]\\]\n\\[\\mathcal L(\\beta, \\sigma^2 | y_1, ..., y_n) = \\prod_{i=1}^n f_Y(y_i|\\beta, \\sigma^2),\\]\nand\n\\[\\mathcal L(\\beta, \\sigma^2|y_1, ..., y_n) = \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n\n\\exp \\left[ - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i'\\beta)^2 \\right]\\] \\[= \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n\n\\exp \\left[ - \\frac{1}{2\\sigma^2} (y - X\\beta)'(y-X\\beta) \\right].\\]\nTurning to the log-likelihood function:\n\\[\\ell(\\beta, \\sigma^2 | y_1, ..., y_n) \\propto \\cancel{-n/2\\log(\\sigma^2)} \\underbrace{- \\frac{1}{2\\sigma^2} (y-X\\beta)'(y-X\\beta)}_{= \\frac{-1}{2\\sigma^2} S(\\beta)}.\\]\nThe values that maximize this log-likelihood with respect to \\(\\beta\\), call them \\(\\hat \\beta_{MLE}\\) are the same as those that minimize \\(S(\\hat \\beta)\\), i.e.,\n\\[\\hat \\beta_{MLE} = (X'X)^{-1}X'y\\]\nand it’s straightforward to show that\n\\[\\hat \\beta_{MLE} \\sim MVN_{p+1}\\left[ \\beta, \\sigma^2(X'X)^{-1} \\right].\\]\n\n\n\\(\\sigma^2_{MLE}\\)\nWhile the estimates for \\(\\hat \\beta\\) are the same for OLS vs. MLE, we have that \\[\\hat \\sigma^2_{MLE} = \\frac{1}{n}(y-X\\hat\\beta)'(y-X\\hat\\beta) = \\frac{(n-p-1)}{n}MSE\\]\nSo of note, the MLE for \\(\\beta\\) are the same as the least squares estimator. However the MLE for \\(\\sigma^2\\) is not.\nRecall that the least squares estimator of \\(\\sigma^2\\) is unbiased. The MLE of \\(\\sigma^2\\) is biased, although it is consistent: \\[\\lim_{n\\to\\infty} P(|\\hat\\sigma^2 - \\sigma^2| \\leq \\epsilon) \\to 1, \\, \\forall \\epsilon > 0.\\]"
  },
  {
    "objectID": "week3/week3.html#inference-in-linear-regression",
    "href": "week3/week3.html#inference-in-linear-regression",
    "title": "Week 3",
    "section": "Inference in Linear Regression",
    "text": "Inference in Linear Regression\nOften it’s of interest to determine if, collectively, a group of predictors significantly contribute to the variability in \\(y\\) given another group of predictors are in the model.\nCommon examples are:\n\nIs a categorical variable, represented by dummy variables, significant (analagous to the overall ANOVA F-test)?\nCan the effect of a predictor be represented as a linear effect or is a higher-level polynomial (i.e., using \\(x^2\\), \\(x^3\\), etc.) necessary?\nIs a model that contains only main effects adequate or do we need to incorporate a set of interactions between variables in the models?\n\n\nSum of squares decomposition\n\\[(y_i - \\bar y)^2 = ((y_i - \\hat y_i) + (\\hat y_i - \\bar y))^2\\]\nThen, when computing the sums of squares, we get\n\\[\\sum(y_i - \\bar y)^2 = \\sum_{i=1}^n (\\hat y_i - \\bar y)^2 + \\sum_{i=1}^n (y_i - \\hat y_i)^2,\\]\nwhich happily features a “freshman’s dream”.\nWe thus have that \\[SST = \\underbrace{SSR}_{\\text{explained by regression}} + \\underbrace{SSE}_{\\text{left over}},\\]\nwhere \\(SST = \\text{Sums of Squares Total}\\), \\(SSR = \\text{Sums of Squares Regression}\\), nad \\(SSE = \\text{Sums of Squares Error}\\).\n\nThe ANOVA-like table\nWe often will write something like this type of table:\n\n\n\n\n\n\n\n\n\n\nSource\n\\(SS\\)\n\\(\\text{df}\\)\n\\(\\text{MS}\\)\n\\(\\mathbb E[\\text{MS}]\\)\n\n\n\n\nRegression\n\\(SSR = \\hat \\beta'X'y-n\\bar y^2\\)\n\\(p\\)\n\\(\\frac{SSR}{p}\\)\n\\(\\sigma^2 + \\frac{\\beta'_Rx'_Cx_C\\beta_R}{p}\\)\n\n\nError\n\\(SSE = y'y - \\hat \\beta' X'y\\)\n\\(n-(p+1)\\)\n\\(\\frac{SSE}{n-(p+1)}\\)\n\\(\\sigma^2\\)\n\n\nTotal\n\\(SST=y'y - n \\bar y^2\\)\n\\(n-1\\)\n\n\n\n\n\nwhere \\(MS = \\text{Mean Square Error}\\), and \\[X_c = \\left( \\begin{array}{cccc}\nx_{11}-\\bar{x_1} & x_{12}- \\bar{x_2} & \\cdots & x_{1p}-\\bar{x_p} \\\\\nx_{21}-\\bar{x_1} & x_{22}- \\bar{x_2} & \\cdots & x_{2p}-\\bar{x_p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1}-\\bar{x_1} & x_{n2}- \\bar{x_2} & \\cdots & x_{np}-\\bar{x_p}\n\\end{array}\\right)\\]\n\n\nTesting for Groups of Predictors\nHow do we use this decomposition to test for a group of coefficients?\nThe hypothesis can be formulated as\n\\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_q = 0, q \\leq p\\] \\[H_1: \\text{ at least one of } \\beta_1, ..., \\beta_q \\neq 0.\\]\nAs an aside, tests of the overall regression and tests for a single variable fall within this framework as well:\nThe overall test:\n\\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_p = 0\\] \\[H_1: \\beta_j \\neq 0 \\text{ for at least one } j, j = 1,...,p\\]\nFor a single predictor:\n\\[H_0: \\beta_j = 0\\] \\[H_1: \\beta_j \\neq 0\\]\nWe like the property that testing for significance among a “group of coefficients” reduces in two special cases to either the overall test or a test for an individual coefficient.\nIf we consider the model in matrix form:\n\\[Y = X\\beta + \\epsilon,\\]\nto construct a test based on sums of squares, partition \\(\\beta\\) accordingly:\n\\[\\beta = (\\beta_1^, \\beta_2')',\\]\nwhere \\(\\beta_1\\) is a \\(q \\times 1\\) and \\(\\beta_2\\) is \\((p+1-q) \\times 1\\). We want to test the null hypothesis \\[H_0: \\beta_1 = 0\\] \\[H_1: \\beta_1 \\neq 1\\] and \\(\\beta_2\\) is left unspecified.\nDefining \\(X = \\left[ X_1, X_2 \\right]\\), rewrite the model as\n\\[Y = X_1 \\beta_1 + X_2 + \\beta_2 + \\epsilon.\\]\nNow our model is partitioned so we’re ready to test for significance among the predictors and \\(\\beta\\) coefficients of interest.\nThe full model has SSR expressed as\n\\[SSR(X) = \\hat \\beta' X' y - n \\bar y^2\\]\nand Mean Square Error\n\\[MSE(X) = \\frac{y'y - \\hat \\beta' X' y}{n-p-1}.\\]\nTo find the contribution of \\(X_1\\), fit the model assuming \\(H_0\\) is true. The reduced model is \\[Y = X_2 \\beta_2 + \\epsilon,\\] which yields \\[\\hat \\beta_2 = (X_2'X_2)^{-1}X_2'y \\quad \\text{ and } \\quad SSR(X_2) = \\hat \\beta_2' X_2' y - n' \\bar y^2.\\]\nThe regression sums of squares due to \\(X_1\\) given \\(X_2\\) is in the model is\n\\[SSR(X_1|X_2) = SSR(X) - SSR(X_2)\\]\nwith \\(q\\) degrees of freedom. This is known as the extra sum of squares due to \\(X_1\\) given \\(X_2\\).\nUnder the null hypothesis,\n\\(SSR(X_1|X_2)/\\sigma^2 \\sim \\chi_q^2\\) and \\(SSE/\\sigma^2 \\sim \\chi_{(n-p-1)}^2\\), and these quantities are independent.\nIn general, if one \\(\\chi^2\\) distribution has degrees of freedom \\(d_1\\) and another has \\(d_2\\), then \\((\\chi_{d_1}^2/d_2)/(\\chi_{d_2}^2/d_2) \\sim F_{d_1,d_2}\\) if the two are independent.\nSo we can test \\(H_0: \\beta_1 = 0\\) with the statistic \\[F = \\frac{SSR(X_1|X_2)/q}{MSE(X)} \\stackrel{H_0}{\\sim} F_{q,n-p-1}\\]\nThis \\(F\\) distributional result requires either\n\nnormality of errors \\(\\epsilon_i \\sim \\mathcal N(0,\\sigma^2)\\)\nlarge sample theory\n\nThere’s a handful of things above that we just have to take for granted assumed from a probability & inference class and don’t have time to re-prove here.\nThe \\(F\\) written above is an \\(F\\)-statistic (or \\(F\\)-distributed) because it is the quotient of two \\(\\chi^2\\)-distributed variables divided by their degrees of freedom.\nWith reasonably large sample size, \\(\\mathbb E[F_{q,n-p-1}] \\approx 1\\).\nFor example, one can see that if one runs the regressions:\n\nlm(data = mtcars, hp ~ rnorm(n = nrow(mtcars)))\nsummary(.Last.value)\n#> ... \n#> F-statistic: 0.06081 on 1 and 30 DF\n\nlm(data = mtcars, hp ~ mpg)\n#> ... \n#> F-statistic: 45.46 on 1 and 30 DF\n\nWe can think of this procedure as asking: Is the increase in the regression sums of squares associated with adding \\(q\\) additional predictors, given the presence of the remaining variables in the model, sufficient to warrant removing \\(q\\) additional degrees of freedom from the denominator of MSE?\nAdding an unimportant predictor may increase the MSE, which will increase the uncertainty in the regression coefficient estimates and the variance of \\(\\hat y\\) - so we should include only predictors that explain the response.\nNote however that for the purpose of explanation confounders may not reach significance at given level (e.g. \\(\\alpha = 0.05\\)) but still have a clinically relevant effect on both outcome and exposure and therefore affect the regression coefficients of interest.\n\n\nExample: Test for 2 BMI Terms, HERS Data\n\nlibrary(gt)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nhers <- readr::read_csv(here::here(\"data/hers.csv\"))\n\nRows: 2763 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): HT, raceth, nonwhite, smoking, drinkany, exercise, physact, globra...\ndbl (24): age, medcond, weight, BMI, waist, WHR, glucose, weight1, BMI1, wai...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhers$BMIc <- hers$BMI - mean(hers$BMI, na.rm=TRUE)\n\nlm.ldl.interact <- \n  lm(data = hers %>% filter(! is.na(BMIc)), LDL ~ BMIc*statins + age + nonwhite + drinkany + smoking)\n\nlm.ldl.noBMI <- \n  lm(data = hers %>% filter(! is.na(BMIc)), LDL ~ statins + age + nonwhite + drinkany + smoking)\n\n# perform f-test using anova(reducedModel, fullModel)\nbmi.test <- broom::tidy(anova(lm.ldl.noBMI, lm.ldl.interact))\n\n## format and print results table \ngt(bmi.test) %>% \n  tab_header(title = md(\"**Test of significance of BMI**\"),\n             subtitle = md(\"From LDL model with BMI * statin interaction\")) %>% \n  cols_width(term ~ px(375)) %>% sub_missing(missing_text = '') %>% \n  fmt_number(columns=c('statistic','p.value'),decimals=3) %>% \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Test of significance of BMI\n    \n    \n      From LDL model with BMI * statin interaction\n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LDL ~ statins + age + nonwhite + drinkany + smoking\n2739\n3725955\n\n\n\n\n    LDL ~ BMIc * statins + age + nonwhite + drinkany + smoking\n2737\n3707501\n2\n18454.31\n6.812\n0.001\n  \n  \n  \n\n\n\n\n\n\n\nOverall Test\nUnder the null hypothesis, \\(SSR/\\sigma^2 \\sim \\chi^2_p\\) and \\(SSE/\\sigma^2 \\sim \\chi^2_{n-(p+1)}\\) are independent.\nTherefore we have \\[F = \\frac{SSR/p}{SSE/[n-(p+1)]} = \\frac{MSR}{MSE} \\stackrel{H_0}{\\sim} F_{p,n-p-1}\\]\nWe note that this is reported automatically in a lm().\n\noverall.test <- broom::tidy(anova(lm(data = hers, LDL ~ BMI + age)))\n\ngt(overall.test) %>% \n  tab_header(title = md(\"**Overall test**\"),\n             subtitle = md(\"Model of LDL with BMI and age\")) %>% \n  sub_missing(missing_text = '') %>% \n  fmt_number(columns = c('statistic', 'p.value'), decimals = 3) %>% \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n      Overall test\n    \n    \n      Model of LDL with BMI and age\n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    BMI\n1\n14446.022\n14446.022\n10.155\n0.001\n    age\n1\n7567.195\n7567.195\n5.320\n0.021\n    Residuals\n2744\n3903361.455\n1422.508\n\n\n  \n  \n  \n\n\n\n\nWe can interpret the entries above in the sumsq column as \\(SSR(BMI)\\) and then \\(SSR(age|BMI)\\). These are called the “extra sums of squares” contributed by each variable, and sometimes called the “type 1 sums of squares” (no relation to Type 1 error, but more of a historical idiosyncrasy as a result of how old software [either SAS or S or S-plus] printed these out).\n\\[F = \\frac{(14446 + 7567)/2}{3903361/2744} = 7.74\\]\nUnder \\(H_0\\), \\(F \\sim F_{2, 2744}\\), yielding \\(p = 0.0004458\\).\nWe reject the null hypothesis at \\(\\alpha = 0.05\\) and conclude that at least one of \\(\\beta_1\\) or \\(\\beta_2\\) is not equal to zero.\nOne should note that the above table is one of the places in which order matters because each \\(SSR\\) is conditional on the inclusion of the previously listed variables."
  },
  {
    "objectID": "week3/week3.html#wald-tests",
    "href": "week3/week3.html#wald-tests",
    "title": "Week 3",
    "section": "Wald Tests",
    "text": "Wald Tests\nFor testing individual coefficients \\((H_0: \\beta_j = 0\\) vs \\(H_1: \\beta_j \\neq 0\\)) we can also use the conventional Wald test. To construct the test statistic, consider that\n\\[\\hat \\beta_j \\sim \\mathcal N(\\beta_j, \\sigma^2) D_{jj} \\quad \\text{ and } \\quad\n\\frac{\\hat{\\text{Var}} (\\hat \\beta_j)}{\\sigma^2 D_{jj}} \\sim \\frac{\\chi^2_{n-p-1}}{(n-p-1)}.\\]\nNote that if \\(Z \\sim \\mathcal N(0,1)\\) and \\(S \\sim \\chi^2_d\\) and \\(Z \\perp\\!\\!\\!\\perp S\\) then \\(\\frac{Z}{\\sqrt{S/d}} \\sim t_d\\).\n\\[\\left( \\frac{\\hat \\beta_j - \\beta_j}{\\sqrt{\\sigma^2 D_{jj}}} \\right) \\biggr /\n\\left( \\sqrt{\\frac{\\widehat{\\text{Var}}(\\hat \\beta_j)}{\\sigma^2D_{jj}}} \\right) = \\underbrace{\\boxed{\\frac{\\hat \\beta_j - \\beta_j}{\\sqrt{\\widehat{\\text{Var}}(\\hat \\beta_j)}}}}_{\\text{this should look like a t-statistic}} \\stackrel{H_0}{\\sim} t_{n-p-1}\\]\nIt should be noted that a \\(t^2\\) value where \\(t\\) is a \\(t\\)-statistic follows an \\(F\\)-distribution. This implies that in the case of testing a single coefficient, the \\(t\\)-test and the \\(F\\)-test give the exact same results.\nIn the summary() function, the \\(p\\)-values shown will be from \\(t\\)-tests for each \\(\\beta_j\\), while the \\(F\\)-statistic shown is for the overall model.\n\\[E(LDL_i) = \\beta_0 + \\beta_1 BMI_i + \\beta_2 Age_i\\]\n\nwald.test <- broom::tidy(lm(data = hers, LDL ~ BMI + age))\ngt(wald.test) |> \n  tab_header(title = \n               md(\"**Individual coefficient Wald test**\"),\n             subtitle = \"Test of BMI in model of LDL with age already included\") |> \n  fmt_number(decimals = 3) |> \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n      Individual coefficient Wald test\n    \n    \n      Test of BMI in model of LDL with age already included\n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n151.443\n8.774\n17.260\n0.000\n    BMI\n0.367\n0.132\n2.778\n0.006\n    age\n−0.253\n0.110\n−2.306\n0.021\n  \n  \n  \n\n\n\n\n\\(T = 0.366/0.132 = 2.78 \\quad p = 0.0006\\)\nThis Wald testing strategy extends to testing groups of cofficients\n\\[Y = X_1 \\beta_1 + X_2 \\beta_2 + \\epsilon\\]\nwhere \\(\\beta_1\\) is \\(q \\times 1\\) and \\(\\beta_2\\) is \\((p + 1 - q) \\times 1\\).\n\\[H_0: \\beta_1 = 0\\] \\[H_1: \\beta_1 \\neq 0\\]\nThe multivariate Wald test statistic is \\[W = \\hat \\beta_1' \\left[ \\widehat{\\text{Var}}(\\hat \\beta_1 ) \\right]  \\hat \\beta_1\\]\nUnder the null,\n\n\\((1/q)W \\sim F_{q,n-p-1}\\)\nAsymptotically, \\(W \\sim \\chi_q^2\\)\n\n\nwald.test.group <- broom::tidy(lm.ldl.interact)\n\ngt(wald.test.group) |> \n  tab_header(title = \n               md(\"**LDL model with BMI * statin interaction**\")) |> \n  fmt_number(decimals = 3) |> \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n      LDL model with BMI * statin interaction\n    \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n162.405\n7.583\n21.416\n0.000\n    BMIc\n0.582\n0.160\n3.636\n0.000\n    statinsyes\n−16.253\n1.469\n−11.066\n0.000\n    age\n−0.173\n0.111\n−1.563\n0.118\n    nonwhiteyes\n4.073\n2.275\n1.790\n0.074\n    drinkanyyes\n−2.075\n1.467\n−1.415\n0.157\n    smokingyes\n3.110\n2.167\n1.435\n0.151\n    BMIc:statinsyes\n−0.702\n0.269\n−2.606\n0.009\n  \n  \n  \n\n\n\n\nIn this scenario, \\(H_0: \\beta_2 - \\beta_8 = 0\\).\n\n## Generic function for a Wald test from the output of lm()\n\nwaldTest <- function(fit, vec, digits=c(2,4)) {\n  \n  beta     <- coef(fit)[vec]\n  varMat   <- summary(fit)$cov.unscaled[vec,vec] * (summary(fit)$sigma^2)\n  testStat <- t(beta) %*% solve(varMat) %*% beta \n  pVal     <- 1 - pchisq(testStat, length(vec))\n  value    <- c(Fstat = round(testStat, digits=digits[1]),\n             p = round(pVal, digits=digits[2]))\n  return(value)\n}\n\nwaldTest(lm.ldl.interact, vec = c(2,8))\n\n  Fstat       p \n13.6200  0.0011"
  },
  {
    "objectID": "week3/week3.html#testing-general-linear-hypotheses",
    "href": "week3/week3.html#testing-general-linear-hypotheses",
    "title": "Week 3",
    "section": "Testing general linear hypotheses",
    "text": "Testing general linear hypotheses\nSuppose we are interested in testing linear combinations of the regression coefficients. For example, we may be interested in testing\n\\[H_0: \\beta_i = \\beta_j\\]\nequivalently \\(H_0: \\beta_i - \\beta_j = 0\\).\nSuch hypotheses can be expressed as \\(H_0: C \\beta = 0\\).\nWhere \\(C\\) is an \\(r \\times (p+1)\\) matrix of linearly independent contrasts with \\(r\\) the number of restrictions imposed by the null.\nFor example, consider the model \\[Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i3} + \\epsilon_i,\\]\nand testing the hypothesis \\[H_i : \\beta_1 = 0, \\beta_2 = \\beta_3\\]\nThis could also be written as \\[\\left( \\begin{array}{c} \\beta_1 \\\\ \\beta_2 - \\beta_3 \\end{array} \\right) =\n\\left( \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right)\\]\nThis null hypothesis is equivalent to \\[H_0: \\left( \\begin{array}{cccc} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & -1 \\end{array} \\right)\\beta = 0\\] were \\(\\beta = (\\beta_0, \\beta_1, \\beta_2, \\beta_3)'\\).\nWe can obtain the reduced model by solving \\(C\\beta\\) for \\(r\\) of the regression coefficients in terms of the remaining \\(p+1-r\\) regression coefficients. Substituting these values into the full model will yield a reduced model under the null hypothesis,\n\\[Y = Z \\gamma + \\epsilon,\\]\nwhere \\(\\dim(Z) = n \\times (p+1 -r)\\) matrix and \\(\\dim(\\gamma) = (p + 1 - r) \\times 1\\) vector of regression coefficients.\nThe residual SS for this reduced model is \\[SSE(RM) = y'y - \\hat\\gamma' Z'y \\quad \\quad (n - p - 1 + r \\, \\text{ degrees of freedom})\\]\n\\(SSR(\\text{Full Model}) - SSR(\\text{Reduced Model})\\) is called the sum of squares due to the hypothesis \\(C\\beta=0\\).\nWe can test this hypothesis using \\[F = \\frac{(SSR(FM)-SSR(RM))/r}{MSE} \\stackrel{H_0}{\\sim} F_{r,n-p-1}.\\]\n\nExample with HERS data\nConsider using the physical activity score (1-5):\n\n\n\nPhysact\nActivity\n\n\n\n\n1\nMuch less active\n\n\n2\nSomewhat less active\n\n\n3\nAbout as active\n\n\n4\nSomewhat more active\n\n\n5\nMuch more active\n\n\n\nAn ANOVA model for glucose level regressed on physical activity is\n\\[E(glucose_i) = \\beta_0 + \\beta_1D_{i1} + \\beta_2D_{i2} + \\beta_3D_{i3} + \\beta_4 D_{i4}\\]\nQuestion: For the purposes of predicting glucose level, is the cruder physical activity categorization below adequate?\n|Collapsed Physact| Activity | |1| Less active | |2| About as active | |3| More active |\nRecall the full model is\n\\[E(glucose_i) = \\beta_0 + \\beta_1D_{i1} + \\beta_2D_{i2} + \\beta_3D_{i3} + \\beta_4 D_{i4}\\]\nand this corresponds to the null hypothesis\n\\[H_0: \\beta_1 = \\beta_2, \\beta_4 = 0\\]\nand the reduced model \\[E(glucose_i) = \\beta_0 + \\beta_1(D_{i1} + D_{i2}) + \\beta_3D_{i3}\\]\n…"
  }
]