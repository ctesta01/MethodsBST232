[
  {
    "objectID": "week12/week12.html#continuing-on-link-functions",
    "href": "week12/week12.html#continuing-on-link-functions",
    "title": "Week 12",
    "section": "Continuing on Link Functions",
    "text": "Continuing on Link Functions\n\nLinear (identity) link function\nWhen we use the identity link function with binary outcomes, we call this a linear probability model.\n\\[ \\mu_i = \\beta_0 + \\beta_1 x_i \\]\nThe contrast modeled is the risk difference (RD).\n\\(\\beta_0\\) is the probability of response when \\(x = 0\\); \\(\\beta_1\\) is the change in probability of response when \\(x\\) differs by 1 unit.\nWe’ve noted before that this doesn’t respect the \\([0,1]\\) bounds on the response probability.\nWe could also write \\(\\beta_1 = \\mu[x^\\star + 1] - \\mu[x^\\star]\\).\n\n\nLog link function\nSuppose we choose the log link. The systematic component of the model is\n\\[\\log(\\mu_i) = \\beta_0 + \\beta_1 x_i\\]\nWe interpret \\(\\beta_0\\) as the log of the probability of response when \\(x = 0\\). \\(\\exp(\\beta_0)\\) is the probability of response when \\(x=0\\).\nWe interpret \\(\\beta_1\\) as the change in the log of the probability of response, comparing two populations whose values of \\(x\\) differs by 1 unit.\n\\(\\exp(\\beta_1)\\) is the ratio of the probability of response when comparing \\(x^\\star + 1\\) to \\(x^\\star\\).\n\\[\\beta_1 = \\log(\\mu[x^\\star + 1]) - \\log(\\mu[x^\\star]) = \\log(\\frac{\\mu[x^\\star + 1]}{\\mu[x^\\star]}),\\] \\[\\exp(\\beta_1) = \\frac{\\mu[x^\\star + 1]}{\\mu[x^\\star]}.\\]\nThe contrast we’re modeling here is the risk ratio.\nThe log link doesn’t necessarily respect the fact that the true response probability has an upper bound of 1. We can see this by considering the inverse of the link function: \\[\\mu_i = \\exp\\{ \\beta_0 + \\beta_1 x_i \\}\\] which takes values on \\((0, \\infty)\\).\n\n\nLogit link function\nThe logit link is the canonical link function for binary outcomes, and the GLM for Bernoulli outcomes using the logit link is known as logistic regression. The systematic component is:\n\\[\\text{logit}(\\mu_i) = \\log \\left( \\frac{\\mu_i}{1-\\mu_i} \\right) = \\beta_0 + \\beta_1 x_i. \\]\nThe functional \\[\\frac{\\mu_i}{1-\\mu_i} = \\frac{P(Y_i = 1 \\mid x_i)}{P(Y_i = 0 \\mid x_i)}\\]\nis the odds of response.\nWe interpret \\(\\beta_0\\) as the log of the odds of response when \\(x = 0\\). \\(\\exp(\\beta_0)\\) is the odds of response when \\(x = 0\\).\n\\[\\beta_1 = \\log\\left( \\frac{\\mu[x^\\star + 1]}{1-\\mu[x^\\star+1]}\\right) - \\log \\left( \\frac{\\mu[x^\\star]}{1 - \\mu[x^\\star]} \\right).\\]\n\\[\\beta_1 = \\log \\left( \\frac{\\text{odds}(x^\\star+1)}{\\text{odds}(x^\\star)} \\right)\\]\nWe typically report \\(\\exp(\\beta_1)\\), which is the ratio of the odds when \\(x = x^\\star + 1\\) to when \\(x = x^\\star\\).\nThe inverse of the logit link function is called the expit function:\n\\[\\mu_i = \\frac{\\exp \\{ \\beta_0 + \\beta_1 x_i \\} }{1 + \\exp \\{ \\beta_0 + \\beta_1 x_i \\} }.\\]\n\n\nInverse CDFs as link functions for binary outcomes\nThe logit function (the link for logistic regression) is the inverse CDF of the standard logistic distribution.\nThe CDF (of any distribution) provides a mapping from th support of the random variable to the \\((0,1)\\) interval.\n\\[\\mathbb{F}_X(\\cdot) : (-\\infty, \\infty) \\to (0,1)\\]\nIt’s relatively common to use inverse CDFs as link functions.\n\\[\\mathbb{F}^{-1}_X(\\cdot) : (0,1) \\to (-\\infty, \\infty)\\]\n\\(g(\\cdot) \\equiv \\mathbb F^{-1}(\\cdot)\\) maps \\(\\mu \\in (0,1)\\) to \\(\\eta \\in (-\\infty, \\infty)\\).\n\n\nProbit Link Function\n\\[\\text{probit}(\\mu_i) = \\Phi^{-1}(\\mu_i) = \\beta_0 + \\beta_1 x_i\\]\nwhere \\(\\Phi(\\cdot)\\) is the CDF of the standard normal distribution.\nInterpret \\(\\beta_0\\) as the probit of probability of response when \\(x = 0\\).\nInterpret \\(\\beta_1\\) as the change in the probit of the probability of response, comparing two populations whose values of \\(x\\) differs by 1 unit.\nInterpretation is tricky:\n\nContrast is in terms of the inverse CDF of a standard normal distribution.\nThere is no easy way of relating this contrast to more intuitive measures.\n\n\\(\\beta_1\\) can be viewed as changes in \\(z\\)-scores in the response probability.\n\n\nComplementary log-log\n\\[\\log(-\\log(1-\\mu_i)) = \\beta_0 + \\beta_1 x_i\\]\nInverse CDF of the extreme value (or log-Weibull) distribution.\nAs with the probit function, there isn’t any intuitive way of interpreting regression parameters based on this link function.\nHas the distinction that it is asymmetric.\n\n\nComparing Links\n\nModels using logit and probit links should give very similar predictions.\nShown by Amemiya (1981) there is an approximate relationship between the coefficients from models using the logit and probit links:\n\\[\\beta_1^{\\text{logit}} \\approx 1.6 \\beta_1^{\\text{probit}}\\]\nAmemiya (1981) also shows that when \\(\\mu \\in (0.3,0.7)\\), there is an approximate relationship between the coefficients from models using the logit and linear links:\n\\[\\beta_1^{\\text{logit}} \\approx \\frac{\\beta_1^{\\text{\\linear}}}{4}.\\]\nThis might come from a relatively simple Taylor expansion of the expit function.\n\nWhen working with rare outcomes, the log, logit, and complementary log-log links give us pretty much the same things. When working with more common outcomes, the different links give us quite different outcomes.\nFrom the figures, differences across these link functions manifest primarily in the tails when probability of response is small or large.\nAlso the logit and probit functions are almost linearly related.\nFor small values of \\(\\mu_i\\), the complementary log-log, logit, and log functions are close to each other.\n\nEqually good for rare events\nFor \\(\\mu_i \\leq 0.1\\) \\[\\log \\left( \\frac{\\mu_i}{1-\\mu_i} \\right) \\approx \\log(\\mu_i) \\]\nlog link has the best interpretation\nOR and RR are close numerically\n\nWhat people often do is fit logistic regression, estimate an odds ratio, and then say because the OR is a rare outcome, it’s also approximately a rate ratio/relative risks. Why not just fit the log-link model? It may be computationally challenging/tricky."
  },
  {
    "objectID": "week10/week10.html",
    "href": "week10/week10.html",
    "title": "Week 10",
    "section": "",
    "text": "Generalized Linear Models\nOur goal is to develop statistical models to characterize the relationship between some response variable \\(Y\\) and a vector of covariates \\(x\\).\nStatistical models consist of two components:\nWhen moving beyond linear regression analysis of continuous and response data, we need to be aware of two key challenges: * Sensible specification of the systematic component * Proper accounting of any implicit mean-variance relationships arising from the random component.\nDefinition of a Generalized Linear Model\nA generalized linear model (GLM) specifies a parametric statistical model for the conditional distribution of a response \\(Y_i\\) given a \\(p\\)-vector of covariates \\(x_i\\).\nConsistes of three elements:\nThe first of these elements is the random component, and elements 2 and 3 jointly specify the systematic component.\nIn practice, we see a wide range of response variables with a wide range of associated (possible) distributions\nFor a given choice of probability distribution, a GLM specifies a model for the conditional mean:\n\\[ \\mu_i = \\mathbb E[Y_i | x_i] \\]\nHow do we specify a reasonable model for \\(\\mu_i\\) while ensuring that we respect the appropriate range/scale of \\(\\mu_i\\)?\nAchieved by constructing a linear predictor \\(x_i' \\beta\\) and relating it to \\(\\mu_i\\) via a link function \\(g(\\cdot)\\):\n\\[g(\\mu_i) = x_i' \\beta.\\]\nWe often specify \\(g(\\cdot)\\) such that \\(g^{-1}(x_i' \\beta) = \\mu\\) respects the appropriate bounds on \\(\\mu_i\\).\nWe’ll sometimes use the shorthand \\(\\eta_i = x_i' \\beta\\).\nThe MLE of \\(\\theta\\), denoted \\(\\hat \\theta_{MLE}\\) is obtained by solving the score equations with respect to \\(\\theta\\):\n\\[U(\\theta | y) = \\frac{\\partial}{\\partial \\theta} \\ell (\\theta | y) = 0\\]\nUnder some regularity conditions,\n\\[\\hat \\theta_{MLE} \\sim \\text{MVN}_p (\\theta, \\mathcal I_n(\\theta)^{-1})\\]\nwhere \\(\\mathcal I_n(\\theta)\\) is the Fisher information matrix with\n\\[\\mathcal I_n(\\theta)_{j,k} = -\\mathbb E\\left[ \\frac{\\partial^2 \\ell (\\theta | y)}{\\partial \\theta_j \\partial \\theta_k \\right]\\]\nWe can estimate \\(\\text{Var}(\\hat \\theta_{MLE}) \\approx \\mathcal I_n(\\theta)^{-1}\\) by plugging in \\(\\hat \\theta_{MLE}\\).\nThe Wald test is measuring the distance between the MLE and the null value.\n\\[(\\hat \\theta_{1,MLE} - \\theta_{1,0})^{T} \\widehat{\\text{Var}}(\\hat \\theta_{1,MLE})^{-1} (\\hat \\theta_{1,MLE} - \\theta_{1,0}) \\stackrel{d}{\\longrightarrow} \\chi_q^2 \\]\nThe Score test finds the first derivative of the log likelihood at the null value. If this derivative is large, then that means that this null value is far away from the MLE.\n\\[U(\\hat \\theta_{0,MLE} | y)' \\mathcal I_n(\\hat \\theta_{0,MLE})^{-1} U(\\hat \\theta_{0, MLE} | y)  \\stackrel{d}{\\longrightarrow} \\chi_q^2 \\]\nThe Likelihood ratio test\n\\[2 \\left( \\ell(\\hat \\theta_{MLE} | y) - \\ell(\\hat \\theta_{0,MLE}|y) \\right)  \\stackrel{d}{\\longrightarrow} \\chi_q^2 \\]\nAsymptotically, these should all give the same results, but in small data settings these may differ.\nThe Fisher information is essentially the negative second derivative of the likelihood function. Since the variance of \\(\\hat \\theta_{MLE}\\) can be estimated by \\(\\approx \\mathcal I_n(\\theta)^{-1}\\), if the likelihood function is a very steep (downward) parabola, its second derivative will have large magnitude, and its inverse will thus be small.\nThe systematic component is the specification of how the distribution \\(Y \\sim f_Y(y; \\theta, \\phi)\\) (where \\(f_Y\\) is an exponential family distribution) depends on the covariates \\(x_i\\).\nIn GLMs we model the conditional mean \\(\\mu_i = \\mathbb E[Y_i | x_i].\\)\nThis provides a connection between \\(x_i\\) and the distribution of \\(Y_i\\) via the canonical parameter \\(\\theta_i\\) and the cumulant function \\(b(\\theta_i)\\).\n\\[f_Y(y) \\longrightarrow \\theta_i \\longrightarrow \\mu_i \\longrightarrow x_i'\\beta \\quad \\text{(a.k.a. }\\eta_i)\\]\nTypically we link the linear predictor of the distribution of \\(Y\\) via a transformation of \\(\\mu_i\\), \\(g(\\cdot)\\), so that\n\\[g(\\mu_i) = x_i'\\beta\\]\nTraditionally this is broken into two parts:\nSometimes you’ll find the ‘predictor component’ of the model called the ‘systematic component’, as in McCullough and Nelder (1989). In practice, one cannot consider one without the other since the relationship between \\(\\mu_i\\) and \\(x_i\\) are jointly determined by \\(\\beta\\) and \\(g(\\cdot)\\).\nConstructing the linear predictor for a GLM follows the same process one uses for linear regression.\nGiven a set of covariates \\(x_i\\), there are two decisions.\nFor the most part, decisions to include covariates should be driven by scientific considerations\nWe will see that there are some differences (from multiple linear regression) when it comes to identification of confounders.\nIn the GLM framework, we’ll want to note that the inverse of the link function provides the specification of the model on the scale of \\(\\mu_i\\)\n\\[\\mu_i = g^{-1}(x_i' \\beta)\\]\nso we often choose \\(g()\\) such that \\(g^{-1}(x_i' \\beta)\\) respects any bounds on \\(\\mu_i\\).\nWe interpret the link function as specifying a transformation of the conditional mean, \\(\\mu_i\\).\nWe are not specifying a transformation of the response \\(Y_i\\).\nWe are writing \\(g(\\mathbb E(Y_i|x_i))\\), not \\(\\mathbb E(g(Y_i) | x_i) = x_i'\\beta\\).\nRecall that the mean and the canonical parameter are linked via the derivative of the cumulant function.\n\\[\\mathbb E[Y_i] = \\mu_i = b'(\\theta_i)\\]\nAn important link function is the canonical link:\n\\[g(\\mu_i) = \\theta(\\mu_i) = b'^{-1}(\\mu_i),\\]\ni.e., the function that results by viewing the canonical parameter \\(\\theta_i\\) as a function of \\(\\mu_i\\).\nWe’ll see later that this choice results in some mathematical convenience.\nGiven an i.i.d. sample of size \\(n\\), the log-likelihood is\n\\[\\ell(\\beta, \\phi; y) = \\sum_{i=1}^n \\left[ \\frac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c(y_i, \\phi)\\right]\\]\nThroughout this course, we’ve discussed “asymptotic” properties of estimators, including consistency \\((\\hat \\theta \\stackrel{p}{\\to} \\theta)\\). We’ve also discussed asymptotic tests — for example, F-tests which converge to a \\(\\chi^2\\) distribution in large samples. However, previous discussions have been limited to models involving normally distributed rnadom variables.\nAsymptotic likelihood theory provides a framework by which we can construct estimators with optimal properties for arbitrary distributions, as well as asymptotic tests based on those estimators. Consider an analysis of a random sample such that\n\\[Y_1, ..., Y_n \\stackrel{iid}{\\sim} f(y_i \\mid \\theta)\\]\nand we are interested in drawing inference \\(\\theta\\) (which may be a vector of parameters). Let \\(\\mathbf{Y} = (Y_1, ..., Y_n)\\). Then we can define several components of a likelihood model:\nThe likelihood:\n\\[\\mathcal L(\\theta \\mid \\mathbf{Y}) = f(y \\mid \\theta) = \\prod_{i=1}^n f(y_i \\mid \\theta)\\]\nLog-likelihood:\n\\[\\ell(\\theta \\mid y) = \\log \\mathcal L(\\theta \\mid y) = \\sum_{i=1}^n \\log f(y_i \\mid \\theta)\\]\nScore:\n\\[U(\\theta \\mid y) = \\frac{\\partial}{\\partial \\theta} \\ell (\\theta \\mid y) = \\left( \\frac{\\partial}{\\partial \\theta_1} \\ell(\\theta \\mid y), ..., \\frac{\\partial}{\\partial \\theta_1} \\ell(\\theta \\mid y) \\right)\\]"
  },
  {
    "objectID": "week10/week10.html#exponential-family",
    "href": "week10/week10.html#exponential-family",
    "title": "Week 10",
    "section": "Exponential Family",
    "text": "Exponential Family\nGLMs form a class of statistical models for response variables whose distribution belongs to the exponential dispersion family\nThese are the family of distributions with a pdf/pmf of the form:\n\\[f_Y(y; \\theta, \\phi) = \\exp \\left\\{ \\frac{y \\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi) \\right\\},\\]\nwhere \\(\\theta\\) is the canonical parameter, \\(\\phi\\) is the dispersion parameter, and \\(b(\\theta)\\) is the cumulant function.\nWe will see that \\(\\theta\\) is always a function of the conditional mean, \\(\\mu_i\\).\nNote that the \\(c(y, \\phi)\\) should not depend on \\(\\theta\\).\n\nBernoulli in the Exponential Notation\nLet \\(Y \\sim \\text{Bernoulli}(\\mu)\\).\nA common first step is to apply a convenient transformation that is equivalent to the identity function: \\(\\exp(\\log(\\cdot))\\).\n\\[\n\\begin{aligned}\nf_Y(y ; \\mu ) & = \\mu^y (1- \\mu)^{1-y} \\\\\n& = \\exp \\{ y \\log(\\mu) + (1-y) \\log (1-\\mu) \\} \\\\\n& = \\exp \\left\\{ y \\log \\left( \\frac{\\mu}{1-\\mu} \\right)  + \\log(1-\\mu) \\right\\}\n\\end{aligned}\n\\]\nLet\n\\[\n\\begin{aligned}\n\\theta = \\log\\left( \\frac{\\mu}{1-\\mu} \\right) \\quad \\quad & b(\\theta) = \\log (1 + \\exp \\{ \\theta \\}) \\\\\na(\\phi) = 1 \\quad \\quad & c(y, \\phi) = 0\n\\end{aligned}\n\\]\nThen \\[\n\\begin{aligned}\nf_Y(y; \\theta, \\phi) & = \\exp \\{ y \\theta - \\log (1 + \\exp \\{ \\theta \\} ) \\} \\\\\n& = \\exp \\left\\{ \\frac{y \\theta - b(\\theta) }{a(\\phi)} + c(y, \\phi) \\right\\}.\n\\end{aligned}\n\\]\nMany other common distributions are members of this faimly. The canonical parameter \\(\\theta\\) has key relationships with both \\(\\mathbb E(Y)\\) and \\(\\text{Var}(Y)\\). Typically varies across study units since \\(\\mathbb E(Y_i) = \\mu_i\\). We also index \\(\\theta\\) by \\(i\\), as in \\(\\theta_i\\).\nThe dispersion parameter \\(\\phi\\) has a key relationship with \\(\\text{Var}(Y)\\). It may, but does not typically, vary across study units. Typically it is not unit-specific, so we just write \\(\\phi\\). In some settings, we may have \\(a(\\cdot)\\) vary with \\(i\\), as in $\\(a_i(\\phi)\\). E.g., \\(a_i(\\phi) = \\phi / w_i\\) where \\(w_i\\) is the prior weight.\nWhen the dispersion parameter is known, some may say that this distribution is a member of the natural exponential family.\nConsider the likelihood function for a single observation\n\\[\\mathcal L(\\theta_i, \\phi ; y_i) = \\exp \\left\\{ \\frac{y_i \\theta - b(\\theta_i)}{a_i(\\phi)} + c(y_i, \\phi) \\right\\}.\\]\nThe log-likelihood is\n\\[\\ell(\\theta_i, \\phi; y_i) = \\frac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c(y_i, \\phi).\\]\nThe first partial derivative with respect to \\(\\theta_i\\) is the score function for \\(\\theta_i\\) and is given by\n\\[\\frac{\\partial}{\\partial \\theta_i} \\ell (\\theta_i, \\phi; y_i) = U(\\theta_i) = \\frac{y_i - b'(\\theta_i)}{a_i(\\phi)}.\\]\nNote that we consider \\(\\frac{\\partial}{\\partial \\theta_i} \\ell (\\theta_i, \\phi; y_i)\\) for the purpose of showing properties of exponential families (not for doing estimation).\nWe know that (under some regularity conditions)\n\\[\n\\begin{aligned}\n\\mathbb E[U(\\theta_i)] & = 0 \\\\\n\\text{Var}[U(\\theta_i)] & = \\mathbb E[U(\\theta_i)^2] = \\mathbb E\\left[ \\frac{\\partial U(\\theta_i)}{\\partial \\theta_i} \\right]\n\\end{aligned}\n\\]\nWe will use these properties to get expressions for \\(\\mathbb E(Y_i)\\) and \\(\\text{Var}(Y_i)\\) in terms of the exponential family parameters.\nSince the score has mean zero,\n\\[\\mathbb E(U(\\theta_i)) = \\mathbb E\\left[ \\frac{Y_i - b'(\\theta_i)}{a_i(\\phi)} \\right] = 0 \\]\nand, consequently,\n\\[\\mu_i = \\mathbb E[Y_i] = b'(\\theta_i).\\]"
  },
  {
    "objectID": "week10/week10.html#exponential-family-log-likelihood-and-score",
    "href": "week10/week10.html#exponential-family-log-likelihood-and-score",
    "title": "Week 10",
    "section": "Exponential Family Log-Likelihood and Score",
    "text": "Exponential Family Log-Likelihood and Score\nConsider the likelihood function for a single observation\n\\[\\mathcal L(\\theta_i, \\phi; y) = \\exp \\{ \\frac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c(y_i, \\phi) \\}\\]\n\\[\\frac{partial}{\\partial \\theta_i} \\ell(\\theta_i, \\phi; y_i) = U(\\theta_i) = \\frac{y_i - b'(\\theta_i)}{a_i(\\phi)}\\]\nFrom previously,\n\\[\\mathbb E[U(\\theta_i)] = 0\\]\n\\[\\text{Var}[U(\\theta_i)] = \\mathbb E[U(\\theta_i)^2] = -\\mathbb E\\left[ \\frac{\\partial U(\\theta_i)}{\\partial \\theta_i} \\right]\\]\nSince this score has mean zero,\n\\[\\mathbb E(U(\\theta_i)) = \\mathbb E(\\frac{Y_i - b'(\\theta_i)}{a_i(\\phi)}) = 0\\]\nand hence\n\\[\\mu_i = \\mathbb E[Y_i] = b'(\\theta_i).\\]"
  },
  {
    "objectID": "week10/week10.html#variance-in-exponential-families",
    "href": "week10/week10.html#variance-in-exponential-families",
    "title": "Week 10",
    "section": "Variance in Exponential Families",
    "text": "Variance in Exponential Families\nThe second partial derivative is\n\\[\\frac{\\partial^2}{\\partial \\theta_i^2} \\ell(\\theta_i, \\phi; y) = - \\frac{b''(\\theta_i)}{a_i(\\phi)}\\]\nUsing the above properties, it follows that\n\\[\\text{Var}(U(\\theta_i)) = \\text{Var}\\left[ \\frac{Y_i - b'(\\theta_i)}{a_i(\\phi)} \\right] = \\frac{b''(\\theta_i)}{a_i(\\phi)}\\]\nso that\n\\[\\text{Var}[Y_i] = b''(\\theta_i)a_i(\\phi)\\]\nThe variance of \\(Y_i\\) is therefore a function of both \\(\\theta_i\\) and \\(\\phi\\).\nNote that the canonical parameter is a function of \\(\\mu_i\\)\n\\[\\mu_i = b'(\\theta_i) \\Rightarrow \\theta_i = b'^{-1}(\\mu_i) = \\theta(\\mu_i)\\]\nso that we can write\n\\[\\text{Var}[Y_i] = b''(\\theta_i)a_i(\\phi) = b''(\\theta(\\mu_i))a_i(\\phi)\\]\nThe function \\(V(\\mu_i) = b''(\\theta(\\mu_i))\\) is called the variance function.\nThe specific form indicates the nature of the (if any) mean-variance relationship.\n\nBernoulli Example\nLet \\(Y \\sim \\text{Bernoulli}(\\mu)\\).\n\\[\\theta = \\log \\left( \\frac{\\mu}{1-\\mu} \\right), \\quad \\quad \\text{ the logit of } \\mu\\]\n\\[a(\\phi) = 1\\]\n\\[b(\\theta) = \\log(1 + \\exp \\theta)\\]\n\\[\\mathbb E[Y] = b'(\\theta) = \\frac{\\exp \\theta }{1 + \\exp \\theta} = \\mu, \\quad \\quad \\text{ the expit of } \\theta\\]\n\\[\\text{Var}[Y] = b''(\\theta)a(\\phi) = \\frac{\\exp \\theta}{(1 + \\exp \\theta)^2} = \\mu(1 - \\mu) = V(\\mu)\\]"
  },
  {
    "objectID": "week10/week10.html#estimation",
    "href": "week10/week10.html#estimation",
    "title": "Week 10",
    "section": "Estimation",
    "text": "Estimation\nThere are \\((p+2)\\) unknown parameters: \\((\\beta, \\phi)\\).\nTo obtain the MLE we need to solve the score equations:\n\\[U(\\beta, \\phi | y) = \\left( \\frac{\\partial \\ell}{\\parital \\beta_0}, ..., \\frac{\\partial \\ell}{\\partial \\beta_p}, \\frac{\\partial \\ell}{\\partial \\phi} \\right)' = 0\\]\nThis is a system of \\(p+2\\) equations. The contribution to the score for \\(\\phi\\) is the \\(i^{\\mathrm{th}}\\) unit is \\[\\frac{\\partial \\ell}{\\partial \\phi} = - \\frac{a_i'(\\phi)}{a_i(\\phi)^2}(y_i \\theta_i - b(\\theta_i)) + c'(y_i, \\phi)\\]\nWe can use the chain rule to obtain a convenient expression for the \\(i^{\\mathrm{th}}\\) contribution to the score function for \\(\\beta_j\\):\n\\[\\frac{\\partial \\ell}{\\partial \\beta_j} = \\frac{\\partial \\ell}{\\partial \\theta_i} \\times \\frac{\\partial \\theta_i}{\\partial \\mu_i} \\times \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\times \\frac{\\partial \\eta_i}{\\partial \\beta_j}\\]\nNote the following results:\n\\[\\frac{\\partial \\ell}{\\partial \\theta_i} = \\frac{y_i - b'(\\theta_i)}{a_i(\\phi)} = \\frac{y_i-\\mu_i}{a_i(\\phi)}\\]\n\\[\\frac{\\partial \\theta_i}{\\partial \\mu_i} = \\left( \\frac{\\partial \\mu_i}{\\partial \\theta_i} \\right)^{-1} = (b''(\\theta_i))^{-1} = (V(\\mu_i))^{-1}\\]\n\\[\\frac{\\partial \\eta_i}{\\partial \\beta_j} = x_{ij}\\]\nThe score function for \\(\\beta_j\\) can therefore be written as \\[\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{x_{ij}}{V(\\mu_i)a_i(\\phi)} (y_i - \\mu_i)\\]\nSuppose \\(a_i(\\phi) = \\phi / w_i\\). The score equations become\n\\[\\frac{\\partial \\ell}{\\partial \\phi} = \\sum_{i=1}^n - \\frac{w_i ( y_i \\theta_i - b(\\theta_i))}{\\phi^2} + c' (y_i, \\phi) = 0\\]\n\\[\\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n w_i \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{x_{ij}}{V(\\mu_i)} (y_i - \\mu_i) = 0\\]\nNotice that the \\((p+1)\\) score equations for \\(\\beta\\) do not depend on \\(\\phi\\).\nConsequently, obtaining the MLE of \\(\\beta\\) doesn’t require knowledge of \\(\\phi\\).\n\\(\\phi\\) isn’t required to be known or estimated (if unknown).\nFor exmaple, in linear regression, we don’t need \\(\\sigma^2\\) (or \\(\\hat \\sigma^2\\)) to obtain \\[\\hat \\beta_{MLE} = (X^T X)^{-1} X^T Y\\]\nInference does require an estimate of \\(\\phi\\)."
  },
  {
    "objectID": "week10/week10.html#asymptotic-sampling-distribution",
    "href": "week10/week10.html#asymptotic-sampling-distribution",
    "title": "Week 10",
    "section": "Asymptotic Sampling Distribution",
    "text": "Asymptotic Sampling Distribution\nSubject to appropriate regularity conditions,\n\\[\n\\begin{bmatrix}\n\\hat \\beta_{MLE} \\\\\n\\hat \\phi_{MLE}\n\\end{bmatrix} \\stackrel{\\cdot}{\\sim}\n\\text{MVN} \\left(\n  \\begin{bmatrix}\n  \\beta \\\\\n  \\phi\n  \\end{bmatrix},\n\\mathcal I_n(\\beta, \\phi)^{-1}\n\\right)\n\\]\nNow we can compute the components of \\(\\mathcal I_n(\\beta, \\phi):\\)\n\\[\\begin{aligned}\n\\frac{\\partial^2 \\ell(\\beta, \\phi; y_i)}{\\partial \\beta_j \\partial \\beta_k} & =\n\\frac{\\partial}{\\partial \\beta_k} \\overbrace{\\left\\{ \\frac{\\partial \\mu_i}{\\partial \\eta_i}\n\\frac{x_{ij}}{V(\\mu_i)a_i(\\phi)} (y_i - \\mu_i)\n\\right\\}}^{\\text{score for } \\beta_j} \\\\\n& = (y_i-\\mu_i) \\frac{\\partial}{\\partial \\beta_k} \\left\\{ \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{x_{ij}}{V(\\mu_i) a_i(\\phi)} \\right\\} - \\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right)^2 \\frac{x_{ij} x_{ik}}{V(\\mu_i)a_i(\\phi)}\n\\end{aligned}\\]\nAnd hence\n\\[-\\mathbb E\\left[ \\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k} \\right] = \\sum_{i=1}^n \\left(\n  \\frac{\\partial \\mu_i}{\\partial \\eta_i}\n  \\right)^2\n  \\frac{x_{ij}x_{ik}}{V(\\mu_i)a_i(\\phi)}\n\\]\n\\[ \\begin{aligned}\n\\frac{\\partial^2 \\ell(\\beta, \\phi; y_i)}{\\partial \\phi \\partial \\phi} & =\n\\frac{\\partial}{\\partial \\phi} \\overbrace{\\left\\{\n  - \\frac{a_i'(\\phi)}{a_i(\\phi)^2} (y_i \\theta_i - b(\\theta_i)) + c'(y_i, \\phi)\n\\right\\}}^{\\text{score for } \\phi}  \\\\\n& = - \\left\\{ \\frac{a_i(\\phi)^2 a_i'\\right\\}\n\\end{aligned} \\]"
  },
  {
    "objectID": "week10/week10.html#the-score",
    "href": "week10/week10.html#the-score",
    "title": "Week 10",
    "section": "The Score",
    "text": "The Score\nInformation Matrix\n\\[\\mathcal I(\\theta) = \\mathbb E( U(\\theta | y) \\otimes U(\\theta \\mid y)^T) \\]\nSince \\(\\mathbb E[U(\\theta \\mid y)]^2 = 0\\).\nThis is the variance of the score. Under regularity conditions, the \\((i,j)\\)th entry is equal to \\[- \\mathbb E( \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\ell ( \\theta \\mid y)).\\]\nAs we’ve seen previously, the MLE is the solution to \\(U(\\theta \\mid y) = 0\\).\nThe nice property of this estimator is that, for distributions with reasonably well-behaved likelihood functions, \\(\\hat \\theta_{MLE}\\) is consistent and asymptotically normal (CAN). Therefore\n\\[\\sqrt{n} (\\hat \\theta_{MLE} - \\theta) \\stackrel{\\mathcal D}{\\to} \\mathcal N(0, \\mathcal I(\\theta)^{-1}).\\]\nThis identity is from where “asymptotic tests” come from — no matter the true distribution of \\(\\mathbf Y\\), \\(\\hat \\theta_{MLE}\\) will be asymptotically normal (or \\(\\chi^2\\) if we square the estimator). There are three different ways, denoted in the figure below, to construct asymptotic tests of \\(H_0 : \\theta = \\theta_0\\) — each is obtained by using the above identity in a different way."
  },
  {
    "objectID": "week10/week10.html#wald-test",
    "href": "week10/week10.html#wald-test",
    "title": "Week 10",
    "section": "Wald Test",
    "text": "Wald Test\nDirectly evaluates whether \\(\\hat \\theta_{MLE}\\) is consistent with \\(H_0 : \\theta = \\theta_0\\).\n\\[W_n = (\\hat \\theta_{MLE} - \\hat \\theta_0)^T \\mathcal I(\\hat \\theta_{MLE}) (\\hat \\theta_{MLE} - \\theta_0) \\approx \\chi^2(p)\\]\nNote that if \\(\\theta\\) is one dimensional this simplifies to\n\\[W_n = (\\hat \\theta_{MLE} - \\theta_0) \\mathcal I(\\hat \\theta_{MLE})^{1/2} \\approx \\mathcal N(0,1)\\]\n\nAdvantages: Simple to compute, easy to construct confidence interval\nDisadvantages: requires MLE, approximation not as accurate in small samples"
  },
  {
    "objectID": "week10/week10.html#score-test",
    "href": "week10/week10.html#score-test",
    "title": "Week 10",
    "section": "Score Test",
    "text": "Score Test\nAsks “how consistent is the observed data with the null hypothesis?” and answers this by considering the gradient of the log-likelihood at the value specified by \\(H_0\\) (should be close to 0).\n\\[S_n = U(\\theta_0 \\mid y)^{T} \\mathcal I(\\theta_0)^{-1} U(\\theta_0 \\mid y) \\approx \\chi^2(p)\\]\nNote thta if \\(\\theta\\) is 1-dimensional, we can simplify this expression to\n\\[S_n = U(\\theta_0 \\mid y) \\mathcal I(\\theta_0)^{-1/2} \\approx \\mathcal N(0, 1)\\]\n\nAdvantages: Do not need to compute MLE, more computationally efficient.\nDisadvatage: Hard to construct confidence intervals since inverting this equation is difficult.\n\nNote to remember: For all of the above, if our null hypothesis is a function of multiple parameters, we can construct a \\(C\\) matrix and test \\(C \\theta\\) just as we did for the linear model in the first half of the course."
  },
  {
    "objectID": "week10/week10.html#likelihood-ratio-test",
    "href": "week10/week10.html#likelihood-ratio-test",
    "title": "Week 10",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\nConsiders the relative likelihood of the “best fitting model” under no restrictions to the model under \\(H_0\\).\n\\[Q_n = 2 \\left( \\ell(\\hat\\theta_{MLE} - \\ell(\\theta_0) \\right) \\approx \\chi^2(p)\\]\n\nAdvantages: No derivatives needed, most accurate approximation\nDisdvantage: Requires both MLE and knowledge of log-likelihood under \\(H_0\\).\n\n\nInference on a single proportion\nSuppose we have data on the number of independent successes in a population of \\(n\\) study units. This gives rise to the model\n\\[Y \\sim \\text{Binom}(n, \\pi)\\]\nSuppose our goal is to estimate \\(\\pi\\) and test the null hypothesis \\(H_0 : \\pi = \\pi_0\\). There are two ways to do this: we can either conduct an asymptotic \\(\\chi^2\\) test using one of the three previously discussed options, or we can perform an “exact” test by comparing observed values against the Binomial distribution directly."
  },
  {
    "objectID": "week10/week10.html#asymptotic-inference",
    "href": "week10/week10.html#asymptotic-inference",
    "title": "Week 10",
    "section": "Asymptotic Inference",
    "text": "Asymptotic Inference\nFor any test we use, we’ll need the components of the likelihood. Recall that the pmf of the Binomial distribution is\n\\[{n \\choose y} \\pi^y (1-\\pi)^{n-y}\\]\nStart by writing out the log-likelihood, score, and information for this model. Note that the information matrix will only be a single value since we only have one parameter \\(\\pi\\) and that we will only have one observation \\(Y\\).\n\\[\\begin{aligned}\n\\ell(\\pi \\mid y) & = \\log \\left( {n \\choose y} \\pi^y (1-\\pi)^{n-y} \\right) \\\\\n& = \\log({n \\choose y}) + y \\log \\pi + (n - y) \\log (1-\\pi) \\\\\n& \\propto y \\log( \\pi / (1-\\pi)) + n \\log (1-\\pi).\\end{aligned} \\]\n$$U(y) = y() ( \\frac{1-- (-)}{ )\n (y) = $$\n$$I() = -\nFind the MLE of \\(\\pi\\).\n\\[\\begin{aligned}\n\\frac{n \\pi(1-2\\pi)}{\\pi^2(1-\\pi)^2} + \\frac{n \\pi }{\\pi (1-\\pi)^2} & =\n\\frac{n - 2n \\pi + n \\pi}{\\pi (1-\\pi)^2} \\\\\n& = \\frac{n}{\\pi (1-\\pi)}\n\\end{aligned}\n\\]\n\\[U(\\pi) = 0 \\Rightarrow \\frac{y}{n(1-\\pi)} = \\frac{n}{1-\\pi} ...\\]\nDerive the Wald statistic using the definition.\nNow invert this test statistic to obtain a \\(1-\\alpha\\) percentile Wald-style confidence interval.\nWhat potential issue might arise when using a Wald test statistic?\nDerive the Score test statistic using the definition.\nProve that inverting this test statistic to obtain a \\(1-\\alpha\\) percentile Score-style confidence interval yields …"
  },
  {
    "objectID": "week9/week9.html",
    "href": "week9/week9.html",
    "title": "Week 9",
    "section": "",
    "text": "Recap\nSo far we’ve compared differences in probabilities on the absolute scale (i.e., via risk differences), and then we looked at relative measures (odds ratios, log odds ratios).\nNow we’ll look at contingency table methods.\nThe way we’ll be approaching this is to condition on the marginal totals, treating those as fixed and known.\nOne can show (Agresti, CDA Section 3.5) that\n\\[P(O_{11} = o_{11} | o_{1.}, o_{.1}, o_{..}, \\psi) =\n\\frac{\n  {o_{.1} \\choose o_{11}} {o_{..} - o_{.1} \\choose {o_{1.} - o_{11}} }\\psi^{o_{11}}\n  }{\n    \\sum_{\\ell=0}^{o_{.1}} { o_{.1} \\choose \\ell} { o_{..} - o_{.1} \\choose o_{1.} - \\ell } \\psi^{\\ell}\n  }\n\\]\nThis is a known distribution (non-central hypergeoemtric), but it depends on the unknown \\(\\psi\\), the odds-ratio.\nThe null hypothesis of no association between row and column variables for a \\(2\\times 2\\) contingency table holds if and only if \\(\\psi = 1\\).\nSo under the null hypothesis \\(H_0 : \\psi = 1\\) there are no unknown parameters and \\(O_{11} |o_{1.}, o_{.1}, o_{..}\\) follows a (central) hypergeometric distribution that we can use to calculate \\(p\\)-values.\nOnce we’ve conditioned on the marginals, knowing \\(O_{11}\\) tells us that all the other cell counts in a \\(2 \\times 2\\) table. So testing whether the observed \\(O_{11}\\) is extreme relative to its conditional distribution under the null is equivalent to testing whether any of the cell counts are extreme.\nWe could also construct exact tests for \\(H_0: \\psi = 2.5\\) for example, using the non-central hypergeometric distribution."
  },
  {
    "objectID": "week9/week9.html#pearson-chi-square-test-statistic",
    "href": "week9/week9.html#pearson-chi-square-test-statistic",
    "title": "Week 9",
    "section": "Pearson Chi-Square Test Statistic",
    "text": "Pearson Chi-Square Test Statistic\n\n\n\n\nPht\nCbz\nPb\n\n\n\n\nYes\n\\(O_{11} = 9\\)\n\\(O_{12} = 0\\)\n\\(O_{13} = 5\\)\n\n\nNo\n\\(O_{21} = 65\\)\n\\(O_{22} = 46\\)\n\\(O_{23} = 47\\)\n\n\n\nWhere the rows correspond to having digit hypoplasia and the columns are different drugs used in a study.\nWe will denote \\(O_{.1} = 74\\) to be the total in the first row, etc., and \\(\\hat \\pi_{i.} = O_{i.}/O{..}\\), \\(\\hat \\pi_{.j} = O_{.j}/O_{..}\\).\n$H_0 : $ There is no association between row (outcome) and column (exposure) variables.\n$H_A : $ There is an association between row and column.\n\nSometimes \\(H_0\\) is also written as “There is independence between the row and column variables.”\n\nThe Pearson Chi-square test statistic is\n\\[\\chi_P^2  \\sum_{i,j} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}} \\stackrel{H_0}\\sim \\chi_{(R-1) \\times (C-1)}^2\n\\]\nwhere \\(E_{ij}\\) is the expected count in cell \\(i,j\\) under the null, i.e.,\n\\[E_{ij} = O_{..} \\times \\hat \\pi_{i.} \\times \\hat \\pi_{.j}.\n\\]\nBecause under the null of no association we should have that \\(\\pi_{ij} = \\pi_{i.} \\times \\pi_{.j}.\\)"
  },
  {
    "objectID": "week9/week9.html#pearson-chi-square-test-as-a-score-test-2-times-2-tables",
    "href": "week9/week9.html#pearson-chi-square-test-as-a-score-test-2-times-2-tables",
    "title": "Week 9",
    "section": "Pearson Chi-Square Test as a Score Test: \\(2 \\times 2\\) Tables",
    "text": "Pearson Chi-Square Test as a Score Test: \\(2 \\times 2\\) Tables\n\n\n\n\nMajor Malformation: Yes\nNo\n\n\n\n\n\nAED Exposed: Yes\n\\(y_1 = 18\\)\n\\(n_1 - y_1 = 298\\)\n\\(n_1 = 316\\)\n\n\nNo\n\\(y_0 = 9\\)\n\\(n_0 - y_0 = 597\\)\n\\(n_0 = 606\\)\n\n\n\n\\(y = 27\\)\n\\(n- y = 895\\)\n\\(n = 922\\)\n\n\n\nCompare the following:\nchisq.test(xtabs(~aed+major, data=newborn.dat),correct=F)\n\n    Pearson’s Chi-squared test\n\ndata: xtabs(~aed + major, data = newborn.dat)\nX-squared = 12.9564, df = 1, p-value = 0.0003188\n\n\nprop.test(y,n,alternative=\"two.sided\",correct=F)\n\n    2-sample test for equality of proportions\n    without continuity correction\n\ndata: y out of n\nX-squared = 12.9564, df = 1, p-value = 0.0003188\nalternative hypothesis: two.sided\n\n95 percent confidence interval:\n  -0.06941918 -0.01480190\n\nsample estimates:\n    prop 1     prop 2\n0.01485149 0.05696203"
  },
  {
    "objectID": "week9/week9.html#validity-of-pearson-chi-square-in-small-samples",
    "href": "week9/week9.html#validity-of-pearson-chi-square-in-small-samples",
    "title": "Week 9",
    "section": "Validity of Pearson Chi-Square in Small Samples",
    "text": "Validity of Pearson Chi-Square in Small Samples\nThe Pearson chi-square test statistic is only asymptotically chi-square distributed. Inferences constructed using Pearson’s chi-square test statistic are usually considered valid when all of the \\(R \\times C\\) table have expected values greater than or equal to five (Fisher and van Belle, 1993, page 250).\nThere are other rules of thumb on this:\n\nThe test might still be valid if all of the expected values, except one, are five or greater (Fisher and van Belle, 1993, page 250).\nFor Pearson chi-square test with more than one degree of freedom, many statisticians use the rule that a minimum expected value of one is acceptable as long as no more than 20% of cells have expected values less than 5 (Agresti, 2013, page 77).\n\nchisq.test(xtabs(~nails + brand, data=monodrug.dat),correct=F)\n\n    Pearson’s Chi-squared test\n\ndata: xtabs(~nails + brand, data = monodrug.dat)\nX-squared = 5.8289, df = 2, p-value = 0.05423\n\nWarning message:\nChi-squared approximation may be incorrect in:\nchisq.test(xtabs(~nails + brand, data = monodrug.dat),\ncorrect = F)\nR prints this warning if any of the cells has an observed value less than 5.\nAn alternative is ot use an exact test that avoids the asymptotic approximation to the distribution of the Pearson statistic."
  },
  {
    "objectID": "week9/week9.html#test-statistics-for-h_0-psi-1.",
    "href": "week9/week9.html#test-statistics-for-h_0-psi-1.",
    "title": "Week 9",
    "section": "Test Statistics for \\(H_0 : \\psi = 1\\).",
    "text": "Test Statistics for \\(H_0 : \\psi = 1\\).\nUnder the null, \\(H_0 : \\psi = 1\\), the target cell count \\(O_{11}\\) follows the central hypergeometric distribution.\n\\[\n\\begin{aligned}\nPr_C(O_{11} = o_{11} | \\psi = 1) & =\n\\frac{\\displaystyle\n  {o_{.1} \\choose o_{11}} {o_{..} - o_{.1} \\choose {o_{1.} - o_{11}}}\n  }{\\displaystyle\n    \\sum_{\\ell=0}^{o_{.1}} { o_{.1} \\choose \\ell} { o_{..} - o_{.1} \\choose o_{1.} - \\ell }\n  } \\\\\n& = \\frac{\\displaystyle\n  {o_{.1} \\choose o_{11}} {o_{.2} \\choose o_{12}}\n  }{\\displaystyle\n    { o_{..} \\choose o_{1.}}\n  },\n\\end{aligned}\n  \\]\nsince \\(\\displaystyle \\sum_{\\ell = 0}^{o_{.1}} { o_{.1} \\choose \\ell} { o_{..} - o_{.1} \\choose o_{1.} - \\ell } = { o_{..} \\choose o_{1.}}\\)."
  },
  {
    "objectID": "week9/week9.html#one-sided-p-value",
    "href": "week9/week9.html#one-sided-p-value",
    "title": "Week 9",
    "section": "One-sided \\(p\\)-value",
    "text": "One-sided \\(p\\)-value\nSuppose we want to test\n\\[H_0 : \\psi = 1, \\quad \\quad H_A : \\psi > 1. \\]\nThe exact (right-tail) \\(p\\)-value is\n\\[\\begin{aligned}\np & = Pr(O_{11} \\geq o_{11} \\mid H_0 \\colon \\psi = 1)\\\\\n& = \\sum_{\\ell = o_{11}^z } \\frac{\\displaystyle\n  {o_{.1} \\choose \\ell } {o_{.2} \\choose o_{1.} - \\ell }\n  }{\\displaystyle\n    { o_{..} \\choose o_{1.}}\n  }\n\\end{aligned}\n\\]\nwhere \\(z = \\min (o_{.1, o_{1.}})\\).\nSuppose we want to test\n\\(H_0 : \\psi = 1, \\quad \\quad H_A : \\psi < 1\\)$\nThe exact (left-tail) \\(p\\)-value is\n\\[\\begin{aligned}\np & = Pr(O_{11} \\leq o_{11} \\mid H_0 : \\psi = 1) \\\\\n& = \\sum_{\\ell = z}^{o_{11}} \\frac{\\displaystyle\n  {o_{.1} \\choose \\ell } {o_{.2} \\choose o_{1.} - \\ell }\n  }{\\displaystyle\n    { o_{..} \\choose o_{1.}}\n  }\n\\end{aligned}\n\\]\nwhere \\(z = \\max(0, o_{1.} + o_{.1} - o_{..})\\)."
  },
  {
    "objectID": "week9/week9.html#two-sided-p-value",
    "href": "week9/week9.html#two-sided-p-value",
    "title": "Week 9",
    "section": "Two-sided \\(p\\)-value",
    "text": "Two-sided \\(p\\)-value\nSuppose we want to test \\[H_0 : \\psi = 1$, \\quad \\quad H_A : \\psi \\neq 1\\]\nThe possible values for \\(O_{11}\\) are\n\\[\\max (0, o_{1.} + o_{.1} - o_{..}) \\leq o_{11} \\leq \\min(o_{1.}, o_{.1}),\\]\n\nCalculate the probabilities of all of these values,\n\n\\[P_{\\ell} = Pr(O_{11} = \\ell \\mid H_0 : \\psi = \\ell) \\]\n\nSum the probabilities \\(P_{\\ell}\\) that are less than or equal to the observed probability \\(P\\) in the first equation above.\n\n\\[\np = \\sum_{\\ell = z_1}^{z_2} P_\\ell I(P_\\ell \\leq P) \\]\nwhere \\(z_1 = \\max(0, o_{1.} + o_{.1} - o_{..})\\) and \\(z_2 = \\min(o_{1.}, o_{.1})\\).\nIn general if we have a \\(2\\times 2\\) table, we can compute the odds ratio as follows:\n\nGeneric \\(2 \\times 2\\) table\n\n\n\n\n\n\n\n\n1\n0\n\n\n\n\n1\na\nb\n\n\n0\nc\nd\n\n\n\nThen the \\(\\mathrm{OR} = \\frac{ad}{bc}\\).\n# Function: SuppDists::dghyper(x, n, r, N, log=FALSE)\n# arguments: x is o_11 cell, n is o_{.1},\n# r is o_{1.}, N is total\n\nlibrary(SuppDists)\n\nhyper.probs <- dghyper(seq(0,12,1),12,58,566)\n\ncbind(seq(0,12,1), round(hyper.probs,4))\n\n    [,1]  [,2]\n[1,]   0 0.2696\n[2,]   1 0.3775\n[3,]   2 0.2377\n[4,]   3 0.0889\n[5,]   4 0.0220\n[6,]   5 0.0038\n[7,]   6 0.0005\n[8,]   7 0.0000\n[9,]   8 0.0000\n[10,]  9 0.0000\n[11,] 10 0.0000\n[12,] 11 0.0000\n[13,] 12 0.0000\n\nfisher.pvalue <- sum(hyper.probs[4:13])\nfisher.pvalue\n[1] 0.1152\nThe function fisher.test() runs a Fisher’s exact test directly.\nxtabs(~MONOCBZ + major, data=monocbz.dat)\n        major\nMONOCBZ   0 1\n      0 499 9\n      1 55  3\n\nfisher.test(xtabs(~MONOCBZ + major, data=monocbz.dat))\n      Fisher’s Exact Test for Count Data\n\ndata: xtabs(~MONOCBZ + major, data = monocbz.dat)\np-value = 0.1152\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.5101941 12.5603706\nsample estimates:\nodds ratio\n  3.015342\nOne can compute an exact CI for the OR by inverting Fisher’s exact test."
  },
  {
    "objectID": "week11/week11.html",
    "href": "week11/week11.html",
    "title": "Week 11",
    "section": "",
    "text": "Asymptotic Sampling Distribution\nSubject to appropriate regularity conditions,\n\\[\\begin{bmatrix} \\hat \\beta_{MLE} \\\\ \\hat \\phi_{MLE} \\end{bmatrix} \\sim \\text{MVN} \\left(\n    \\begin{bmatrix} \\beta \\\\ \\phi \\end{bmatrix}, \\mathcal I_n (\\beta, \\phi)^{-1}\n    \\right).\\]\nNow we compute the components of \\(\\mathcal I_n(\\beta, \\phi):\\)\n\\[\\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k} = \\frac{\\partial}{\\partial \\beta_k} \\underbrace{\\left\\{ \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{x_{ij}}{V(\\mu_i) a_i(\\phi)} (y_i - \\mu_i) \\right\\}}_{\\text{score for } \\beta_j}.\\]\n\\[ = (y_i - \\mu_i) \\frac{\\partial}{\\partial \\beta_k} \\left\\{ \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{x_{ij}}{V(\\mu_i) a_i(\\phi)} \\right\\} - \\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right)^2 \\frac{x_{ij} x_{ik}}{V(\\mu_i) a_i(\\phi)}.\\]\nHence \\[-\\mathbb E\\left[\\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k} \\right] =\n\\sum_{i=1}^n \\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right)^2 \\frac{x_{ij}x_{ik}}{V(\\mu_i) a_i(\\phi)}.\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\phi} & =\n\\frac{\\partial}{\\partial \\phi} \\left\\{\n    \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{x_{ij}}{V(\\mu_i)a_i(\\phi)} (y_i - \\mu_i)\n\\right\\} \\\\\n& = - \\frac{a_i'(\\phi) \\partial \\mu_i x_{ij}}{a_i(\\phi)^2 \\partial \\eta_i V(\\mu_i)} (y_i - \\mu_i)\n\\end{aligned}\n\\]\nThus\n\\[-E \\left[ \\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\phi} \\right] = 0\\]\n\\[\\begin{aligned}\\frac{\\partial^2 \\ell}{\\partial \\phi \\partial \\phi} & = \\frac{\\partial}{\\partial \\phi}\n\\left\\{\n    - \\frac{a_i'(\\phi)}{a_i(\\phi)^2} (y_i\\theta_i - b(\\theta_i)) + c'(y_i, \\phi)\n\\right\\} \\\\\n& = - \\left\\{ \\frac{a_i(\\phi)^2 a_i''(\\phi) - 2a_i(\\phi) a_i'(\\phi)^2}{a_i(\\phi)^4}\\right\\} [y_i \\theta_i - b(\\theta_i)] + c''(y_i, \\phi) \\\\\n& = -K(\\phi)[y_i\\theta_i - b(\\theta_i)] + c''(y_i, \\phi)\n\\end{aligned}\\]\nand thus \\[-\\mathbb E\\left[ \\frac{\\partial^2 \\ell}{\\partial \\phi \\partial \\phi} \\right] =\n\\sum_{i=1}^n K(\\phi)[b'(\\theta_i)\\theta_i - b(\\theta_i)] - \\mathbb E[c''(Y_i, \\phi)].\\]\nFor the linear predictor \\(x_i' \\beta\\), suppose we partition \\(\\beta = (\\beta_1, \\beta_2)\\) and we are interested in testing:\n\\[H_0 : \\beta_1 = \\beta_{1,0} \\quad \\text{vs.} \\quad H_a : \\beta_1 \\neq \\beta_{1,0}\\]\nThe length of \\(\\beta_1\\) is \\(q \\leq (p+1)\\) and \\(\\beta_2\\) is left arbitrary.\nIn most settings \\(\\beta_{1,0} = 0\\) which represents some form of ‘no effect’\nFollowing our review of asymptotic theory, there are three common hypothesis testing frameworks: Wald, score, and likelihood ratio testing.\nWe saw that the score equation for \\(\\beta_j\\) is\n\\[\\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{x_{ij}}{V(\\mu_i) a_i(\\phi)} (y_i - \\mu_i) = 0\\]\nEstimation of \\(\\beta\\) requires solving \\((p+1)\\) of these equations simultaneously. This is tricky because \\(\\beta\\) appears in several places (implicitly anywhere a \\(\\mu\\) appears).\nNo closed form solution in general is available (though there is one for linear settings), so we need an iterative method. A commonly used general purpose optimization routine is the Newton-Raphson algorithm.\nGeneral idea of Newton-Raphson and Fisher Scoring\nGeneral idea of Newton-Raphson\n\\[\\beta^{(r+1)} = \\beta^{(r)} - \\left[ \\mathcal{I}_{\\beta \\beta}^{(r)}\\right]^{-1} U^{(r)}_{\\beta}\\]\nFisher scoring is an adaptation of the Newton-Raphson algorithm that uses the expected information, \\(\\mathcal I_{\\beta \\beta}\\), rather than observed information \\(\\mathcal I_{\\beta \\beta}\\) for the update.\nThere’s a close relationship between Fisher scoring and IRLS. Often this is exploited for computation in software packages, many of which use IRLS by default (including R’s glm()).\nThe idea:\nDefine\n\\[Z_i = \\overbrace{g(\\mu_i) + (Y_i - \\mu_i)g'(\\mu_i)}^{\\text{First order Taylor approx of }g(Y_i)}\\]\nto be an “adjusted response variable” to show that\n\\[\\text{Var}(Z_i) = (W_i)^{-1}, \\text{ where }W_i = \\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right)^2 \\frac{1}{V(\\mu_i)a_i(\\phi)}\\]\nThen note that\n\\[\\mathbb E[Z_i] = x_i'\\beta\\]\nso we can consider a linear model for \\(Z\\) to estimate \\(\\beta\\), i.e., using the GLS estimator \\(\\hat \\beta_{GLS} = (X'WX)^{-1} X'WZ\\).\nTwo immediate challenges to estimating \\(\\hat \\beta_{GLS}\\) in practice are\nHowever, we can use IRLS to do estimation.\nSuppose the current estimate of \\(\\beta\\) is \\(\\hat \\beta^{(r)}\\). Compute\n\\[\\eta_i^{(r)} = x_i'\\hat \\beta^{(r)}\\] \\[\\mu_i^{(r)} = g^{-1} (\\eta_i^{(r)})\\] \\[W_i^{(r)} = \\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\biggr \\mid_{\\eta_i^{(r)}} \\right)^2 \\frac{1}{V(\\mu_i^{(r)})}\\] \\[z_i^{(r)} = \\eta_i^{(r)} + (y_i - \\mu_i^{(r)}) \\frac{\\partial \\eta_i}{\\partial \\mu_i} \\mid_{\\mu_i^{(r)}}\\]\n\\(W_i\\) is called the ‘working weight’\nCheck the McCullagh and Nelder (1989) book which shows that using IRLS for estimation here is equivalent to Fisher scoring. For more detail, see Agresti 4.6.3 or https://grodri.github.io/glms/notes/\nThe updated version of \\(\\hat \\beta\\) is obtained as the WLS estimate to the regression of \\(Z\\) on \\(X\\):\n\\[\\hat \\beta^{(r+1)} = (X' W^{(r)} X)^{-1} (X' W^{(r)} Z^{(r)})\\]\n\\(X\\) is the \\(n \\times (p+1)\\) design matrix from the initial specification of the model. \\(W^{(r)}\\) is the diagonal \\(n \\times n\\) matrix with entries \\(\\{ W_1^{(r)}, ..., W_n^{(r)}}\\). \\(Z^{(r)}\\) is the \\(n\\)-vector \\((z_1^{(r)}, ..., z_n^{(r)})\\).\nIterate until the \\(\\hat \\beta\\) values converges.\nA generic call to glm() is given by\nfit0 <- glm(formula, family, data, ...)\nformula specifies the structure of the linear predictor \\(\\eta_i = x_i' \\beta\\).\nfamily jointly specifies the probability distribution \\(f_Y()\\), link function \\(g()\\) and variance function \\(V()\\).\nE.g.,\nglm(Y ~ X_1 + X2, family = binomial(), data = df)\nMost common family objects are binomial() or poisson().\nThe residuals inside a glm() are called working residuals and used internally for iteratively reweighted least squares.\nIn linear models, we used residual diagnostics to examine the adequacy of model fit and investigate potential data issues such as outliers. Adequacy of model fit included functional form for terms in the linear predictor and the homoscedasticity assumption.\nIn GLMs, residual diagnostics are much more complex and visual inspection of residual plots is typically less informative.\nIn general, residuals are meant to represent variation in the outcome that is not explained by the model.\nVariation after the systematic component is accounted for, and therefore residuals are therefore model-specific.\nPearson residuals account for the heteroscedasticity via standardization.\n\\[r_i^p = \\frac{y_i - \\hat \\mu_i}{\\sqrt{\\text{Var}(Y_i)}}\\]\nWe’ll see that Pearson residuals can be useful for diagnosing departures from our model’s variance assumption.\nThe deviance residual (sometimes called the “preferred residual”) is defined as\n\\[r_i^d = \\text{sign}(y_i - \\hat \\mu_i)\\sqrt{d_i}\\]\nwhere, letting \\(\\ell(\\hat \\mu_i, \\phi; y_i)\\) be the log-likelihood contribution of unit \\(i\\) evaluated as \\(\\hat \\mu_i\\) and \\(\\phi\\), then \\[d_i = 2 \\phi(\\ell(y_i, \\phi; y_i) - \\ell(\\hat \\mu_i, \\phi; y_i))\\]\nWe can think about this as the distance between \\(\\hat \\mu_i\\) and \\(y_i\\) on the log-likelihood scale.\nPierce and Shafer (JASA, 1986) examined various residuals for GLMs. https://www.jstor.org/stable/2289071\nOne can get both deviance and Pearson residuals from glm() in R:\nA prospective study of coronary heart disease (CHD) with \\(n = 3,154\\) male participants aged 39-54 at risk for CHD, employed at 10 companies in California with a baseline survey and measurements taken at intake (1960-61) along with annual surveys until December 1969 to assess incident CHD.\nOur primary goal is to investigate the relationship between ‘behavior pattern’ and risk of CHD.\nParticipants were categorized into one of two behavior pattern groups.\nType A: characterized by enhanced aggressiveness, ambitiousness, competitive drive, and chronic sense of urgency.\nType B: characterized by more relaxed and non-competitive.\nFor now, we will ignore any loss to follow-up and consider the binary outcome of\n\\[Y = \\left\\{ \\begin{array}\n1 \\quad & \\text{occurrence of CHD during follow-up} \\\\\n0 & \\text{otherwise}\n\\end{array} \\right. \\]\n8.1% of participants developed CHD during follow-up, while 50.4 of participants were classified as Type A at baseline.\n\\[\\hat P(\\text{CHD} = 1 \\mid \\text{Type} = A) = 0.112\\] \\[\\hat P(\\text{CHD} = 1 \\mid \\text{Type} = B) = 0.050\\]\nI.e., 11.2% of Type A men and 5% of Type B men developed CHD during follow-up.\nOften we will use the generic term ‘risk,’ e.g., “risk of CHD” rather than probability of CHD during follow-up."
  },
  {
    "objectID": "week11/week11.html#form-of-the-fisher-matrix",
    "href": "week11/week11.html#form-of-the-fisher-matrix",
    "title": "Week 11",
    "section": "Form of the Fisher Matrix",
    "text": "Form of the Fisher Matrix\nWe can write the expected information matrix in block-diagonal form:\n\\[\\mathcal I_n(\\beta, \\phi) = \\begin{bmatrix}\n    \\mathcal I_{\\beta \\beta} & 0 \\\\\n    0 & \\mathcal I_{\\phi \\phi}\n\\end{bmatrix}.\\]\nThe inverse of the information matrix is the asymptotic variance\n\\[\\text{Var}[\\hat \\beta_{MLE}, \\hat \\phi_{MLE}] = \\mathcal I_n(\\beta, \\phi)^{-1} =\n\\begin{bmatrix} \\mathcal I^{-1}_{\\beta \\beta} & 0 \\\\ 0 & \\mathcal I_{\\phi \\phi}^{-1} \\end{bmatrix}.\\]\nThe block diagonal structure of \\(\\text{Var}[\\hat \\beta_{MLE}, \\hat \\phi_{MLE}]\\) implies that for GLMs, valid characterization of the uncertainty in \\(\\hat \\beta_{MLE}\\) does not require the propagation of uncertainty in the estimate of \\(\\phi\\).\nFor example, for linear regression on Normally distributed outcomes, we plug in an estimate of \\(\\sigma^2\\) into \\[\\text{Var}[\\hat \\beta_{MLE}] = \\sigma^2 (X^T X)^{-1}\\] without worrying about uncertainty in estimation of \\(\\sigma^2\\).\nFor GLMs, therefore, estimation of \\(\\text{Var}(\\hat \\beta_{MLE})\\) proceeds by plugging in the values of \\((\\hat \\beta_{MLE}, \\hat \\phi)\\) into \\(\\mathcal I_{\\beta \\beta}^{-1}\\), i.e., \\[\\widehat{\\text{Var}}[\\hat \\beta_{MLE}] = \\hat{\\mathcal{I}}_{\\beta \\beta}^{-1}\\] where \\(\\hat \\phi\\) is any consistent estimator of \\(\\phi\\).\n\nMatrix notation\nRecall that the \\((j,k)^{\\text{th}}\\) element of \\(\\mathcal I_{\\beta \\beta}\\) has the form\n\\[(\\mathcal I_{\\beta \\beta})_{jk} = \\sum_{i=1}^n \\underbrace{\\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right)^2 \\frac{1}{V(\\mu_i) a_i(\\phi)}}_{\\coloneqq W_i} x_{ij}x_{jk},\\]\nso we can write\n\\[(\\mathcal I_{\\beta \\beta})_{jk} = \\sum_{i=1}^n W_i x_{ij}x_{jk}.\\]\n\\(W_i\\) sort of looks like the inverse of the variance of \\(Y_i\\).\nWe can therefore write:\n\\[\\mathcal I_{\\beta \\beta} = X'WX \\quad \\text{ and } \\quad \\text{Var}(\\hat \\beta_{MLE}) = \\mathcal I_{\\beta \\beta}^{-1} = (X'WX)^{-1},\\]\nwhere \\(W = \\text{diag}(W_1, ..., W_n)\\) and \\(X\\) is the design matrix.\nNote that \\((X'WX)^{-1}\\) looks a lot like \\(\\text{Var}(\\hat \\beta_{GLS})\\), just instead of \\(W\\) we used \\(\\Sigma^{-1}\\)."
  },
  {
    "objectID": "week11/week11.html#wald-test",
    "href": "week11/week11.html#wald-test",
    "title": "Week 11",
    "section": "Wald Test",
    "text": "Wald Test\nLet \\(\\hat \\beta_{MLE} = (\\hat \\beta_{1,MLE}, \\hat \\beta_{MLE})\\).\nUnder \\(H_0\\)\n\\[(\\hat \\beta_{1,MLE} - \\beta_{1,0})' \\widehat{\\text{Var}}[\\hat \\beta_{1,MLE}]^{-1}(\\hat\\beta_{1,MLE} - \\beta_{1,0}) \\longrightarrow_d \\chi^2_q\\]\nwhere \\(\\widehat{\\text{Var}}[\\hat \\beta_{1,MLE}]\\) is the \\(q\\times q\\) sub-matrix of \\(\\mathcal{I}_{\\beta\\beta}^{-1}\\) corresponding to \\(\\beta_1\\), evaluated at \\(\\left( \\hat \\beta_{MLE}, \\hat \\phi_{MLE} \\right).\\)\nThis is the multivariate analog of \\(((\\text{Est} - \\text{Null})/\\text{SE})^2\\).\nThe Wald test is just looking at how far our estimate of \\(\\beta\\) is from the null and seeing if that distance is large relative to the uncertainty in our estimate."
  },
  {
    "objectID": "week11/week11.html#score-test",
    "href": "week11/week11.html#score-test",
    "title": "Week 11",
    "section": "Score Test",
    "text": "Score Test\nLet \\(\\hat \\beta_{0,MLE} = (\\beta_{1,0}, \\hat \\beta_{2,MLE})\\) and \\(\\hat \\phi_{0,MLE}\\) denote the MLEs under \\(H_0\\).\nUnder \\(H_0\\),\n\\[U(\\hat \\beta_{0,MLE}, \\hat \\phi_{0,MLE}; y)' \\mathcal I_n(\\hat \\beta_{0,MLE}; \\hat \\phi_{0, MLE})^{-1} U(\\hat \\beta_{0,MLE}, \\hat \\phi_{0,MLE}; y) \\longrightarrow_d \\chi^2_q.\\]"
  },
  {
    "objectID": "week11/week11.html#likelihood-ratio-test",
    "href": "week11/week11.html#likelihood-ratio-test",
    "title": "Week 11",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\nObtain unrestricted MLEs: \\((\\hat \\beta_{MLE}, \\hat \\phi_{MLE})\\), and obtain MLEs under \\(H_0:\\) \\((\\hat \\beta_{0, MLE}, \\hat \\phi_{0,MLE})\\).\nUnder \\(H_0\\),\n\\[2(\\ell(\\hat \\beta_{MLE}, \\hat \\phi_{MLE}; y) - \\ell(\\hat \\beta_{0,MLE}, \\hat \\phi_{0, MLE} ; y)) \\longrightarrow_d \\chi^2_q.\\]"
  },
  {
    "objectID": "week11/week11.html#types-of-contrasts",
    "href": "week11/week11.html#types-of-contrasts",
    "title": "Week 11",
    "section": "Types of Contrasts",
    "text": "Types of Contrasts\nWe need to pick a contrast —\nRisk difference:\n\\[RD = P(Y = 1 \\mid x = 1) - P(Y = 1 \\mid x = 0)\\] \\[\\widehat{RD} = \\hat P(Y = 1 \\mid x = 1) - \\hat P(Y=1 \\mid x = 0) = .112 - .050 = .062\\]\nThe difference in the estimated risk of CHD during follow-up between type A and type B men is 0.062 (or 6.2%). This characterizes the way in which the additional risk of CHD of being a type A person manifests through an absolute increase.\nWe’ve already seen the OR as a relative-scale contrast. A more interpretable option is the relative risk:\n\\[RR = \\frac{P(Y = 1 \\mid x = 1)}{P(Y = 1 \\mid x = 0)}\\]\n\\[\\widehat{RR} = \\frac{\\hat P(Y = 1 \\mid x = 1)}{\\hat P(Y = 1 \\mid x = 0)} = \\frac{0.112}{0.050} = 2.24 \\]\n\nLog-likelihood for GLMs with binary outcomes\n\\[\\ell(\\beta; y) = \\sum_{i=1}^n y_i \\theta_i - b(\\theta_i)\\]\n\\[ = \\sum_{i=1}^n y_i \\theta_i - \\log(1 + \\exp\\{\\theta_i\\})\\]\nWhere \\(\\theta_i\\) is a function of \\(\\beta\\) via\n\\[\\mu_i = b'(\\theta_i) = \\frac{\\exp \\theta_i}{1 + \\exp \\theta_i}\\]\nand\n\\[\\mu_i = g^{-1}(x_i'\\beta)\\]"
  },
  {
    "objectID": "week11/week11.html#score-and-information-for-glms-with-binary-outcomes",
    "href": "week11/week11.html#score-and-information-for-glms-with-binary-outcomes",
    "title": "Week 11",
    "section": "Score and information for GLMs with Binary outcomes",
    "text": "Score and information for GLMs with Binary outcomes\nThe score function for \\(\\beta_j\\) is\n\\[\\frac{\\partial \\ell }{\\partial \\beta_j} = \\sum_{i=1}^n \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{x_{ij}}{\\mu_i (1-\\mu_i)} (y_i - \\mu_i)\\]\nwhere the expression for \\(\\partial \\mu_i / \\partial \\eta_i\\) depends on the choice of \\(g()\\).\nSince \\(\\phi\\) is fixed and known, the expected information matrix is just \\[\\mathcal I_{\\beta \\beta} = X' W X\\]\nwhere \\(X\\) is the design matrix for the model and \\(W\\) is a diagonal matrix with \\(i^{\\text{th}}\\) diagonal element\n\\[W_i = \\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right)^2 \\frac{1}{\\mu_i (1-\\mu_i)},\\]\nwhich we’ve said previously looks like an inverse variance of \\(Y\\)."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. Introduction to Statistical Learning [Internet]. [cited 2023 Nov 8]. Available from: https://trevorhastie.github.io/ISLR/\nAllen Akinkunle. The Bias-Variance Decomposition Demystified [Internet]. [cited 2023 Nov 8]. Available from: https://allenkunle.me/bias-variance-decomposition\nEfron B, Tibshirani RJ. An Introduction to the Bootstrap. CRC Press; 1994. 453 p. \nJulian J Faraway. Routledge & CRC Press. [cited 2023 Nov 8]. Linear Models with R. Available from: https://www.routledge.com/Linear-Models-with-R/Faraway/p/book/9781439887332\nTrevor Hastie, Junyang Qian, Kenneth Tay. An Introduction to glmnet [Internet]. [cited 2023 Nov 8]. Available from: https://glmnet.stanford.edu/articles/glmnet.html\nFrank Harrell. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis [Internet]. Cham: Springer International Publishing; 2015 [cited 2023 Nov 8]. (Springer Series in Statistics). Available from: https://link.springer.com/10.1007/978-3-319-19425-7\nEric Vittinghoff, David V. Glidden, Stephen C. Shiboski, Charles E. McCulloch. Regression Methods in Biostatistics. [cited 2023 Nov 8]. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. Available from: https://regression.ucsf.edu/regression-methods-biostatistics-linear-logistic-survival-and-repeated-measures-models\nAgresti A, Coull BA. Approximate Is Better than “Exact” for Interval Estimation of Binomial Proportions. The American Statistician. 1998;52(2):119–26. https://www.jstor.org/stable/2685469\nErnst MD. Permutation Methods: A Basis for Exact Inference. Statistical Science. 2004 Nov;19(4):676–85. https://www.jstor.org/stable/4144438\nAgresti A. Categorical Data Analysis. John Wiley & Sons; 2012. 756 p. \nShmueli G. To Explain or to Predict? Statist Sci [Internet]. 2010 Aug 1 [cited 2023 Aug 28];25(3). Available from: https://projecteuclid.org/journals/statistical-science/volume-25/issue-3/To-Explain-or-to-Predict/10.1214/10-STS330.full\nPierce DA, Schafer DW. Residuals in Generalized Linear Models. Journal of the American Statistical Association. 1986;81(396):977–86.\nKutner MH, Neter J. Applied Linear Regression Models. McGraw-Hill/Irwin; 2004. 728 p. \nPrentice RL, Pyke R. Logistic Disease Incidence Models and Case-Control Studies. Biometrika. 1979;66(3):403–11."
  },
  {
    "objectID": "week12/week12.html",
    "href": "week12/week12.html",
    "title": "Week 12",
    "section": "",
    "text": "Modeling Wester Collaborative Group Study (WCGS)\nReturning to the WCGS, the dataset has a number of covariates that we might consider including in a model for CHD risk:\nHow do we approach making decisions about what to include in the model?\nWe discussed model selection based on a priori knowledge previously.\nOne strategy is to fit and report the following three models:\nThen we would report analysis 2 as the main analysis, reporting analysis 1 and 3 as sensitivity analyses.\nAdd core adjustment variables:\nWe think the \\(\\chi^2\\) reported in the model.fit statistics is the global \\(\\chi^2\\) statistic for a likelihood ratio test, but we should double-check.\nThe coefficients’ z-values and \\(p\\)-values are related to univariate Wald tests.\nInterpretation of \\(\\widehat{OR} = \\exp(\\beta_{\\text{behave}}) = 1.99\\) from the core model:\nIt’s hard to make much of the residuals… Recall that the deviance residuals are the preferred residuals in GLM settings.\nSo far, we’ve only considered the logit link function:\n\\[g(\\mu_i) = \\log \\left( \\frac{\\mu_i}{1-\\mu_i} \\right) = x_i' \\beta\\]\nBy far this is the most common link function used for GLMs of binary data.\nWhat about other link functions?\nPotential choices include:\nFor the goal of characterizing the association between behavior type and risk of CHD, interpretabililty is crucial.\nIn R we use the family= argument to change the link. Other components of the GLM that are functions of the link are appropriately adjusted.\nFirst, consider changing the link in the unadjusted analysis:\nThe main difference between using link = 'identity' vs. without inside the binomial() is that it’s changing the variance to use the binomial variance.\nNote for thought: Think more about how this is not GLS: is it because it’s using Newton Raphson (?)\nInterpretation of \\(\\widehat{RR} = \\exp(\\hat \\beta_{\\text{behave}}) = 2.21\\):\nThe fitted values are the same from the unadjusted logit, identity, and log link models because these are saturated models.\nA saturated model is a model that estimates a separate parameter for all unique values of \\(x_i\\).\nThere are numerous equivalent ways to define saturated models, so you may encounter other (equivalent) definitions elsewhere.\nA model with a single binary predictor is saturated because there are two possible values of \\(x_i\\) and two parameters \\((\\beta_0, \\beta_1)\\).\nBecause saturated models contain a separate parameter corresponding to each possible level of \\(\\mathbb E[Y_i \\mid x_i] = \\mu_i\\), they can perfectly fit the expected value in each \\(x\\) group.\nI.e., \\(\\hat{\\mathbb E}[Y_i \\mid x_i = x^\\star]\\) will be the sample mean of units with \\(x = x^\\star\\).\nSaturated models will thus give the same fitted values, \\(\\hat \\mu_i\\), regardless of the link function.\nChanging the link function from the default can cause IRLS to have trouble finding starting values.\nFor example, if we try to fit glm(chd ~ behave + age + wt + sbp + chol + smoker, family = binomial(link='identity'), data = wcgs), we will get an error.\nThe problem is that it can’t find a set of starting coefficients where all of the predicted values are in the \\((0,1)\\) range.\nOne option is to pass your own starting values in:\nWhat people often do is to use the output from another model that didn’t fail. One option is to fit the logistic regression, get the fitted values from that model, and then specify those as the starting fitted values for any of these models using alternative link functions.\nIf at any point during the IRLS algorithm, one of the fitted values is outside \\((0,1)\\), then \\(\\text{Var}(Y_i) = \\mu_i ( 1 - \\mu_i)\\) will be negative. This makes it likely that the model will either error or be unlikely to converge.\nAn alternative computational option in the identity link setting is to use OLS with an appropriate variance estimator to account for the heteroscedasticity induced by the mean-variance relationship.\nIn R:\nWe might also get similar errors about not finding valid starting values using a log link in a binomial outcome regression.\nIf we use the log-link, we get a relative risk. The interpretation of \\(\\exp(\\hat \\beta_{\\text{behave}}) = 1.78\\) is that it is the relative risk for Type A men is 78% higher than for Type B men, conditional on age, weight, SBP, cholesterol, and smoking.\nFor a continuous response variable, consider two models that could be used to assess the \\(Y - X\\) association:\n\\[\\mathbb E[Y \\mid X, Z] = \\beta_0^c + \\beta_X^c X + \\beta_Z^c Z \\label{eqn:1} \\tag{1}\\] \\[\\mathbb E[Y \\mid X] = \\beta_0^m + \\beta_X^m X \\label{eqn:2} \\tag{2} \\]\nIn model \\(\\ref{eqn:1}\\), \\(\\beta_X^c\\) is a conditinoal parameter, where contrasts condition on the value of \\(Z\\).\nIn model \\(\\ref{eqn:2}\\), \\(\\beta_X^m\\) is a marginal parameter, and contrasts using it do not condition on anything.\nThe relationship between the two parameters is clarified as follows:\n\\[\\mathbb E[Y\\mid X ] = \\mathbb E_Z[\\mathbb E_Y[Y \\mid X,Z] \\mid X ] \\tiny{\\tag{ Law of iterated expectations}}\\] \\[ = \\mathbb E_Z[\\beta_0^c + \\beta_X^c X + \\beta_Z^c Z ] \\] \\[ = \\beta_0^c + \\beta_X^c X + \\beta_Z^c \\mathbb E_Z[Z \\mid X] \\]\nSo the marginal contrast equals:\n\\[\\beta_X^m = \\mathbb E[Y \\mid X = (x+1)] - \\mathbb E[Y \\mid X = x]\\] \\[ = \\beta_X^c + \\beta_Z^c \\underbrace{\\{ \\mathbb E[Z \\mid X = (x+1) - \\mathbb E[Z \\mid X = x]] \\}}_{\\text{slope from a linear regression of } Z \\sim X } \\]\nConsidering the model \\(\\mathbb E[Z \\mid X] = \\gamma_0 + \\gamma_X X\\),\n\\[\\beta_X^m = \\beta_X^c + \\beta_Z^c \\gamma_X.\\]\nThe marginal contrast is the conditional plus a bias term.\nThe bias term (\\(\\beta_Z^c \\gamma_X\\)) is non-zero if both\nIf \\(Z\\) is a confounder and we don’t adjust for it, the marginal association we estimate will be biased.\nThe direction of the bias depends on the interplay between \\(\\beta_Z^c\\)\nIf \\(Z\\) isn’t a confounder, then one or both of \\(\\{ \\beta_Z^c, \\gamma_X \\}\\) are zero, so the bias term is zero and \\[\\beta_X^m = \\beta_X^c.\\]\nSo the marginal and conditional parameters are equivalent when the variables conditioned on are not confounders.\nThis is a result of the collapsibility of parameters from a linear regression (or any GLM using the identity link).\nIf \\(Z\\) is not a confounder, conditioning on it won’t do any harm since doing so does not change the quantity we’re estimating.\nWe call \\(Z\\) a precision variable if it is associated with \\(Y\\) but not \\(X\\).\nIf \\(Z\\) is a precision variable, the standard error of \\(\\beta_X^c\\) will be smaller than the standard error of \\(\\beta_X^m\\).\nIf \\(Z\\) explains a substantial amount of variability in \\(Y\\) then the conditional model SSE will be smaller, so \\(\\hat \\sigma^2\\) will also generally be smaller for the conditional model.\nBecause linear regression parameters are collapsible and including precision variables in the model leads to smaller uncertainties, it’s generally good practice to condition on precision variables.\nIs the same true for logistic regression?\nFor a binary outcome, consider two models:\n\\[\\text{logit} \\mathbb E[Y \\mid X, Z] = \\beta_0^c + \\beta_X^c X + \\beta_Z^c Z \\label{eqn:3} \\tag{3}\\] \\[\\text{logit} \\mathbb E[Y \\mid X] = \\beta_0^m + \\beta_X^m X \\label{eqn:4} \\tag{4}\\]\nThe conditional odds ratio for a binary \\(X\\) is\n\\[\\theta_X^c = \\exp (\\beta_X^c) = \\frac{\\mathbb E[Y \\mid X = 1, Z]}{1 - \\mathbb E[Y \\mid X = 1, Z]}\n\\bigg / \\frac{\\mathbb E[Y \\mid X = 0, Z]}{1 - \\mathbb E[Y \\mid X = 0, Z]} \\]\nThe marginal odds ratio for \\(X\\) is\n\\[\\theta_X^m = \\exp (\\beta_X^m) = \\frac{\\mathbb E[Y \\mid X = 1]}{1 - \\mathbb E[Y \\mid X = 1]}\n\\bigg / \\frac{\\mathbb E[Y \\mid X = 0]}{1 - \\mathbb E[Y \\mid X = 0]}.\\]\nAnalagous to what we did for linear regression, we’d like to try to write the marginal OR as a function of the conditional.\nRecall that to do this, we\n\\[\n\\begin{aligned}\n\\mathbb E[Y \\mid X] & = \\mathbb E_Z[\\mathbb E_Y[Y \\mid X, Z] \\mid X] \\\\\n& = \\mathbb E_Z\\left[ \\frac{\\exp\\{ \\beta_0^c + \\beta_X^c X + \\beta_Z^c Z \\} }{1 + \\exp\\{\n  \\beta_0^c + \\beta_X^c X + \\beta_Z^c Z\n\\}} \\bigg\\vert X \\right] \\\\\n& = \\int_Z \\left( \\frac{\\exp\\{ \\beta_0^c + \\beta_X^c X + \\beta_Z^c Z \\} }{1 + \\exp\\{\n  \\beta_0^c + \\beta_X^c X + \\beta_Z^c Z\n\\}} \\right) f_{Z|X}(Z = z \\mid X) \\partial z \\\\\n%  & = \\int_Z \\left( \\frac{\\theta_X^c \\exp\\{ \\beta_0^c+ \\beta_Z^c Z \\} }{1 + \\theta_X^c \\exp\\{\n%  \\beta_0^c + \\beta_Z^c Z\n% \\}} \\right) f_{Z|X}(Z = z \\mid X) \\partial z.\n\\end{aligned}\n\\]\nIf we consider plugging this expression for \\(\\mathbb E[Y\\mid X]\\) into\n\\[\\theta_X^m = \\exp (\\beta_X^m) = \\frac{\\mathbb E[Y \\mid X = 1]}{1 - \\mathbb E[Y \\mid X = 1]}\n\\bigg / \\frac{\\mathbb E[Y \\mid X = 0]}{1 - \\mathbb E[Y \\mid X = 0]},\\]\nwe can see that the relationship between the conditional OR \\(\\theta_X^m\\) and marginal OR \\(\\theta_X^m\\) is not straightforward.\nThere is no simple, closed-form expression for \\(\\theta_X^m\\) as a function of \\(\\theta_X^c\\). Unlike in linear regression, they are not linearly related.\nHowever, given \\(\\theta_X^c\\) we can calculate \\(\\theta_X^m\\) numerically.\nTo do so, from the expression for \\(\\mathbb E[Y \\mid X]\\) we need to specify:\nFor the purposes of this exercise, we’ll consider binary \\(X\\) and \\(Z\\) that are related via logistic regression\n\\[\\text{logit} \\mathbb E[Z \\mid X] = \\gamma_0 + \\gamma_X X.\\]\nNotationally, let \\(\\phi_{XZ} = \\exp(\\gamma_X)\\) denote the \\(Z \\sim X\\) odds ratio.\nWe consider a range of values of\nand for each scenario we compute \\(\\theta_X^m\\) and the percent difference:\n\\[\\frac{\\theta_X^m - \\theta_X^c}{\\theta_X^c} \\times 100.\\]\nQuick review of GLMs.\nThe exponential dispersion family can be written as\n\\[f(y \\mid \\theta, \\phi) = \\exp \\left\\{ \\frac{y \\theta - b(\\theta)}{a(\\phi)}  + c(y, \\phi)\\right\\}\\]\nwhere \\(\\theta\\) is the canonical parameter and \\(\\phi\\) is the dispersion parameter. We also showed in class that\n\\[\\mathbb E[Y_i] = \\mu_i = b'(\\theta_i)\\] \\[\\text{Var}[Y_i] = b''(\\theta_i) a_i(\\phi)\\]\nWe refer to \\(b''(\\theta_i)\\) as the variance function and sometimes we write it as \\(V(\\mu_i) = b''(\\theta_i) = b''(b'^{-1}(\\mu_i)),\\) i.e., \\(V\\) can be some function of \\(\\mu\\). Then we fit a GLM by assuming the model \\(g(\\mu_i) = \\eta_i = X_i'\\beta\\)."
  },
  {
    "objectID": "week12/week12.html#comparison-of-approaches",
    "href": "week12/week12.html#comparison-of-approaches",
    "title": "Week 12",
    "section": "Comparison of Approaches",
    "text": "Comparison of Approaches\nOften linear models are motivated by the following minimization problem:\n\\[\\min_{\\beta} || \\mathbf{y} - \\mathbf{X} \\beta ||_2^2\\]\nAssuming that \\(\\mathbb E[Y] = X \\beta\\).\nA more statistically driven approach is to assume that\n\\[Y\\mid X \\sim \\mathcal N(X \\beta, \\sigma^2),\\]\nand we want to \\(\\max_{\\beta, \\sigma^2} f_N(y, X, \\beta) \\approx e^{-\\frac{(y-X\\beta)^2}}.\\)\nIt is a happy coincidence that in both the ML and statistically driven approach, we derive that \\(\\hat \\beta = (X^T X)^{-1} X^T y\\), “our favorite equation.”\n\nConsider a Binomial Problem\nConsider the problem where we have a binary random variable \\(Y \\sim \\text{Bernoulli}(\\mu)\\) with pdf\n\\[f_Y(y) = \\mu^y(1-\\mu)^{1-y}.\\]\nIn this case, we assume that \\[Y \\mid X \\sim \\text{Bernoulli}(\\mathbb E(Y|X)),\\]\nand \\(\\mathbb E[Y|X] = \\text{P}(Y = 1)\\), therefore \\(0 \\leq \\mathbb E[Y|X] \\leq 1\\).\nThere are several ways to enforce this bounding. Let \\(\\phi : \\R \\to [0,1]\\).\n\nOne way could be to model \\(\\mathbb E(\\phi(Y) | X)\\), but this has two problems: interpretation, and more importantly, we’re not necessarily interested in the distribution of \\(\\phi(Y)\\).\nAnother option is to transform the \\(X\\) values so that \\(\\mathbb E(Y | X) = \\phi(X) \\beta\\).\nThe last way is to write \\(\\mathbb E(Y) = \\phi(X \\hat \\beta)\\). Another way to write this is to let \\(g = \\phi^{-1}\\) and write \\(g(\\mathbb E(Y|X)) = X \\hat \\beta.\\)\n\nWe have two constraints on \\(g\\): it has to convert values from \\(\\R\\) to \\([0,1]\\) as well as has nice interpretability.\n\n\n\n\nRespects Constraints\nInterpretable\n\n\n\n\n\\(g(y) = y\\)\nNo\nVery\n\n\nprobit\nYes\nNot very\n\n\n\n\n\n\n\n\n(a). Show that this distribution belongs to the exponential dispersion family and find expressions for \\(\\theta\\), \\(b(\\theta)\\), \\(a(\\phi)\\), and \\(c(y, \\phi)\\).\n\\[\n\\begin{aligned}\nf_Y(y) & = \\mu^y (1 - \\mu)^{1-y} \\\\\n& = \\exp \\{ \\log \\{ \\mu^y (1-\\mu)^{1-y} \\} \\} \\\\\n& = \\exp \\{ y \\log(\\mu) + (1-\\mu) \\log(1-\\mu) \\} \\\\\n& = \\exp \\{ y \\log \\left( \\frac{\\mu}{1-\\mu} + \\log(1-\\mu) \\right) \\} \\\\\n& = \\exp \\{ y \\theta - \\log(1 + \\exp(\\theta)) \\},\n\\end{aligned}\n\\]\nwhere \\(\\theta = \\log \\left( \\frac{\\mu}{1-\\mu} \\right)\\), \\(b(\\theta) = \\log(1 + \\exp(\\theta))\\), \\(a(\\phi) = 1\\), and \\(c(y,\\phi) = 0\\).\n\\(\\theta\\) is often a nice candidate for the link function.\n\nThe exponential dispersion family is a generalization of what’s called the natural exponential family.\n\nRecall that \\(\\text{expit}(X \\hat \\beta) = \\frac{e^{X\\beta}}{1-e^{X\\hat\\beta}}\\).\nConsider the simple logistic regression model for a binary outcome \\(y_i\\) and a single covariate \\(x_i\\)\n\\[\\text{\\text{logit}}(\\mu_i) = \\beta_0 + \\beta_1 x_i \\tag{1} \\]\nwhere \\(\\mu_i = P(Y_i = 1 | X_i = x_i) = \\mathbb E(Y_i | X_i = x_i)\\) and \\(\\text{logit}(\\mu_i) = \\log \\left( \\frac{\\mu_i}{1-\\mu_i} \\right)\\). Since we’ve already shown the Bernoulli distribution falls within the exponential dispersion family, it should be clear that model (1) is a GLM with the \\(\\text{logit}\\) link function.\n\\(\\beta_0\\) is the log odds of the outcome \\(Y\\) for subjects with \\(x_i = 0\\).\n\\[\\log \\left( \\frac{\\mu_i}{1-\\mu_i} \\right) = \\text{logit}(\\mu_i) = \\beta_0 + \\beta_1(0) = \\beta_0\\]\n\\(\\beta_1\\)\n\nAdvice for interpreting \\(\\beta_0\\) and \\(\\beta_1\\) for some random GLM on an exam:\nLook at the difference between an observation with \\(x^\\star\\) and \\(x^\\star + 1\\).\n\\[\\log \\left( \\frac{\\mu_i / (1-\\mu_i)}{\\mu_j/(1-\\mu_j)} \\right) =\n\\text{logit}(\\mu_i) - \\text{logit}(\\mu_j) = \\beta_0 + \\beta_1(x + 1) - \\beta_0 - \\beta_1x = \\beta_1\\]\n\nNow suppose we have collected \\(n\\) independent observations. Show that the log-likelihood based on model (1) is\n\\[\\ell(\\beta_0, \\beta_1 | y) = \\sum_{i=1}^n \\{ y_i (\\beta_0 + \\beta_1 x_i) - \\log(1 + \\exp(\\beta_0 + \\beta_1 x_i)) \\}.\\]\nAnswer:\n\\[L(\\theta) = \\prod \\exp \\{ y_i \\theta_i - \\log (1 + \\exp \\theta_i) \\}\\]\nTherefore\n\\[\\ell(\\theta | y) = \\sum y_i \\theta_i - \\log (1 + \\exp \\theta_i)\\] \\[\\ell(\\beta_0, \\beta_1, y) = \\sum y_i ( \\beta_0 + \\beta_1 x_i) - \\log (1 + \\exp(\\beta_0 + \\beta_1 x_i))\\]\nObtain the score equations \\(U(\\beta_0)\\) and \\(U(\\beta_1)\\).\n\\[\n\\begin{aligned}\nU(\\beta_0) = \\frac{\\partial \\ell}{\\partial \\beta_0} & = \\sum_{i=1}^n \\left\\{\n  y_i - \\frac{\\exp(\\beta_0 + \\beta_1 x_i)}{1 + \\exp(\\beta_0 + \\beta_1 x_i)}\n  \\right\\} \\\\\n  & = \\sum (y_i - \\mu_i) \\stackrel{\\text{set}}{=} 0\n  \\end{aligned}\n  \\]\n\\[\n\\begin{aligned}\nU(\\beta_1) = \\frac{\\partial \\ell}{\\partial \\beta_1} & = \\sum_{i=1}^n \\left\\{\n  y_i x_i - \\frac{x_i \\exp(\\beta_0 + \\beta_1 x_i)}{1 + \\exp(\\beta_0 + \\beta_1 x_i)}\n  \\right\\} \\\\\n  & = \\sum x_i (y_i - \\mu_i) \\stackrel{\\text{set}}{=} 0\n  \\end{aligned}\n\\]\nShow that the Fisher Information Matrix \\(\\mathcal I(\\beta)\\) takes the following form:\n\\[\\mathcal I(\\beta) = \\pmatrix{\n\\sum \\mu_i (1- \\mu_i) & \\sum x_i \\mu_i ( 1- \\mu_i) \\\\\n\\sum x_i \\mu_i (1-\\mu_i) & \\sum x_i^2 \\mu_i(1-\\mu_i)\n}.\\]\nWhere \\(\\mu_i = \\text{expit}(\\beta_0 + \\beta_1 x_i)\\) and \\(\\text{expit}(x) = \\frac{\\exp(x)}{1 + \\exp(x)}\\)\nThe Fisher Information Matrix is like the Hessian of the log-likelihood.\nThe higher the curvature, the lower the variance.\nAppendix:\n(Fill out soon…)"
  },
  {
    "objectID": "week13/week13.html",
    "href": "week13/week13.html",
    "title": "Week 13",
    "section": "",
    "text": "Binary Response Data: Case-Control Studies\nOutline:"
  },
  {
    "objectID": "week13/week13.html#study-design",
    "href": "week13/week13.html#study-design",
    "title": "Week 13",
    "section": "Study Design",
    "text": "Study Design\nSo far, we’ve considered estimation and inference based on an independent sample size of size \\(n\\), \\(\\{ (X_i, Y_i) : i = 1, ..., n \\}\\) and the likelihood:\n\\[\\mathcal L = \\prod_{i=1}^n P(Y_i | X_i)\\]\nWe parametrize \\(P(Y|X)\\) in terms of a regression model, \\(\\mu = \\mathbb E[Y|X,\\beta]\\).\nWe want to learn about the regression coefficients, \\(\\beta\\).\nWe’ve often implicitly assumed our data come from cross-sectional sampling where one chooses individuals completely at random and we observe their outcomes and/or covariates.\nIn a cross-sectional sample, \\((Y, X)\\) are jointly random, so that the likelihood is:\n\\[\\mathcal L_{\\text{joint}} = \\prod_{i=1}^n P(Y_i, X_i)\\] \\[ = \\prod_{i=1}^n P(Y_i | X_i) P(X_i)\\]\nHowever, we generally assume that the marginal covariate distribution, \\(P(X_i)\\) does not involve \\(\\beta\\). Thus we can base estimation/inference on\n\\[\\mathcal L = \\prod_{i=1}^n P(Y_i|X_i)\\]\nThis is because\n\\[\\log \\mathcal L = \\sum_i \\log P(Y_i | X_i) + \\sum_i \\log P(X_i).\\]\nIf \\(P(X_i)\\) doesn’t involve any \\(\\beta\\) terms, then when we go to maximize/optimize against \\(\\mathcal L\\) with respect to \\(\\beta\\), the \\(P(X_i)\\) terms will fall out.\nIn binary outcome settings, we often need a surprisingly large sample size to have reasonable power. Recall that \\(\\text{Power} = P(\\text{Reject } H_0 | H_0 \\text{ false})\\).\nPower analyses are commonly used in the design of studies to determine how large \\(n\\) should be in order to allow reasonable power to detect the anticipated effect size.\nFor some very simple analytic approaches (e.g., \\(t\\)-tests), there are closed form formulas for power under a given sample and effect size.\nHere we will conduct a power analysis using a simulation approach to both: 1) demonstrate that random sampling designs often lead to low power with (rare) binary outcomes, and 2) give a sense of how to do power analyses using simulations.\nWe conduct a simulation study to assess power to detect an association between a binary outcome and a binary exposure, conditional on several other variables.\nWe’ll look at power in a range of scenarios:\n\nEffect sizes (conditional ORs): 1.5, 2.0, and 3.0 (all considered large)\nRandom samples of size: 3,000 to 8,000\nFor each scenario (combination of OR and \\(n\\)), we\n\nSimulate many datasets with the given specs\nFit the model to each simulated dataset\nCompute the percent of datasets for which the null is rejected.\n\n\nWe’re going to assume that incidence in the population is about 5%.\n\n\nlibrary(tidyverse) \n\nORs <- c(1.5, 2.0, 3.0)\n\nn <- seq(3000, 8000, length.out = 8)\n\npct_to_odds <- function(pct) { pct / (1 - pct) }\nodds_to_pct <- function(o) { o / (1+o) }\nN_iters <- 100\n\npower_outcomes <- list()\n\nfor (or in ORs) {\n  for (sample_size in n) { \n    for (iteration in 1:N_iters) {\n      # simulate data\n      sample_size_exposed <- round(sample_size / 50)\n      sample_size_unexposed <- round(sample_size / 50 * 49)\n\n      df <- data.frame(\n        Y = c(\n          rbinom(n = sample_size_unexposed, size = 1, prob = .05),\n          rbinom(n = sample_size_exposed, size = 1, prob = odds_to_pct(pct_to_odds(.05) * or))),\n        X = c(rep('cat1', sample_size_unexposed), rep('cat2', sample_size_exposed))\n      )\n\n      # fit model \n      model <- glm(Y ~ X, family = binomial(), data = df)\n\n      # assess significance \n      reject_H0 <- broom::tidy(model)[[2,'p.value']] < 0.05\n\n      # store outcomes\n      power_outcomes[[length(power_outcomes) + 1]] <- c(\n        OR = or, sample_size = sample_size, significant = reject_H0\n      )\n    }\n  }\n}\n\npower_outcomes <- dplyr::bind_rows(power_outcomes)\n\npower_outcomes_summary <- power_outcomes |> \n  group_by(sample_size, OR) |> \n  summarize(pct_significant = sum(significant)/n())\n\nggplot(power_outcomes_summary, \n  aes(x = sample_size, y = pct_significant, color = factor(OR), shape = factor(OR))) + \ngeom_point() + \ngeom_line()\n\nA key reason why power is so low is because the outcome is somewhat rare (5%).\nWe get few events, leading to large standard errors, and therefore low power.\nRepeated simulations increasing the incidence (fixing \\(n=4000\\)), achieved by manipulating \\(\\beta_0\\), here we pick \\(\\beta_0\\) values yielding incidences from \\(0.05\\) to \\(0.25\\).\nConsidering varying incidence rates:\n\nlibrary(gt)\npwr_df <- tibble::tribble(\n  ~`Odds Ratio`, ~`0.05`, ~`0.10`, ~`0.15`, ~`0.20`, ~`0.25`,\n      1.5    ,  18.6 ,  24.5 ,  29.3 ,  30.8 ,  32.8 , \n      2.0    ,  42.9 ,  57.2 ,  65.3 ,  69.1 ,  70.5 \n)\n\ngt(pwr_df) |> \n  tab_header(\n    title = md(\"**Relationship of Incidence, Odds Ratios, and Statistical Power**\")\n  ) |> \n  tab_spanner(\n    columns = c(2:6), \n    label = \"Incidence Rate\"\n  )  \n\n\n\n\n\n  \n    \n      Relationship of Incidence, Odds Ratios, and Statistical Power\n    \n    \n    \n      Odds Ratio\n      \n        Incidence Rate\n      \n    \n    \n      0.05\n      0.10\n      0.15\n      0.20\n      0.25\n    \n  \n  \n    1.5\n18.6\n24.5\n29.3\n30.8\n32.8\n    2.0\n42.9\n57.2\n65.3\n69.1\n70.5\n  \n  \n  \n\n\n\n\n\nIt’s also important if the exposure is relatively rare. That combination of rare exposure and rare outcome is particularly damaging to power.\nAs incidence increases, power increases. But of course, we cannot and usually should not increase incidence in the population. However, we can manipulate the relative number of cases and non-cases that we observe in the data. I.e., we artificially inflate the observed incidence. For example, this can be done via a case-control design, sometimes called “outcome dependent sampling.”\nThe problem is that the sample is no longer representative of the target population. But this non-randomness is by design, under the control of researchers. Such designs can be referred to as biased sampling schemes and we can use statistical techniques to account for the non-random sampling.\nIn a case-control study, we initially stratify the population by outcome status, we know \\(Y = 0/1\\) for everyone a priori.\nWe proceed by sampling, at random, to get \\(n_1\\) cases (i.e., for whom \\(Y = 1\\)) and \\(n_0\\) non-cases or controls (i.e., for whom \\(Y = 0\\)).\nFor all \\(n = n_0 + n_1\\) sampled individuals, ‘observe’ the value of their covariates. It’s crucial that \\(X\\) is random and not \\(Y\\).\nThe appropriate likelihood is\n\\[\\mathcal L_R = \\prod_{i=1}^n P(X_i | Y_i)\\]\n\\[ = \\prod_{i=1}^{n_0} P(X_i | Y_i = 0) \\prod_{i=n_0+1}^{n} P(X_i | Y_i=1)\\]\nThis is often referred to as a retrospective likelihood. However, the scientific goal is typically to learn about the prospective associations. I.e., \\(P(Y|X)\\)\nHow do we learn about prospective associations from the retrospective likelihood?\nAs we’ve noted, case-control sampling is non-random with respect to the target population. We formalize this by introducing a random variable \\(S\\), which is an indicator of selection by the sampling scheme.\n\\[ S = \\left\\{ \\begin{array}{ll} 1 \\quad & \\text{ selected} \\\\ 0 & \\text{ not selected.} \\end{array} \\right.\\]\n\\(S\\) is a binary random variable with some probability \\(P(S = 1)\\).\nCross-sectional sampling is where selection is independent of \\((Y,X)\\), and \\(P(S = 1)\\) is constant. If we’re going to sample \\(n\\) people out of a population of size \\(N\\), then \\(P(S_i = 1) = n/N\\).\nIn case-control sampling, selection depends on outcome status \\(Y\\) and we could write \\(P(S=1 \\mid Y = y)\\).\nApplying Bayes’ rule, we can write the retrospective likelihood as\n\\[\\mathcal L_R = \\prod_{i=1}^n P(X_i | Y_i) = \\prod_{i=1}^n P(X_i | Y_i, S_i = 1)\\] \\[ = \\prod_{i=1}^n P(Y_i | X_i, S_i) \\frac{P(X_i | S_i = 1)}{P(Y_i | S_i = 1)}\\]\nFor now, let’s focus on the \\(P(Y | X, S = 1)\\) term. This looks similar to our quantity of interest, \\(P(Y|X)\\), and is something we can learn about from the case-control data.\nLet’s assume that the true model of the prospective associations in the target population of interest is given by\n\\[\\text{logit}P(Y=1 \\mid X)  = X' \\beta.\\]\nWe can derive an expression for \\(P(Y=1 \\mid X, S=1)\\) in terms of the true prospective association parameters of interest, \\(\\beta\\).\nApplying Bayes’ rule and noting that selection depends solely on \\(Y\\):\n\\[\\begin{aligned}\nP(Y = 1 \\mid X , S = 1) & = \\frac{P(S = 1 \\mid X, Y = 1) P(Y = 1 \\mid X)}{P(S = 1 \\mid X)} \\\\\n& = \\frac{P(S = 1 \\mid X, Y = 1) P(Y = 1 \\mid X)}{\\sum_{y=0}^1 P(S = 1 \\mid X, Y = y) P(Y= y | X)} \\\\\n& = \\frac{P(S = 1 \\mid Y = 1) P(Y = 1 \\mid X)}{\\sum_{y=0}^1 P(S = 1 \\mid Y = y) P(Y= y | X)} \\\\\n& = \\frac{\\pi_1 P(Y = 1 \\mid X)}{\\sum_{y=0}^1 \\pi_y P(Y= y | X)}, \\\\\n\\end{aligned}\\]\nwhere \\(\\pi_y = P(S = 1 \\mid Y = y)\\) (and \\(\\pi_1 = \\pi_{y = 1}\\)).\nSo\n\\[P(Y = 1 \\mid X, S = 1) = \\frac{\\pi_1 P(Y=1 \\mid X)}{\n  \\pi_1 P(Y=1 \\mid X) + \\pi_0 P(Y=0 \\mid X).\n}\\]\nIf we divide the numerator and denominator by \\(\\pi_0 \\times P(Y=0 \\mid X)\\),\n\\[\\begin{aligned}\nP(Y=1 \\mid X, S = 1) & = \\frac{\\frac{\\pi_1}{\\pi_0} \\overbrace{\\frac{P(Y = 1|X)}{P(Y=0|X)}}^{\\text{odds of outcome}}}{\n\\frac{\\pi_1}{\\pi_0}  \\frac{P(Y = 1|X)}{P(Y=0|X)} + \\underbrace{\\frac{\\pi_0}{\\pi_0} \\frac{P(Y = 0|X)}{P(Y=0|X)}}_{=1}\n} \\\\\n& = \\frac{\n  \\frac{\\pi_1}{\\pi_0} \\exp \\{ X' \\beta \\}\n}{\n  1 + \\frac{\\pi_1}{\\pi_0} \\exp \\{ X' \\beta \\}\n} \\\\\n& = \\frac{\n  \\exp \\{ \\log \\left( \\frac{\\pi_1}{\\pi_0} \\right) + X' \\beta \\}\n}{\n  1 + \\exp \\{  \\log \\left( \\frac{\\pi_1}{\\pi_0} \\right) + X' \\beta \\}\n} \\\\\n& = \\frac{\n  \\exp \\{ \\beta_0^* + \\beta_1 X_1 + ... + \\beta_p X_p \\}\n}{\n  1 + \\exp \\{ \\beta_0^* + \\beta_1 X_1 + ... + \\beta_p X_p \\}\n} \\\\\n& = \\text{expit}(X' \\beta^*)\n\\end{aligned}\n\\]\nwhere \\(\\beta_0^* = \\beta_0 + \\log\\left( \\frac{\\pi_1}{\\pi_0} \\right)\\) and the only difference between \\(\\beta\\) and \\(\\beta^*\\) is in the intercept term.\nWe see that \\(P(Y=1\\mid X, S=1)\\) has the same functional form as the desired logistic regression model, with only the intercept differing.\nIf the true \\(P(Y=1 \\mid X)\\) is given by a logistic regression, then so is \\(P(Y=1 \\mid X, S = 1)\\).\nThe odds ratio relationships between \\(X\\) and \\(Y\\) are preserved despite the selection process.\nThe intercept for the two logistic models are different, but usually we aren’t concerned about estimating/interpreting the intercept.\nRecall the retrospective likelihood is\n\\[\\mathcal L_R = \\prod_{i=1}^n P(X_i | Y_i) = \\prod_{i=1}^n P(Y_i | X_i, S_i = 1) \\frac{P(X_i | S_i = 1)}{P(Y_i|S_i=1)}\\]\nwhere we now know the form of \\(P(Y_i \\mid X_i, S_i = 1)\\).\nDo we need to worry about the \\(\\frac{P(X_i | S_i=1)}{P(Y_i|S_i=1)}\\) term? Or can we proceed with estimation of \\(\\beta\\) ignoring that term?\nIn theory, no, we cannot ignore this term.\nConsider that \\(P(Y=0 \\mid S=1)\\) and \\(P(Y=1 \\mid S = 1)\\) are fixed by design. This imposes constraints on \\(P(Y\\mid X, S = 1)\\) and thereby also on \\(\\beta\\), i.e.,\n\\[P(Y = 1 \\mid S = 1) = \\int_{\\mathcal X} P(Y = 1 \\mid X = x, S = 1) P(X = x) \\mathrm d x.\\]\nThis indicates that to obtain an estimate of \\(\\beta\\) via\n\\[\\mathcal L^* = \\prod_{i=1}^n P(Y_i \\mid X_i, S_i = 1),\\]\none must maximize over a constrained parameter space. E.g., using a constrained optimization procedure such as Lagrange multipliers.\nHowever, Prentice and Pyke (1979) showed that it turns out that the constrained MLE is the same as the unconstrained MLE, and the asymptotic variance is also the same.\nSo we can ignore the constraints imposed by the sampling scheme and we can proceed fitting the logistic regression as usual in case-control studies. These results only hold for logistic regression, i.e., binomial regression with the logit link.\nIt has also been shown that ordinal likelihood ratio tests are valid in case-control studies (Scott and Wild 1989).\nCaveats:\n\nWe cannot draw conclusions about \\(\\beta_0\\) without additional information.\nCannot perform prediction without additional information.\nCannot learn about other contrasts such as the relative risk without additional information.\n\nFurther, one must also be aware of a number of non-statistical issues such as issues associated with observational studies, appropriate selection of controls, and recall bias.\nRecall bias refers to the bias that people often don’t do a good job reporting (recalling) exposures that they were exposed to in the past, especially if it’s been a long time since the exposure (whatever “long time” means depends on context)."
  },
  {
    "objectID": "week13/week13.html#esophageal-cancer-example",
    "href": "week13/week13.html#esophageal-cancer-example",
    "title": "Week 13",
    "section": "Esophageal cancer example",
    "text": "Esophageal cancer example\nConsider data from a case-control study on the association between alcohol and tobacco consumption and risk of esophageal cancer conducted in France in the 1970s. Data are available directly in R’s datasets package.\nIn the sample, 200, 1,175 individuals are cases. 17% of the sample had esophageal cancer. Overall incidence in the U.S. is around 5 per 100,000.\nThe data also contain information on 3 covariates categorized in: age in years, tobacco consumption in gm/day, and alcohol consumption in gm/day."
  },
  {
    "objectID": "week14/week14.html",
    "href": "week14/week14.html",
    "title": "Week 14",
    "section": "",
    "text": "Preface\nWe’re continuing on to matched case-control studies and conditional logistic regression.\nReview Moral: Only logistic regression can be applied “same as always” in case control studies to estimate odds ratios as long as one doesn’t care about the intercept (which is biased).\nToday we’ll talk about difficulties with matched case control study.\nThe case-control design assumes the population can be stratified on the basis of \\(Y\\).\nSuppose we also have access to information on certain covariates for everyone: e.g., we know the sex and race of the baby, but we don’t (readily) have information on the mother or the pregnancy. We’ll denote this set of covariates by \\(Z\\).\nWe could impose balance on \\(Z\\) by drawing controls such that the distribution of \\(Z\\) is the same as in cases. E.g., each time you draw a case, only draw from the controls with the same (or very similar) values of \\(Z\\).\nIn matched case-control studies, the observations are structured into a series of “matched sets”. Often each matched set contains a single case, a fixed common number of controls, \\(M\\), which is referred to as an \\(M:1\\) matched case-control study.\nIn general, we will allow for:\nThe data structure is given as:\nThe data might look like:\nRecall the retrospective likelihood: \\(\\mathcal L_R = \\prod P(X_i | Y_i)\\).\nRecall we were able to rewrite that as a function of the prospective likelihood multiplied by a nuisance term.\nBy having matched, we have introduced a new form of non-randomness. Retrospective sampling of \\(X\\) is only random within each matched set.\nThe appropriate retrospective likelihood is thus: \\[\\mathcal L_r = \\prod_{k=1}^K \\prod_{i=1}^{N_k} P(X_{ki} | Y_{ki}, \\text { set } k).\\]\nWe could follow the same arguments as before and base estimation/inference on the prospective likelihood\n\\[\\mathcal L^* = \\prod_{k=1}^K \\prod_{i=1}^{N_k} P(Y_{ki} | X_{ki}, \\text{ set } k, S = 1)\\]\nagain ignoring the constraints imposed by the fixed number of cases and controls.\nNote for what follows, we’ll often be implicitly conditioning on \\(S\\).\nThe likelihood requires specification of the distribution of \\(Y\\) conditional on both \\(X\\) and being in the \\(k\\)th matched set.\nThis can be achieved by including a set-specific intercept into the model. For a single covariate,\n\\[\\text{logit}P(Y_{ki} = 1 \\mid X_{ki}, \\text{ set }k) = \\alpha_k + \\beta_1 X_{ki}.\\]\nAs with a standard case-control study, we could proceed with estimation of \\(\\{ \\alpha_1, ..., \\alpha_K, \\beta_1 \\}\\) by ignoring the (retrospective) sampling scheme (Prentice and Pyke, 1979).\nHowever, the asymptotics are not well-behaved because the number of parameters increases with the sample size due to the set-specific intercepts.\nThe solution is that set-specific intercepts should be treated as nuisance parameters. Interest lies in \\(\\beta_1\\). The interpretation of the \\(\\alpha_k\\) values doesn’t correspond to anything real, the matched set constructs that they represent are artificial.\nThe \\(K\\) intercepts \\(\\{ \\alpha_1, ..., \\alpha_K \\}\\) are nuisance parameters.\nTwo general techniques for eliminating nuisance parameters include:\n“This course considers the use of regression as a tool for data analysis with outcomes that are univariate and independent.”\nSo far, we’ve primarily considered two types of outcomes:\nContinuous outcomes, \\(Y \\in \\mathbb R\\) where\n\\[Y_i \\sim \\mathcal N(\\mu_i, \\sigma^2)\\] \\[\\mu_i = x_i' \\beta\\]\nAnd binary outcomes,\n\\[Y_i \\sim \\text{Bernoulli}(\\mu_i)\\] \\[\\text{logit}(\\mu_i) = x_i'\\beta\\]\nIn many applications, the response variable is in the form of a count:\n\\(Y = 0, 1, 2, ...\\)\nExamples could include:\nWhen we count events, we can consider them as arising in one of two ways:\nWhat is a saturated model? A model where the number of observations is equal to the number of parameters. The result is often a perfectly fit model.\nThis can be useful for a binomial model on a contingency table."
  },
  {
    "objectID": "week14/week14.html#north-carolina-birth-example-data",
    "href": "week14/week14.html#north-carolina-birth-example-data",
    "title": "Week 14",
    "section": "North Carolina Birth Example Data",
    "text": "North Carolina Birth Example Data\nSuppose interest lies in the relationship between gestational age at birth and infant mortality, i.e., death within the first year of life.\nThe infants dataset has information on 225,152 births for all white and African-American births in North Carolina in 2003-04.\nInfant mortality is a rare event: 1,752 events in the available data constituting a rate of 8 per 1,000 births.\nIn the absence of these complete data, we would need to conduct a study."
  },
  {
    "objectID": "week14/week14.html#emulating-a-case-control-study",
    "href": "week14/week14.html#emulating-a-case-control-study",
    "title": "Week 14",
    "section": "Emulating a case-control study",
    "text": "Emulating a case-control study\nSuppose we have sufficient resources to collect information on \\(n=400\\) births.\nSimple random sampling would yield few cases, probably around the average of 3 deaths.\nA case-control design would be much more efficient:\n\nRandomly sample \\(n_1 = 200\\) cases from the 1,752 infant deaths\nRandomly sample \\(n_0 = 200\\) controls from the 223,400 non-deaths\nRetrospectively ‘observe’ their covariate values, \\(X\\), and\nInclude information on a number of potential confounders such as:\nMothers’ age, smoking during pregnancy, weight gained, baby’s race and sex\n\n\nload(here::here(\"data/NorthCarolina_data.dat\"))\n\n## create an indicator of case/control status\ninfants$Y <- as.numeric(infants$dTime < 365)\n\ntable(infants$Y)\n\n\n     0      1 \n223400   1752 \n\n## randomly sample 200 cases and 200 controls\ncases <- sample(c(1:nrow(infants))[infants$Y == 1], 200)\nconts <- sample(c(1:nrow(infants))[infants$Y == 0], 200)\ndataCC <- infants[c(conts, cases),]\n\n## confirming that we have 200 cases/controls in dataCC\ntable(dataCC$Y)\n\n\n  0   1 \n200 200 \n\n## check covariate distribution\ntapply(dataCC$sex, dataCC$Y, FUN = mean)\n\n   0    1 \n0.53 0.59 \n\ntapply(dataCC$race, dataCC$Y, FUN = mean)\n\n   0    1 \n0.25 0.46 \n\ntapply(dataCC$smoker, dataCC$Y, FUN = mean)\n\n    0     1 \n0.110 0.205"
  },
  {
    "objectID": "week14/week14.html#covariate-imbalance-in-case-control-studies",
    "href": "week14/week14.html#covariate-imbalance-in-case-control-studies",
    "title": "Week 14",
    "section": "Covariate imbalance in case-control studies",
    "text": "Covariate imbalance in case-control studies\nThere appears to be substantial imbalance in the distributions of several key covariates between the cases and controls.\nMany of these covariates are also likely to be strongly associated with gestational age at birth, suggesting that:\n\nStrong confounding may be present\nWe may need to adjust for a large number of covariates\nThere may be areas of covariate space (i.e., combinations of covariate values) for which there’s little or no variability in the exposure and/or outcome\n\nIt would be desirable to have greater balance between cases and controls. One way of achieving this is through matching.\n\nUsually in a GLM, we think of the \\(Y\\) as being random; however, in this setting, the \\(X\\) variables are themselves random.\nDo recall that we worked out a retrospective likelihood in logistic regression showing that it gives the same estimates as the prospective likelihood that we’re used to.\nThis means we can proceed treating \\(Y\\) as random conditional on \\(X\\) and get the same results as if we’d treated \\(X\\) as random."
  },
  {
    "objectID": "week14/week14.html#conditional-logistic-regression",
    "href": "week14/week14.html#conditional-logistic-regression",
    "title": "Week 14",
    "section": "Conditional Logistic Regression",
    "text": "Conditional Logistic Regression\nWe use the conditioning approach to derive a likelihood that depends on the \\(\\beta_1\\) but not the \\(\\{ \\alpha_k \\}\\).\nThe sufficient statistic for \\(\\alpha_k\\) is \\(T_k = \\sum_{i=1}^{N_k} Y_{ki}\\), i.e., the total number of cases in matched set \\(k\\).\nWe will form a conditional likelihood using the conditional distribution of the observed data \\(Y_k = (Y_{k1}, ..., Y_{kN_k})'\\) given \\(T_k\\) for each matched set \\(k\\).\nWe use the conditional likelihood just as we would any other:\n\nSolve score equations to obtain conditional maximum likelihood estimates\nCalculate Fisher information from the second derivatives of the log of this conditional likelihood.\nUse the inverse Fisher information for variance estimation.\n\nLet’s first write out the unconditional prospective likelihood\n\\[\n\\begin{aligned}\n\\mathcal L^* & = \\prod_{k=1}^K \\prod_{i = 1}^{N_k} P(Y_{ki} = y_{ki} \\mid X_{ki}, \\text{ set } k) \\\\\n& = \\prod_{k=1}^K \\prod_{i = 1}^{N_k} [P(Y_{ki} = 1 \\mid X_{ki}, \\text{ set } k)]^{y_{ki}} [P(Y_{ki} = 0 \\mid X_{ki}, \\text{ set } k)]^{1-y_{ki}} \\\\\n& = \\prod_{k=1}^K \\prod_{i = 1}^{N_k}\n\\left[ \\frac{\\exp \\{\\alpha_k + \\beta_1 X_{ki} \\}}{1 + \\exp \\{\\alpha_k + \\beta_1 X_{ki} \\}} \\right]^{y_{ki}} \\left[ \\frac{1}{1 + \\exp \\{\\alpha_k + \\beta_1 X_{ki} \\}} \\right]^{1-y_{ki}} \\\\\n& = \\prod_{k=1}^K \\prod_{i = 1}^{N_k} \\frac{\\exp \\{\\alpha_k + \\beta_1 X_{ki} \\}^{y_{ki}}}{1 + \\exp \\{\\alpha_k + \\beta_1 X_{ki} \\}} \\\\\n& = \\prod_{k=1}^K \\frac{\\exp \\{ \\sum_{i} y_{ki} \\alpha_k + \\beta_1 \\sum_i y_{ki} X_{ki} \\}}{\\prod_i [1 + \\exp \\{\\alpha_k + \\beta_1 X_{ki} \\}]} \\\\\n& = \\prod_{k=1}^K \\frac{\\exp \\{ t_k \\alpha_k + \\beta_1 \\sum_i y_{ki} X_{ki} \\}}{\\prod_i [1 + \\exp \\{\\alpha_k + \\beta_1 X_{ki} \\}]}\n\\end{aligned}\n\\]\nWe’ll call each product term the likelihood of the \\(k\\)th matched set.\nDeriving the conditional likelihood:\n\\(Y_k = (Y_{k1}, ..., Y_{kN_k})'\\) conditional on \\(T_k = \\sum_{i=1}^{N_k} Y_{ki}\\) is\n\\[P(Y_k = y_k \\mid T_k = t_k) = \\frac{P(Y_k = y_k, \\, T_k = t_k)}{P(T_k = t_k)},\\]\nby a simple application of Bayes’ rule.\n(We’re implicitly conditioning on \\(X\\) throughout)\nNumerator:\n\\[\n\\begin{aligned}\nP(Y_k = y_k, \\, T_k = t_k) & = P(Y_k = y_k) I(t_k = \\sum_i y_{ki}) \\\\\n& = \\frac{\\exp \\{ t_k \\alpha_k + \\beta_1 \\sum_i y_{ki} X_{ki} \\}}{\\prod_i [1 + \\exp \\{\\alpha_k + \\beta_1 X_{ki} \\}]} I(t_k = \\sum_i y_{ki}).\n\\end{aligned}\n\\]\nThe denominator:\n\\[\nP(T_k = t_k) = \\sum_{y_k : T_k = t_k} P(Y_k = y_k) = \\sum_{y_k : T_k = t_k} \\frac{\\exp \\{ t_k \\alpha_k + \\beta_1 \\sum_i y_{ki} X_{ki} \\}}{\\prod_i [1 + \\exp \\{\\alpha_k + \\beta_1 X_{ki} \\}]}\n\\]\nThe denominator of the denominator term can pull out, since it doesn’t depend on the sum, and thus it will cancel with the denominator in the sum.\nIt follows that\n\\[\n\\begin{aligned}\nP(Y_k = y_k \\mid T_k = t_k) & = \\frac{I(t_k = \\sum_i y_{ki}) \\times \\exp \\{ t_k \\alpha_k + \\beta_1 \\sum_i y_{ki} X_{ki} \\}}{\\sum_{u_k : T_k = t_k} \\exp \\{ t_k \\alpha_k + \\beta_1 \\sum_i u_{ki} X_{ki} \\}} \\\\\n& = \\frac{I(t_k = \\sum_i y_{ki}) \\times \\exp \\{ \\beta_1 \\sum_i y_{ki} X_{ki} \\}}{\\sum_{u_k : T_k = t_k} \\exp \\{\\beta_1 \\sum_i u_{ki} X_{ki} \\}}.\n\\end{aligned}\n\\]\nBecause we can pull out the \\(t_k \\alpha_k\\) terms from inside the \\(\\exp\\) and then they just cancel.\nTaking the product over the \\(K\\) sets gives the conditional likelihood:\n\\[\\mathcal L_C(\\beta_1) = \\prod_{k=1}^K = \\frac{I(t_k = \\sum_i y_{ki}) \\times \\exp \\{ \\beta_1 \\sum_i y_{ki} X_{ki} \\}}{\\sum_{u_k : T_k = t_k} \\exp \\{\\beta_1 \\sum_i u_{ki} X_{ki} \\}}.\\]\nThis is solely a function of \\(\\beta_1\\). By conditioning on \\(\\{ T_1, ..., T_K\\}\\), the likelihood is no longer a function of nuisance parameters.\nSuppose there is only one case in the matched set and they occupy the first index:\n\n\\(Y_{k1} = 1 \\quad \\forall k\\)\n\\(Y_{ki} = 0 \\quad \\forall i \\in \\{ 2, ..., N_k \\}\\)\n\\(T_k = 1, \\quad \\forall k\\).\n\nThe conditional likelihood simplifies to\n\\[\n\\begin{aligned}\n\\mathcal L_C(\\beta_1) & = \\prod_{k=1}^K \\frac{\\exp\\{ \\beta_1 X_{k1}\\}}{\\sum_i \\exp\\{ \\beta_1 X_{ki}\\}} \\\\\n& = \\prod_{k=1}^K \\frac{\\exp\\{ \\beta_1 X_{k1}\\}}{\\exp\\{ \\beta_1 X_{k1}\\} + ... + \\exp\\{ \\beta_1 X_{kN_k}\\}}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "week14/week14.html#practical-considerations",
    "href": "week14/week14.html#practical-considerations",
    "title": "Week 14",
    "section": "Practical Considerations",
    "text": "Practical Considerations\nMatched sets with no exposure variability do not contribute to the likelihood, as in when all the \\(X_{ki}\\) are the same within the set.\nFor example, in a 1:1 matched study, the contributions by any set for which \\(X_{k1} = X_{k2}\\) is\n\\[\\frac{\\exp\\{\\beta_1 X_{k1}\\}}{\\exp\\{ \\beta_1 X_{k1}\\} + \\exp\\{ \\beta_1 X_{k2}\\}} = \\frac{1}{2},\n\\]\nwhich is independent of \\(\\beta_1\\) and so do not contribute any information.\nAnalyses using conditional logistic regression for matched data thus only use matched sets with discordant exposures.\nA consequence of this is that we cannot use conditional logistic regression to learn about covariates used in the matching process.\nSuppose we conducted a matched case-control study of the association between gestational age and infant mortality which was\n\nMatched on child sex at birth\nWithin each matched set, there is no variation in sex\nNone of the matched sets can contribute to estimation of the effect of sex at birth\n\nA benefit of having no variation within each set is that we do not have to worry about sex as a confounder.\nConsider the interpretation of \\(\\beta_1\\) in the underlying logistic regression:\n\\[\\text{logit}P(Y_{ki} = 1 \\mid X_{ki}) = \\alpha_k + \\beta_1 X_{ki}\\]\nBy holding the matched set constant, sex is implicitly held constant.\nWhen we match on a continuous covariate, we have to be careful about residual confounding. Suppose we matched on mothers’ age in 5-year age bands (≤20, 21-25, 40-45, …). Within any given set there will likely remain some variation in the mothers exact age leading to residual confounding.\nThe remedy is to include mothers’ age as a covariate in the model. However, one should be careful about interpreting the coefficient since one has already controlled for most of the effect of age by controlling for the age-bands.\n\n\nExample: Diabetes and acute MI\nConsider a matched case-control study among Navajo Indians.\n144 cases experienced myocardial infarction (MI) who were matched to 144 controls who were free of heart disease. (This constitutes 1:1 matching based on age and sex.)\nThey recorded whether or not each individual had diabetes:\n\n\n\nDiabetic among Cases\nDiabetic among Controls\nN\n\n\n\n\n0\n0\n82\n\n\n0\n1\n16\n\n\n1\n0\n37\n\n\n1\n1\n9\n\n\n\n\nset <- rep(1:144, rep(2, 144))\n\nmi <- rep(c(1,0), 144)\n\ndiab <- c(rep(c(0,0), 82), rep(c(0,1), 16), rep(c(1,0), 37), rep(c(1,1), 9))\n\nnavajo <- data.frame(set, mi, diab)\n\nhead(navajo)\n\n  set mi diab\n1   1  1    0\n2   1  0    0\n3   2  1    0\n4   2  0    0\n5   3  1    0\n6   3  0    0\n\n\n\nlibrary(survival)\n\nfit0 <- clogit(mi ~ diab + strata(set), data = navajo)\n\nsummary(fit0)\n\nCall:\ncoxph(formula = Surv(rep(1, 288L), mi) ~ diab + strata(set), \n    data = navajo, method = \"exact\")\n\n  n= 288, number of events= 144 \n\n       coef exp(coef) se(coef)     z Pr(>|z|)   \ndiab 0.8383    2.3125   0.2992 2.802  0.00508 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     exp(coef) exp(-coef) lower .95 upper .95\ndiab     2.313     0.4324     1.286     4.157\n\nConcordance= 0.573  (se = 0.035 )\nLikelihood ratio test= 8.55  on 1 df,   p=0.003\nWald test            = 7.85  on 1 df,   p=0.005\nScore (logrank) test = 8.32  on 1 df,   p=0.004\n\n\nWhat do we get if we ignore the matching? Fitting a logistic regression model, we get an odds ratio estimate and 95% CI of 2.23 (1.28, 3.89).\nIgnoring the matching will, in general, bias the estimated odds ratio towards the null. This is most evident when matching on important predictors of the outcome.\n\nIt’s important to remember when interpreting the results that one has to emphasize they’re conditional on the matched covariates."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods (BST 232) Notes",
    "section": "",
    "text": "Welcome to my notes on Methods for Simple, Multiple, and Generalized Linear Models!\n\n\n\n\n\nI hope you will enjoy them, but you do have to be prepared: the notation is pretty all over the place.\nI’ve been honing my \\(\\LaTeX\\) tikz and pgfplots skills throughout this course, so I hope you enjoy the diagrams I have diligently been creating throughout.\nBesides the above, here are a couple more figures I’m so thrilled to have been able to learn how to make in \\(\\LaTeX\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may be asking, why on Earth would you spend your time learning tikz when it’s so convoluted, tortured, arcane, and generally a pain-in-the-ass?\nI have two reasons:\n\nI find it to be nicer for producing mathematical “infographics” (explanatory graphics) than R’s {ggplot2} which I love to death for data visualization.\n[left to the reader as an exercise]"
  },
  {
    "objectID": "week1/week1.html",
    "href": "week1/week1.html",
    "title": "Week 1",
    "section": "",
    "text": "Linear Regression\nWe will generally be intereseted in the relationship between an outcome \\(Y\\) and \\(p\\) covariates or predictors denoted \\((x_1, ..., x_p)\\).\nWe generally say that a statistical model has a systematic component and a random component.\nWe often hypothesize that the “real” relationship between our outcome and predictors might be “super-complex”. Like cancer and environmental exposures might have complex dependencies on genetics, etc.. So sometimes instead of having a systematic component that captures variables in their full complexity, we might prioritize interpretability and we might be satisfied with an imperfect (but more intuitive, simpler) model that might give us some intuition about reality.\nThe random component may provide both a means to explain everything uncaptured by our predictors, as well as “real” randomness. For example, we might theorize that every time cells divide, there’s some small chance due to genetic drift that cells become metastatic and cancerous which is just random (did a mutation happen that was harmful in the right way in the right spot?)."
  },
  {
    "objectID": "week1/week1.html#il-famoso-smoking-ra-fisher",
    "href": "week1/week1.html#il-famoso-smoking-ra-fisher",
    "title": "Week 1",
    "section": "il famoso Smoking RA Fisher",
    "text": "il famoso Smoking RA Fisher\nWe’ll talk about a lot of the methods that Ronald A. Fisher developed. Already in the 1900s it was being observed that there was a strong association between smoking and lung cancer. However, Fisher was a smoker himself and posited that the association between lung cancer and smoking could be explained away by some genetic or biological difference between the smoking and non-smoking population (positing some genes that caused people to desire to smoke).\n\n\n\n\n\nRonald Fisher’s unsupported theory of genetics confounding the smoking-lung cancer relationship\n\n\n\n\nWe’re pretty sure that this was driven not by any substance matter expertise, but rather by Fisher’s love of smoking."
  },
  {
    "objectID": "week1/week1.html#hormone-replacement-therapy",
    "href": "week1/week1.html#hormone-replacement-therapy",
    "title": "Week 1",
    "section": "Hormone Replacement Therapy",
    "text": "Hormone Replacement Therapy\nIn the mid- to late- 20th century there were a ton of studies linking hormone replacement therapy for older women to better cardiovascular outcomes (lack of coronary heart disease).\nHowever, thankfully due to the Heart and Estrogen/Progestin Study (HERS, in the early 90s) we now know that a lot of those studies were not controlling for socioeconomic status. It turns out that socioeconomic status was highly associated with HRT usage, and associated at least in the US with a lot of better health outcomes across the board.\nIt turned out that HRT when applied at certain times for some people can actually be harmful — but the point is the picture is much muddier than was initially thought and recommendations were rolled back. Later randomized studies were performed that produced reliable bodies of evidence demonstrating either no effect or in some cases harmful effects.\nWe’ll use the baseline data from HERS (not so much interested in the HRT treatment effect), but to investigate the research question:\n\nHow is systolic blood pressure related to age, independently of other well-known cardiovascular risk factors? (Age, diabetes, smoking, etc.)"
  },
  {
    "objectID": "week1/week1.html#prediction-studies",
    "href": "week1/week1.html#prediction-studies",
    "title": "Week 1",
    "section": "Prediction Studies",
    "text": "Prediction Studies\nTypically in prediction settings, there’s no single exposure of particular interest; mechanisms and confounding is treated as less of a concern (if at all), and the main challenge is that we need to take care to not overfit the data.\nA major theme of this class will be that different tasks require different analysis strategies and diffrent statistical tools."
  },
  {
    "objectID": "week1/week1.html#quantifying-uncertainty",
    "href": "week1/week1.html#quantifying-uncertainty",
    "title": "Week 1",
    "section": "Quantifying Uncertainty",
    "text": "Quantifying Uncertainty\nTypically standard statistical models have nice theoretical properties because years-and-years ago, we didn’t have much data so people spent their time studying theory instead of data. As a result, we have a lot of nice theories about the uncertainty represented in statistical models.\nAn example of the kind of uncertainty we might be interested in is shown in this figure relating Alzheimer’s disease rates and exposure to PM2.5.\n\n\n\n\n\n\n\n\n\nThis figure is taken from the article Long-term effects of PM2·5 on neurological disorders in the American Medicare population: a longitudinal cohort study by Shi et al, Lancet Planetary Health (2020)."
  },
  {
    "objectID": "week1/week1.html#why-learn-methods-before-study-design",
    "href": "week1/week1.html#why-learn-methods-before-study-design",
    "title": "Week 1",
    "section": "Why Learn Methods Before Study Design",
    "text": "Why Learn Methods Before Study Design\nAn interesting point made is that it’s important to understand the limitations, strengths of methods, what they can and can’t do, and how to use them before designing a study."
  },
  {
    "objectID": "week1/week1.html#recommended-reading",
    "href": "week1/week1.html#recommended-reading",
    "title": "Week 1",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nKutner M, Nachtsheim C, Neter J, Li W. Applied Linear Statistical Model. 5th edition. chapters 1-3\nShmueli, G. (2010). To explain or to predict? Statistical Science. https://www.stat.berkeley.edu/~aldous/157/Papers/shmueli.pdf"
  },
  {
    "objectID": "week1/week1.html#hers-study-sbp-in-post-menopausal-women",
    "href": "week1/week1.html#hers-study-sbp-in-post-menopausal-women",
    "title": "Week 1",
    "section": "HERS Study: SBP in Post-Menopausal Women",
    "text": "HERS Study: SBP in Post-Menopausal Women\nIn a clinical trial of hormone therapy for preventing heart attacks and deaths among 2,763 post-menopausal women with existing coronary heart disease:\n\nthe outcome variable is systolic blood pressure\nthe data collected on covariates included age, diabetes diagnosis, smoking status, etc.\nthe research question was how is systolic blood pressure jointly related to age, statin use, and other risk factors in this cohort.\n\nReference: Vittinghoff et al., Regression methods in Biostatistics 2005 https://regression.ucsf.edu/regression-methods-biostatistics-linear-logistic-survival-and-repeated-measures-models"
  },
  {
    "objectID": "week1/week1.html#multiple-linear-regression-scalar-notation",
    "href": "week1/week1.html#multiple-linear-regression-scalar-notation",
    "title": "Week 1",
    "section": "Multiple Linear Regression: Scalar Notation",
    "text": "Multiple Linear Regression: Scalar Notation\nWe consider the model\n\\[Y_i = \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_p x_{ip} + \\epsilon_i, \\quad i = 1, ..., n.\\]\n\\[\\mathbb E(\\epsilon_i) = 0, \\, \\underbrace{\\text{Var}(\\epsilon_i) = \\sigma^2}_{\\text{homoscedasticity assumption}}, \\, \\text{ and Cov}(\\epsilon_i, \\epsilon_j) = 0.\\]\nTypically we assume that the predictors \\(x_i\\) are fixed and measured without error.\nWe require that \\(p < n\\).\nTJ notes that \\(\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0\\) doesn’t imply independence. This is the assumption we’re making for the time-being, but independence will be implied after we later assume that the errors are normally distributed.\nRoman asks if we need a conditional mean zero assumption — i.e., \\(\\mathbb E(\\epsilon | x) = 0\\)? Rachel notes that \\(\\epsilon\\) and \\(x\\) are assumed to be independent, so \\(\\mathbb E(\\epsilon | x) = \\epsilon\\). We will make stronger assumptions later, but we aren’t introducing those yet.\nBy taking the expected value of both sides, we can equivalently write that\n\\[\\mathbb E(Y_i) = \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_p x_{ip}.\\]\nThe parameters \\(\\beta_j\\) for \\(j = 1, ..., p\\) represent the change in the expected value \\(\\mathbb E(Y_i)\\) per unit change in \\(x_j\\) holding the remaining predictors \\(x_k \\, (k \\neq j)\\) constant.\nOne can see this by observing:\n\\[ \\mathbb E(Y_i | x_{i1} = x^* + 1) = \\beta_0 + \\beta_1 (x^* + 1) + ...\\] \\[ \\mathbb E(Y_i | x_{i1} = x^*) = \\beta_0 + \\beta_1 x^* + ...\\] \\[ \\mathbb E(Y_i | x_{i1} = x^* + 1) - \\mathbb E(Y_i | x_{i1} = x^*) = \\beta_1.\\]\nWe most often interested in testing whether \\(\\beta_j = 0\\), interpreted as a test of whether there’s an association between \\(x_j\\) and \\(Y\\)."
  },
  {
    "objectID": "week1/week1.html#vector-notation",
    "href": "week1/week1.html#vector-notation",
    "title": "Week 1",
    "section": "Vector Notation",
    "text": "Vector Notation\nWe can write this model more succinctly by writing:\n\\[ Y_i = x_i' \\beta + \\epsilon_i, \\quad i = 1, ..., n,\\]\nwhere \\(x_i = (1, x_{i1}, ..., x_{ip})',\\) and \\(\\beta = (\\beta_0, \\beta_1, ..., \\beta_p)'\\), \\(\\mathbb E(\\epsilon_i) = 0\\), \\(\\text{Var}(\\epsilon_i) = \\sigma^2\\), and \\(\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0\\)."
  },
  {
    "objectID": "week1/week1.html#examples",
    "href": "week1/week1.html#examples",
    "title": "Week 1",
    "section": "Examples",
    "text": "Examples\n\nlibrary(here)\n\nhere() starts at /Users/cht180/Documents/2023/BST232 Methods Quarto\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nhers <- readr::read_csv(here(\"data/hers.csv\"))\n\nRows: 2763 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): HT, raceth, nonwhite, smoking, drinkany, exercise, physact, globra...\ndbl (24): age, medcond, weight, BMI, waist, WHR, glucose, weight1, BMI1, wai...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nplt1 <- ggplot(hers %>% dplyr::sample_frac(.1), aes(x = age, y = SBP)) + \n  geom_point() + \n  theme_bw() + \n  ggtitle(\"Scatterplot of a 10% sample of our data\") \nplt1 \n\n\n\nplt2 <- plt1 + \n  stat_summary_bin(bins = 10, breaks = quantile(hers$age, seq(0,1, 0.1)), geom = 'line', fun = mean, color = 'cadetblue', size = 1.5) + \n  stat_summary_bin(bins = 10, breaks = quantile(hers$age, seq(0,1, 0.1)), geom = 'point', shape = 23, fun = mean, color = 'black', size = 3.5, fill = 'white') + \n  ggtitle(\"Now with overlain means for decile groups by age\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nplt2 \n\n\n\nplt3 <- \n  plt2 + \n  geom_smooth(method = 'lm', se = FALSE) + \n  ggtitle(\"Now with a linear model\") \nplt3 \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nlm.sbp.age <- lm(SBP ~ age, data = hers)\njtools::summ(lm.sbp.age)\n\n\n\n\n  \n    Observations \n    2763 \n  \n  \n    Dependent variable \n    SBP \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(1,2761) \n    77.21 \n  \n  \n    R² \n    0.03 \n  \n  \n    Adj. R² \n    0.03 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    103.63 \n    3.60 \n    28.82 \n    0.00 \n  \n  \n    age \n    0.47 \n    0.05 \n    8.79 \n    0.00 \n  \n\n\n Standard errors: OLS\n\n\n\nSo now we would say that \\(\\hat{\\mathbb E}(SBP_i) = 103.63 + 103.63 Age_i\\). Though, the intercept is kind of useless since our model doesn’t have any observations for systolic blood pressure for age 0 infants. We might want to fit an age centered model.\n\nlm.sbp.agec <- lm(SBP ~ I(age - mean(age)), data = hers)\njtools::summ(lm.sbp.agec)\n\n\n\n\n  \n    Observations \n    2763 \n  \n  \n    Dependent variable \n    SBP \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(1,2761) \n    77.21 \n  \n  \n    R² \n    0.03 \n  \n  \n    Adj. R² \n    0.03 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    135.07 \n    0.36 \n    378.24 \n    0.00 \n  \n  \n    I(age - mean(age)) \n    0.47 \n    0.05 \n    8.79 \n    0.00 \n  \n\n\n Standard errors: OLS\n\n\n\nIn which case we’d say that \\(\\hat{\\mathbb E}(SBP_i) = 135.07 + 135.07 Age_i\\)."
  },
  {
    "objectID": "week1/week1.html#multiple-linear-regression",
    "href": "week1/week1.html#multiple-linear-regression",
    "title": "Week 1",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\n# fitting a p=2 model\nlm.sbp.age.weight <- lm(SBP ~ age + weight, data = hers)\nx <- y <- seq(0, 100, length= 30)\nf <- function(x,y){ z <- x*coef(lm.sbp.age.weight)[2] + y*coef(lm.sbp.age.weight)[3] + coef(lm.sbp.age.weight)[1] }\nz <- outer(x,y,f)\npersp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = \"lightblue\", xlab = \"age\", ylab = \"weight\", zlab = \"expected SBP\", ticktype = 'detailed', nticks = 4)"
  },
  {
    "objectID": "week1/week1.html#indicator-dummy-variables",
    "href": "week1/week1.html#indicator-dummy-variables",
    "title": "Week 1",
    "section": "Indicator / Dummy Variables",
    "text": "Indicator / Dummy Variables\nWe might be interested in modeling categorical variables as well. To do so, we would include dummy variables.\nIf a categorical variable has \\(K\\) levels, then we’ll need to compute \\(K-1\\) dummy variables where the omitted variable is called the “reference level”.\n\nunique(hers$physact)\n\n[1] \"much more active\"     \"much less active\"     \"about as active\"     \n[4] \"somewhat less active\" \"somewhat more active\"\n\nhers$physact <- as.factor(hers$physact) \n# the reference category will be \"much more active\" since it's the \n# first factor level — \n# if we wanted to change it, we could run: \n# hers$raceth <- relevel(hers$raceth, \"much less active\") \n# for example. \nlm.sbp.physact <- lm(SBP ~ physact, data = hers)\nhead(model.matrix(lm.sbp.physact))\n\n  (Intercept) physactmuch less active physactmuch more active\n1           1                       0                       1\n2           1                       1                       0\n3           1                       0                       0\n4           1                       1                       0\n5           1                       0                       0\n6           1                       0                       0\n  physactsomewhat less active physactsomewhat more active\n1                           0                           0\n2                           0                           0\n3                           0                           0\n4                           0                           0\n5                           1                           0\n6                           0                           0\n\njtools::summ(lm.sbp.physact)\n\n\n\n\n  \n    Observations \n    2763 \n  \n  \n    Dependent variable \n    SBP \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(4,2758) \n    0.48 \n  \n  \n    R² \n    0.00 \n  \n  \n    Adj. R² \n    -0.00 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    135.00 \n    0.63 \n    215.01 \n    0.00 \n  \n  \n    physactmuch less active \n    0.07 \n    1.49 \n    0.05 \n    0.96 \n  \n  \n    physactmuch more active \n    -0.90 \n    1.26 \n    -0.72 \n    0.47 \n  \n  \n    physactsomewhat less active \n    0.96 \n    1.06 \n    0.91 \n    0.36 \n  \n  \n    physactsomewhat more active \n    -0.05 \n    0.91 \n    -0.05 \n    0.96 \n  \n\n\n Standard errors: OLS\n\n\n\nIn machine learning this is called “one-hot” encoding."
  },
  {
    "objectID": "week1/week1.html#polynomial-regressions",
    "href": "week1/week1.html#polynomial-regressions",
    "title": "Week 1",
    "section": "Polynomial Regressions",
    "text": "Polynomial Regressions\n\nlm.sbp.agepolynomial <- lm(SBP ~ poly(age), data = hers)\n\n# we could plug in coef(lm.sbp.agepolynomial)[1] through \n# coef(lm.sbp.agepolynomial)[4] into the following equation, \n# but it's kind of boring — instead here's a more fun \n# looking cubic polynomial we could imagine being the result of\n# a polynomial regression:\ncurve(1 - 2.8*x + 4*(x^2) - .65*(x^3), from = 0, to = 5,\n      xlab = expression(x[1]),\n      ylab = \"E(Y)\")\n\n\n\n\nIt’s worth emphasizing that usually we prefer splines or generalized additive models to polynomial regression these days since polynomial regression can act strangely."
  },
  {
    "objectID": "week1/week1.html#marginal-associations",
    "href": "week1/week1.html#marginal-associations",
    "title": "Week 1",
    "section": "Marginal Associations",
    "text": "Marginal Associations\nIf we have one binary predictor \\(x_1\\) and one continuous \\(x_2\\), we might be interested in the marginal association between \\(Y\\) and \\(x_2\\). This would look like a model fit with lm(Y ~ x2) and \\(x_1\\) is not included."
  },
  {
    "objectID": "week1/week1.html#conditional-association",
    "href": "week1/week1.html#conditional-association",
    "title": "Week 1",
    "section": "Conditional Association",
    "text": "Conditional Association\nWe might also want to fit models that include \\(x_1\\), so those could be fit with lm(Y ~ x1 + x2) and this would include an effect for \\(x_2\\) (one effect, not multiple) that is applied while also considering an effect for \\(x1\\)."
  },
  {
    "objectID": "week3/week3.html",
    "href": "week3/week3.html",
    "title": "Week 3",
    "section": "",
    "text": "Lab\nOne perspective is that statistics is really only good for two things:"
  },
  {
    "objectID": "week3/week3.html#variance-of-hat-beta",
    "href": "week3/week3.html#variance-of-hat-beta",
    "title": "Week 3",
    "section": "Variance of \\(\\hat \\beta\\)",
    "text": "Variance of \\(\\hat \\beta\\)\nThe variance of \\(\\hat \\beta\\) is expressed as the variance-covariance matrix\n\\[\\text{Var}(\\hat \\beta) = (X'X)^{-1} X' \\text{Var}(Y) X (X'X)^{-1}\\] \\[ = \\sigma^2 (X'X)^{-1}\\]\nIf we let \\(D = (X'X)^{-1}\\), the variance of \\(\\hat \\beta_j = \\sigma D_{jj}\\) and the covariance between \\(\\hat \\beta_i\\) and \\(\\hat \\beta_j\\) is \\(\\sigma^2 D_{ij}\\).\nHow would we get to this result?\nWe are using a shorthand where we denote \\((X'X)^TX' = A\\), and now we’re just looking at the \\(\\text{Var}(AY)\\). When we are working with the matrix-variance formula, we can rewrite \\(\\text{Var}(AY) = A\\text{Var}(Y)A'\\).\nPlugging in the formula for \\(A\\), we get to the above.\nRemember we said that \\(\\text{Var}(\\epsilon) = \\sigma^2\\) and \\(Y = X\\beta + \\epsilon\\) where the only randomness comes from \\(\\epsilon\\). In other words\n\\[\\text{Var}(\\hat \\beta) = (X'X)^{-1} X' \\text{Var}(Y) X (X'X)^{-1}\\] \\[ = (X'X)^{-1} X' \\text{Var}(X\\beta + \\epsilon) X (X'X)^{-1}\\] \\[ = (X'X)^{-1} X' \\text{Var}(\\epsilon) X (X'X)^{-1}\\] \\[ = (X'X)^{-1} X' \\sigma^2 I X (X'X)^{-1}\\] \\[ = \\sigma^2 (X'X)^{-1} X' I X (X'X)^{-1}\\] \\[ = \\sigma^2 \\cancel{(X'X)^{-1} X' X }\\underbrace{(X'X)^{-1}}_{\\stackrel{set}{=}D}.\\]\n\nGauss-Markov Theorem\n\nUnder the standard linear assumptions, \\(\\hat \\beta_{OLS}\\) is the best linear unbiased estimator (BLUE) for \\(\\beta\\).\nLinear unbiased estimator: \\(\\hat \\beta_{OLS}\\) is a linear combination of the observed \\(y\\) values (given that \\(\\hat \\beta = (X'X)^{-1}X'y\\) is a matrix of constants times a vector \\(y\\)) and is an unbiased estimator.\nIt’s “best” in the sense that it is the lowest variance (most precise).\n\nSo the Gauss Markov Theorem tells us that among all linear, unbiased estimators of \\(\\beta\\), \\(\\hat \\beta_{OLS}\\) has the lowest variance.\n\n\nSimple Linear Regression as a Special Case\nThe least squares estimators \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) can be expressed as\n\\[\\hat \\beta_0 = \\sum_{i=1}^n l_i y_i, \\quad \\hat \\beta_1 = \\sum_{i=1}^n k_i y_i,\\]\nwhere \\(l_i = \\frac{1}{n} - \\frac{\\bar x(x_i - \\bar x)}{\\sum_{i=1}^n (x_i - \\hat x)^2},\\) and \\(k_i = \\frac{(x_i - \\bar x)}{\\sum_{i=1}^n (x_i - \\bar x)^2}\\).\n\nVariance of LS Estimators\n\\[\\text{Var}(\\hat \\beta_0) = \\sigma \\left\\{ \\frac{1}{n} + \\frac{\\bar x^2}{\\sum_{i=1}^n (x_i-\\bar x)^2}\\right\\},\\] \\[\\text{Var}(\\hat \\beta_1) = \\sigma \\left\\{\\frac{1}{\\sum_{i=1}^n (x_i-\\bar x)^2}\\right\\},\\] \\[\\text{Cov}(\\hat \\beta_0, \\hat \\beta_1) = \\sigma \\left\\{ - \\frac{\\bar x}{\\sum_{i=1}^n (x_i-\\bar x)^2}\\right\\}.\\]\nThe variance-covariance matrix is\n\\[\\text{Var}(\\hat \\beta) = \\sigma(X'X)^{-1} = \\left[ \\begin{array}{cc} \\text{Var}(\\hat \\beta_0) & \\text{Cov}(\\hat \\beta_0, \\hat \\beta_1) \\\\ \\text{Cov}(\\hat \\beta_0, \\hat \\beta_1) & \\text{Var}(\\hat \\beta_1) \\end{array} \\right].\\]\n\n\nEstimation of \\(\\sigma^2\\)\nIn order to estimate \\(\\text{Var}(\\hat \\beta)\\), we need an estimator of \\(\\sigma^2\\):\nWe base this on the sum of squared errors (SSE):\n\\[SSE = (y - X\\hat\\beta)'(y-X\\hat \\beta)\\] \\[ = \\sum_{i=1}^n(y_i-x'_i\\hat\\beta)^2\\] \\[ = \\sum_{i=1}^n (\\hat \\epsilon_i)^2\\]\n\\[\\hat \\sigma^2 = MSE = \\frac{SSE}{n - p - 1}.\\]\nThis estimator \\(\\hat \\sigma^2\\) is an unbiased estimator.\nThe \\(n-p-1\\) in the denominator is because we estimate \\(p+1\\) parameters and we divide by the degrees of freedom, which is \\(n - \\text{\\# things we had to estimate}\\). The Kutner book has a more rigorous presentation of why this is the right amount to divide by.\n\n\n\nNormality assumption\nIf we are willing to make the stronger assumption that \\(\\epsilon_i \\stackrel{iid}{\\sim} \\mathcal N(0, \\sigma^2)\\), then we can perform inference on \\(\\beta\\).\nFirst note that \\(\\epsilon_i \\stackrel{iid}{\\sim} \\mathcal N(0, \\sigma^2) \\Longrightarrow Y_i \\stackrel{ind}{\\sim} \\mathcal N(x_i'\\beta, \\sigma^2)\\), such that\n\\[f_Y(y_i|\\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left[ -\\frac{1}{2\\sigma^2} (y_i - x_i'\\beta)^2\\right]\\]\nNotice that the \\(Y_i\\) values are independent but not identically distributed.\nBefore we were just assuming that the \\(\\epsilon\\) values were uncorrelated, which in the special case of the normal distribution implies independence, but this isn’t necessarily so for other distributions.\nWe can then use maximum likelihood techniques to obtain \\[\\hat \\beta_{MLE} \\sim MVN_{p+1} \\left[ \\beta, \\sigma^2 (X'X)^{-1} \\right].\\]\n\n\nJoint Density\nRecap of maximum likelihood estimation.\nIn general, suppose we have data \\(Y_1, ..., Y_n\\), which are independent random variables with \\(Y_i\\) having probability density function \\[f_Y(y_i|\\theta)\\] where \\(\\theta\\) is a vector of unknown parameters.\nThen the joint density function of all the \\(y_i\\) given \\(\\theta\\) is the product of the individual densities\n\\[f(y_1, ..., y_n|\\theta) = \\prod_{i=1}^n f_Y(y_i|\\theta).\\]\n\nLikelihood Functions\nThe likelihood function of \\(\\theta\\) given the data has the same form as the joint pdf:\n\\[\\mathcal L(\\theta|y_1,...,y_n) = f(y_1, ..., y_n|\\theta) = \\prod_{i=1}^n f_Y(y_i|\\theta).\\]\nOf course this looks exactly the same as the joint density of the \\(Y_i\\) values, but instead this is a function of \\(\\theta\\) instead of a function of the \\(y_i\\) values.\nOnce you take a random sample of size \\(n\\), the \\(y_i\\) values are known, and the likelihood is considered as a function of unknown parameter \\(\\theta\\).\nThe likelihood function should still integrate to 1.\nThe MLE of \\(\\theta\\) is the value \\(\\hat \\theta\\) that maximizes the likelihood\n\\[\\mathcal L(\\theta | y_1, ..., y_n)\\]\nas a function of \\(\\theta\\).\nThe value \\(\\hat \\theta\\) that maximizes \\(\\mathcal L(\\theta)\\) also maximizes\n\\[\\ell(\\theta | y_1, ..., y_n) = \\log \\mathcal L(\\theta | y_1, ..., y_n).\\]\n\n\nSolving for MLE\n\\[ \\frac{\\partial \\ell}{\\partial \\theta} \\stackrel{set}{=} 0,\\]\nand technically we’re going to need to check that this is a maximum as opposed to a minimum, and we’ll do so by checking that\n\\[\\left[ \\frac{\\partial^2 \\ell}{\\partial \\theta^2} \\right]_{\\theta = \\hat \\theta} < 0.\\]\nIf we were in a matrix setting instead of a vector setting, we’d need to check that the matrix is negative definite for a maximum, or positive definite for a minimum.\nThe negative of the second derivative,\n\\[\\frac{-\\partial^2 \\ell(\\theta | y_1, ..., y_n)}{\\partial \\theta^2},\\]\nis called the information.\n\n\nReturning to MLE for Regression\nThus in the linear regression setting if we assume \\(\\epsilon_i \\stackrel{iid}{\\sim} \\mathcal N(0, \\sigma^2),\\) then \\(Y_i \\stackrel{ind}{\\sim} \\mathcal N(x'_i\\beta, \\sigma^2)\\) and \\[f_Y(y_i|\\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left[ -\\frac{1}{2\\sigma^2} (y_i - x_i'\\beta)^2\\right]\\]\n\\[\\mathcal L(\\beta, \\sigma^2 | y_1, ..., y_n) = \\prod_{i=1}^n f_Y(y_i|\\beta, \\sigma^2),\\]\nand\n\\[\\mathcal L(\\beta, \\sigma^2|y_1, ..., y_n) = \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n\n\\exp \\left[ - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i'\\beta)^2 \\right]\\] \\[= \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n\n\\exp \\left[ - \\frac{1}{2\\sigma^2} (y - X\\beta)'(y-X\\beta) \\right].\\]\nTurning to the log-likelihood function:\n\\[\\ell(\\beta, \\sigma^2 | y_1, ..., y_n) \\propto \\cancel{-n/2\\log(\\sigma^2)} \\underbrace{- \\frac{1}{2\\sigma^2} (y-X\\beta)'(y-X\\beta)}_{= \\frac{-1}{2\\sigma^2} S(\\beta)}.\\]\nThe values that maximize this log-likelihood with respect to \\(\\beta\\), call them \\(\\hat \\beta_{MLE}\\) are the same as those that minimize \\(S(\\hat \\beta)\\), i.e.,\n\\[\\hat \\beta_{MLE} = (X'X)^{-1}X'y\\]\nand it’s straightforward to show that\n\\[\\hat \\beta_{MLE} \\sim MVN_{p+1}\\left[ \\beta, \\sigma^2(X'X)^{-1} \\right].\\]\n\n\n\\(\\sigma^2_{MLE}\\)\nWhile the estimates for \\(\\hat \\beta\\) are the same for OLS vs. MLE, we have that \\[\\hat \\sigma^2_{MLE} = \\frac{1}{n}(y-X\\hat\\beta)'(y-X\\hat\\beta) = \\frac{(n-p-1)}{n}MSE\\]\nSo of note, the MLE for \\(\\beta\\) are the same as the least squares estimator. However the MLE for \\(\\sigma^2\\) is not.\nRecall that the least squares estimator of \\(\\sigma^2\\) is unbiased. The MLE of \\(\\sigma^2\\) is biased, although it is consistent: \\[\\lim_{n\\to\\infty} P(|\\hat\\sigma^2 - \\sigma^2| \\leq \\epsilon) \\to 1, \\, \\forall \\epsilon > 0.\\]"
  },
  {
    "objectID": "week3/week3.html#inference-in-linear-regression",
    "href": "week3/week3.html#inference-in-linear-regression",
    "title": "Week 3",
    "section": "Inference in Linear Regression",
    "text": "Inference in Linear Regression\nOften it’s of interest to determine if, collectively, a group of predictors significantly contribute to the variability in \\(y\\) given another group of predictors are in the model.\nCommon examples are:\n\nIs a categorical variable, represented by dummy variables, significant (analagous to the overall ANOVA F-test)?\nCan the effect of a predictor be represented as a linear effect or is a higher-level polynomial (i.e., using \\(x^2\\), \\(x^3\\), etc.) necessary?\nIs a model that contains only main effects adequate or do we need to incorporate a set of interactions between variables in the models?\n\n\nSum of squares decomposition\n\\[(y_i - \\bar y)^2 = ((y_i - \\hat y_i) + (\\hat y_i - \\bar y))^2\\]\nThen, when computing the sums of squares, we get\n\\[\\sum(y_i - \\bar y)^2 = \\sum_{i=1}^n (\\hat y_i - \\bar y)^2 + \\sum_{i=1}^n (y_i - \\hat y_i)^2,\\]\nwhich happily features a “freshman’s dream”.\nWe thus have that \\[SST = \\underbrace{SSR}_{\\text{explained by regression}} + \\underbrace{SSE}_{\\text{left over}},\\]\nwhere \\(SST = \\text{Sums of Squares Total}\\), \\(SSR = \\text{Sums of Squares Regression}\\), and \\(SSE = \\text{Sums of Squares Error}\\).\n\n\n\n\n\n\n\n\n\n\nThe ANOVA-like table\nWe often will write something like this type of table:\n\n\n\n\n\n\n\n\n\n\nSource\n\\(SS\\)\n\\(\\text{df}\\)\n\\(\\text{MS}\\)\n\\(\\mathbb E[\\text{MS}]\\)\n\n\n\n\nRegression\n\\(SSR = \\hat \\beta'X'y-n\\bar y^2\\)\n\\(p\\)\n\\(\\frac{SSR}{p}\\)\n\\(\\sigma^2 + \\frac{\\beta'_Rx'_Cx_C\\beta_R}{p}\\)\n\n\nError\n\\(SSE = y'y - \\hat \\beta' X'y\\)\n\\(n-(p+1)\\)\n\\(\\frac{SSE}{n-(p+1)}\\)\n\\(\\sigma^2\\)\n\n\nTotal\n\\(SST=y'y - n \\bar y^2\\)\n\\(n-1\\)\n\n\n\n\n\nwhere \\(MS = \\text{Mean Square Error}\\), and \\[X_c = \\left( \\begin{array}{cccc}\nx_{11}-\\bar{x_1} & x_{12}- \\bar{x_2} & \\cdots & x_{1p}-\\bar{x_p} \\\\\nx_{21}-\\bar{x_1} & x_{22}- \\bar{x_2} & \\cdots & x_{2p}-\\bar{x_p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1}-\\bar{x_1} & x_{n2}- \\bar{x_2} & \\cdots & x_{np}-\\bar{x_p}\n\\end{array}\\right)\\]\n\n\nTesting for Groups of Predictors\nHow do we use this decomposition to test for a group of coefficients?\nThe hypothesis can be formulated as\n\\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_q = 0, q \\leq p\\] \\[H_1: \\text{ at least one of } \\beta_1, ..., \\beta_q \\neq 0.\\]\nAs an aside, tests of the overall regression and tests for a single variable fall within this framework as well:\nThe overall test:\n\\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_p = 0\\] \\[H_1: \\beta_j \\neq 0 \\text{ for at least one } j, j = 1,...,p\\]\nFor a single predictor:\n\\[H_0: \\beta_j = 0\\] \\[H_1: \\beta_j \\neq 0\\]\nWe like the property that testing for significance among a “group of coefficients” reduces in two special cases to either the overall test or a test for an individual coefficient.\nIf we consider the model in matrix form:\n\\[Y = X\\beta + \\epsilon,\\]\nto construct a test based on sums of squares, partition \\(\\beta\\) accordingly:\n\\[\\beta = (\\beta_1^, \\beta_2')',\\]\nwhere \\(\\beta_1\\) is a \\(q \\times 1\\) and \\(\\beta_2\\) is \\((p+1-q) \\times 1\\). We want to test the null hypothesis \\[H_0: \\beta_1 = 0\\] \\[H_1: \\beta_1 \\neq 1\\] and \\(\\beta_2\\) is left unspecified.\nDefining \\(X = \\left[ X_1, X_2 \\right]\\), rewrite the model as\n\\[Y = X_1 \\beta_1 + X_2 + \\beta_2 + \\epsilon.\\]\nNow our model is partitioned so we’re ready to test for significance among the predictors and \\(\\beta\\) coefficients of interest.\nThe full model has SSR expressed as\n\\[SSR(X) = \\hat \\beta' X' y - n \\bar y^2\\]\nand Mean Square Error\n\\[MSE(X) = \\frac{y'y - \\hat \\beta' X' y}{n-p-1}.\\]\nTo find the contribution of \\(X_1\\), fit the model assuming \\(H_0\\) is true. The reduced model is \\[Y = X_2 \\beta_2 + \\epsilon,\\] which yields \\[\\hat \\beta_2 = (X_2'X_2)^{-1}X_2'y \\quad \\text{ and } \\quad SSR(X_2) = \\hat \\beta_2' X_2' y - n' \\bar y^2.\\]\nThe regression sums of squares due to \\(X_1\\) given \\(X_2\\) is in the model is\n\\[SSR(X_1|X_2) = SSR(X) - SSR(X_2)\\]\nwith \\(q\\) degrees of freedom. This is known as the extra sum of squares due to \\(X_1\\) given \\(X_2\\).\nUnder the null hypothesis,\n\\(SSR(X_1|X_2)/\\sigma^2 \\sim \\chi_q^2\\) and \\(SSE/\\sigma^2 \\sim \\chi_{(n-p-1)}^2\\), and these quantities are independent.\nIn general, if one \\(\\chi^2\\) distribution has degrees of freedom \\(d_1\\) and another has \\(d_2\\), then \\((\\chi_{d_1}^2/d_2)/(\\chi_{d_2}^2/d_2) \\sim F_{d_1,d_2}\\) if the two are independent.\nSo we can test \\(H_0: \\beta_1 = 0\\) with the statistic \\[F = \\frac{SSR(X_1|X_2)/q}{MSE(X)} \\stackrel{H_0}{\\sim} F_{q,n-p-1}\\]\nThis \\(F\\) distributional result requires either\n\nnormality of errors \\(\\epsilon_i \\sim \\mathcal N(0,\\sigma^2)\\)\nlarge sample theory\n\nThere’s a handful of things above that we just have to take for granted assumed from a probability & inference class and don’t have time to re-prove here.\nThe \\(F\\) written above is an \\(F\\)-statistic (or \\(F\\)-distributed) because it is the quotient of two \\(\\chi^2\\)-distributed variables divided by their degrees of freedom.\nWith reasonably large sample size, \\(\\mathbb E[F_{q,n-p-1}] \\approx 1\\).\nFor example, one can see that if one runs the regressions:\n\nlm(data = mtcars, hp ~ rnorm(n = nrow(mtcars)))\nsummary(.Last.value)\n#> ... \n#> F-statistic: 0.06081 on 1 and 30 DF\n\nlm(data = mtcars, hp ~ mpg)\n#> ... \n#> F-statistic: 45.46 on 1 and 30 DF\n\nWe can think of this procedure as asking: Is the increase in the regression sums of squares associated with adding \\(q\\) additional predictors, given the presence of the remaining variables in the model, sufficient to warrant removing \\(q\\) additional degrees of freedom from the denominator of MSE?\nAdding an unimportant predictor may increase the MSE, which will increase the uncertainty in the regression coefficient estimates and the variance of \\(\\hat y\\) - so we should include only predictors that explain the response.\nNote however that for the purpose of explanation confounders may not reach significance at given level (e.g. \\(\\alpha = 0.05\\)) but still have a clinically relevant effect on both outcome and exposure and therefore affect the regression coefficients of interest.\n\n\nExample: Test for 2 BMI Terms, HERS Data\n\nlibrary(gt)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nhers <- readr::read_csv(here::here(\"data/hers.csv\"))\n\nRows: 2763 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): HT, raceth, nonwhite, smoking, drinkany, exercise, physact, globra...\ndbl (24): age, medcond, weight, BMI, waist, WHR, glucose, weight1, BMI1, wai...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhers$BMIc <- hers$BMI - mean(hers$BMI, na.rm=TRUE)\n\nlm.ldl.interact <- \n  lm(data = hers %>% filter(! is.na(BMIc)), LDL ~ BMIc*statins + age + nonwhite + drinkany + smoking)\n\nlm.ldl.noBMI <- \n  lm(data = hers %>% filter(! is.na(BMIc)), LDL ~ statins + age + nonwhite + drinkany + smoking)\n\n# perform f-test using anova(reducedModel, fullModel)\nbmi.test <- broom::tidy(anova(lm.ldl.noBMI, lm.ldl.interact))\n\n## format and print results table \ngt(bmi.test) %>% \n  tab_header(title = md(\"**Test of significance of BMI**\"),\n             subtitle = md(\"From LDL model with BMI * statin interaction\")) %>% \n  cols_width(term ~ px(375)) %>% sub_missing(missing_text = '') %>% \n  fmt_number(columns=c('statistic','p.value'),decimals=3) %>% \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Test of significance of BMI\n    \n    \n      From LDL model with BMI * statin interaction\n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LDL ~ statins + age + nonwhite + drinkany + smoking\n2739\n3725955\n\n\n\n\n    LDL ~ BMIc * statins + age + nonwhite + drinkany + smoking\n2737\n3707501\n2\n18454.31\n6.812\n0.001\n  \n  \n  \n\n\n\n\n\n\n\nOverall Test\nUnder the null hypothesis, \\(SSR/\\sigma^2 \\sim \\chi^2_p\\) and \\(SSE/\\sigma^2 \\sim \\chi^2_{n-(p+1)}\\) are independent.\nTherefore we have \\[F = \\frac{SSR/p}{SSE/[n-(p+1)]} = \\frac{MSR}{MSE} \\stackrel{H_0}{\\sim} F_{p,n-p-1}\\]\nWe note that this is reported automatically in a lm().\n\noverall.test <- broom::tidy(anova(lm(data = hers, LDL ~ BMI + age)))\n\ngt(overall.test) %>% \n  tab_header(title = md(\"**Overall test**\"),\n             subtitle = md(\"Model of LDL with BMI and age\")) %>% \n  sub_missing(missing_text = '') %>% \n  fmt_number(columns = c('statistic', 'p.value'), decimals = 3) %>% \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n      Overall test\n    \n    \n      Model of LDL with BMI and age\n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    BMI\n1\n14446.022\n14446.022\n10.155\n0.001\n    age\n1\n7567.195\n7567.195\n5.320\n0.021\n    Residuals\n2744\n3903361.455\n1422.508\n\n\n  \n  \n  \n\n\n\n\nWe can interpret the entries above in the sumsq column as \\(SSR(BMI)\\) and then \\(SSR(age|BMI)\\). These are called the “extra sums of squares” contributed by each variable, and sometimes called the “type 1 sums of squares” (no relation to Type 1 error, but more of a historical idiosyncrasy as a result of how old software [either SAS or S or S-plus] printed these out).\n\\[F = \\frac{(14446 + 7567)/2}{3903361/2744} = 7.74\\]\nUnder \\(H_0\\), \\(F \\sim F_{2, 2744}\\), yielding \\(p = 0.0004458\\).\nWe reject the null hypothesis at \\(\\alpha = 0.05\\) and conclude that at least one of \\(\\beta_1\\) or \\(\\beta_2\\) is not equal to zero.\nOne should note that the above table is one of the places in which order matters because each \\(SSR\\) is conditional on the inclusion of the previously listed variables."
  },
  {
    "objectID": "week3/week3.html#wald-tests",
    "href": "week3/week3.html#wald-tests",
    "title": "Week 3",
    "section": "Wald Tests",
    "text": "Wald Tests\nFor testing individual coefficients \\((H_0: \\beta_j = 0\\) vs \\(H_1: \\beta_j \\neq 0\\)) we can also use the conventional Wald test. To construct the test statistic, consider that\n\\[\\hat \\beta_j \\sim \\mathcal N(\\beta_j, \\sigma^2) D_{jj} \\quad \\text{ and } \\quad\n\\frac{\\hat{\\text{Var}} (\\hat \\beta_j)}{\\sigma^2 D_{jj}} \\sim \\frac{\\chi^2_{n-p-1}}{(n-p-1)}.\\]\nNote that if \\(Z \\sim \\mathcal N(0,1)\\) and \\(S \\sim \\chi^2_d\\) and \\(Z \\perp\\!\\!\\!\\perp S\\) then \\(\\frac{Z}{\\sqrt{S/d}} \\sim t_d\\).\n\\[\\left( \\frac{\\hat \\beta_j - \\beta_j}{\\sqrt{\\sigma^2 D_{jj}}} \\right) \\biggr /\n\\left( \\sqrt{\\frac{\\widehat{\\text{Var}}(\\hat \\beta_j)}{\\sigma^2D_{jj}}} \\right) = \\underbrace{\\boxed{\\frac{\\hat \\beta_j - \\beta_j}{\\sqrt{\\widehat{\\text{Var}}(\\hat \\beta_j)}}}}_{\\text{this should look like a t-statistic}} \\stackrel{H_0}{\\sim} t_{n-p-1}\\]\nIt should be noted that a \\(t^2\\) value where \\(t\\) is a \\(t\\)-statistic follows an \\(F\\)-distribution. This implies that in the case of testing a single coefficient, the \\(t\\)-test and the \\(F\\)-test give the exact same results.\nIn the summary() function, the \\(p\\)-values shown will be from \\(t\\)-tests for each \\(\\beta_j\\), while the \\(F\\)-statistic shown is for the overall model.\n\\[E(LDL_i) = \\beta_0 + \\beta_1 BMI_i + \\beta_2 Age_i\\]\n\nwald.test <- broom::tidy(lm(data = hers, LDL ~ BMI + age))\ngt(wald.test) |> \n  tab_header(title = \n               md(\"**Individual coefficient Wald test**\"),\n             subtitle = \"Test of BMI in model of LDL with age already included\") |> \n  fmt_number(decimals = 3) |> \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n      Individual coefficient Wald test\n    \n    \n      Test of BMI in model of LDL with age already included\n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n151.443\n8.774\n17.260\n0.000\n    BMI\n0.367\n0.132\n2.778\n0.006\n    age\n−0.253\n0.110\n−2.306\n0.021\n  \n  \n  \n\n\n\n\n\\(T = 0.366/0.132 = 2.78 \\quad p = 0.0006\\)\nThis Wald testing strategy extends to testing groups of cofficients\n\\[Y = X_1 \\beta_1 + X_2 \\beta_2 + \\epsilon\\]\nwhere \\(\\beta_1\\) is \\(q \\times 1\\) and \\(\\beta_2\\) is \\((p + 1 - q) \\times 1\\).\n\\[H_0: \\beta_1 = 0\\] \\[H_1: \\beta_1 \\neq 0\\]\nThe multivariate Wald test statistic is \\[W = \\hat \\beta_1' \\left[ \\widehat{\\text{Var}}(\\hat \\beta_1 ) \\right]^{-1}  \\hat \\beta_1\\]\nUnder the null,\n\n\\((1/q)W \\sim F_{q,n-p-1}\\)\nAsymptotically, \\(W \\sim \\chi_q^2\\)\n\n\nwald.test.group <- broom::tidy(lm.ldl.interact)\n\ngt(wald.test.group) |> \n  tab_header(title = \n               md(\"**LDL model with BMI * statin interaction**\")) |> \n  fmt_number(decimals = 3) |> \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n      LDL model with BMI * statin interaction\n    \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n162.405\n7.583\n21.416\n0.000\n    BMIc\n0.582\n0.160\n3.636\n0.000\n    statinsyes\n−16.253\n1.469\n−11.066\n0.000\n    age\n−0.173\n0.111\n−1.563\n0.118\n    nonwhiteyes\n4.073\n2.275\n1.790\n0.074\n    drinkanyyes\n−2.075\n1.467\n−1.415\n0.157\n    smokingyes\n3.110\n2.167\n1.435\n0.151\n    BMIc:statinsyes\n−0.702\n0.269\n−2.606\n0.009\n  \n  \n  \n\n\n\n\nIn this scenario, \\(H_0: \\beta_2 - \\beta_8 = 0\\).\n\n## Generic function for a Wald test from the output of lm()\n\nwaldTest <- function(fit, vec, digits=c(2,4)) {\n  \n  beta     <- coef(fit)[vec]\n  varMat   <- summary(fit)$cov.unscaled[vec,vec] * (summary(fit)$sigma^2)\n  testStat <- t(beta) %*% solve(varMat) %*% beta \n  pVal     <- 1 - pchisq(testStat, length(vec))\n  value    <- c(Fstat = round(testStat, digits=digits[1]),\n             p = round(pVal, digits=digits[2]))\n  return(value)\n}\n\nwaldTest(lm.ldl.interact, vec = c(2,8))\n\n  Fstat       p \n13.6200  0.0011"
  },
  {
    "objectID": "week3/week3.html#testing-general-linear-hypotheses",
    "href": "week3/week3.html#testing-general-linear-hypotheses",
    "title": "Week 3",
    "section": "Testing general linear hypotheses",
    "text": "Testing general linear hypotheses\nSuppose we are interested in testing linear combinations of the regression coefficients. For example, we may be interested in testing\n\\[H_0: \\beta_i = \\beta_j\\]\nequivalently \\(H_0: \\beta_i - \\beta_j = 0\\).\nSuch hypotheses can be expressed as \\(H_0: C \\beta = 0\\).\nWhere \\(C\\) is an \\(r \\times (p+1)\\) matrix of linearly independent contrasts with \\(r\\) the number of restrictions imposed by the null.\nFor example, consider the model \\[Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i3} + \\epsilon_i,\\]\nand testing the hypothesis \\[H_i : \\beta_1 = 0, \\beta_2 = \\beta_3\\]\nThis could also be written as \\[\\left( \\begin{array}{c} \\beta_1 \\\\ \\beta_2 - \\beta_3 \\end{array} \\right) =\n\\left( \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right)\\]\nThis null hypothesis is equivalent to \\[H_0: \\left( \\begin{array}{cccc} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & -1 \\end{array} \\right)\\beta = 0\\] were \\(\\beta = (\\beta_0, \\beta_1, \\beta_2, \\beta_3)'\\).\nWe can obtain the reduced model by solving \\(C\\beta\\) for \\(r\\) of the regression coefficients in terms of the remaining \\(p+1-r\\) regression coefficients. Substituting these values into the full model will yield a reduced model under the null hypothesis,\n\\[Y = Z \\gamma + \\epsilon,\\]\nwhere \\(\\dim(Z) = n \\times (p+1 -r)\\) matrix and \\(\\dim(\\gamma) = (p + 1 - r) \\times 1\\) vector of regression coefficients.\nThe residual SS for this reduced model is \\[SSE(RM) = y'y - \\hat\\gamma' Z'y \\quad \\quad (n - p - 1 + r \\, \\text{ degrees of freedom})\\]\n\\(SSR(\\text{Full Model}) - SSR(\\text{Reduced Model})\\) is called the sum of squares due to the hypothesis \\(C\\beta=0\\).\nWe can test this hypothesis using \\[F = \\frac{(SSR(FM)-SSR(RM))/r}{MSE} \\stackrel{H_0}{\\sim} F_{r,n-p-1}.\\]\n\nExample with HERS data\nConsider using the physical activity score (1-5):\n\n\n\nPhysact\nActivity\n\n\n\n\n1\nMuch less active\n\n\n2\nSomewhat less active\n\n\n3\nAbout as active\n\n\n4\nSomewhat more active\n\n\n5\nMuch more active\n\n\n\nAn ANOVA model for glucose level regressed on physical activity is\n\\[E(glucose_i) = \\beta_0 + \\beta_1D_{i1} + \\beta_2D_{i2} + \\beta_3D_{i3} + \\beta_4 D_{i4}\\]\nQuestion: For the purposes of predicting glucose level, is the cruder physical activity categorization below adequate?\n\n\n\nCollapsed Physact\nActivity\n\n\n\n\n1\nLess active\n\n\n2\nAbout as active\n\n\n3\nMore active\n\n\n\nRecall the full model is\n\\[E(glucose_i) = \\beta_0 + \\beta_1D_{i1} + \\beta_2D_{i2} + \\beta_3D_{i3} + \\beta_4 D_{i4}\\]\nand this question corresponds to the null hypothesis .\nRemember that we can also write \\(H_0\\) as \\(\\beta_1 - \\beta_2 = 0, \\beta_4 = 0\\). We can think about this as having solved for \\(\\beta_1\\) in terms of \\(\\beta_2\\) or vice-versa.\n\\[H_0: \\beta_1 = \\beta_2, \\beta_4 = 0\\]\nand the reduced model \\[E(glucose_i) = \\beta_0 + \\beta_1(D_{i1} + D_{i2}) + \\beta_3D_{i3}\\]\n\nlm.glucose.pa <- lm(glucose ~ factor(physact), data = hers)\npa.test.fine <- broom::tidy(anova(lm.glucose.pa))\n\ngt(pa.test.fine) |> \n   tab_header(title = \n               md(\"**Overall test of 5-level physical activity**\"),\n                  subtitle = \"Model of glucose with 5 PA categories\") |> \n  fmt_number(decimals = 1) |> \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n      Overall test of 5-level physical activity\n    \n    \n      Model of glucose with 5 PA categories\n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    factor(physact)\n4.0\n87,696.5\n21,924.1\n16.5\n0.0\n    Residuals\n2,758.0\n3,662,765.0\n1,328.1\nNA\nNA\n  \n  \n  \n\n\n\nhers$collapsed_physact <- \n  case_when(hers$physact %in% c(\"much less active\", 'somewhat less active') ~ 'less',\n            hers$physact %in% c(\"much more active\", 'somewhat more active') ~ 'more',\n            TRUE ~ hers$physact)\n\nlm.glucose.pacoarse <- lm(glucose ~ factor(collapsed_physact), data = hers)\n\npa.test.coarse <- broom::tidy(anova(lm.glucose.pacoarse))\n\ngt(pa.test.coarse) |> \n  tab_header(\n    title = md(\"**Overall test of 3-level physical activity**\"),\n    subtitle = \"Model of glucose with 3 PA categories\") |> \n  fmt_number(columns = df:meansq,\n             decimals=0) |> \n  fmt_number(columns = statistic:p.value,\n             decimals=2) |> \n  tab_options(table.align = 'left')\n\n\n\n\n\n  \n    \n      Overall test of 3-level physical activity\n    \n    \n      Model of glucose with 3 PA categories\n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    factor(collapsed_physact)\n2\n76,501\n38,250\n28.73\n0.00\n    Residuals\n2,760\n3,673,961\n1,331\nNA\nNA\n  \n  \n  \n\n\n\n\nOur \\(F\\)-test then becomes:\n\\[ \\frac{(87,697 - 76,501)/2}{1330} = 4.21 \\stackrel{H_0}{=} F_{2,2760} \\, \\, (p = 0.0149)\\]\nHow would we get that p-value?\nIn our case we can run:\n\nF_stat <- ((pa.test.fine$sumsq[[1]]-pa.test.coarse$sumsq[[1]])/2) / \n  pa.test.fine$meansq[[2]]\n\nprint(F_stat)\n\n[1] 4.215159\n\npf( # the distribution function (cdf) of the F distribution \n  q = F_stat,\n  df1 = 2,\n  df2 = pa.test.fine$df[[2]],\n  lower.tail = FALSE)\n\n[1] 0.01486524\n\n# compare to the anova table p-value\nanova(lm.glucose.pacoarse, lm.glucose.pa) |> \n  broom::tidy() |> \n  gt() |> \n  tab_header(\n    title = md(\"**ANOVA table comparing the 5-level to 3-level model**\"),\n    subtitle = \"Glucose regressed on physical activity\") |> \n  fmt_number(\n    columns = df.residual:df,\n    decimals = 0) |> \n  fmt_number(\n    columns = statistic:`p.value`,\n    decimals = 3\n    )\n\n\n\n\n\n  \n    \n      ANOVA table comparing the 5-level to 3-level model\n    \n    \n      Glucose regressed on physical activity\n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    glucose ~ factor(collapsed_physact)\n2,760\n3,673,961\nNA\nNA\nNA\nNA\n    glucose ~ factor(physact)\n2,758\n3,662,765\n2\n11195.89\n4.215\n0.015\n  \n  \n  \n\n\n\n\n\nWhat is the multivariate Wald test for a general linear hypothesis?\n\\[H_0 : C\\beta = 0\\]\nAnd thus \\[W = (C\\hat \\beta)'(\\widehat{\\text{Var}}(C\\hat\\beta))^{-1}(C\\beta)\\] \\[ = (C\\hat\\beta)'[C \\widehat{\\text{Var}}(\\hat\\beta) C']^{-1}(C\\hat\\beta)\\] \\[ = (C\\hat\\beta)'[C \\hat \\sigma^2 (X'X)^{-1} C']^{-1} (C\\hat\\beta)\\]\nand \\(W/r \\sim F_{r,n-p-1}\\) or asymptotically \\(W \\sim \\chi^2_r\\)."
  },
  {
    "objectID": "week3/week3.html#confidence-intervals",
    "href": "week3/week3.html#confidence-intervals",
    "title": "Week 3",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nRecall that often we obtain CIs by inverting test statistics.\nThus we can construct a confidence interval for \\(\\beta_j\\) by inverting the univariate \\(t\\)-test.\nFirst, letting \\(c\\) denote \\(t_{n-p-1,1-\\alpha/2}\\), note that \\[P(-c < \\frac{\\hat \\beta_j - \\beta_j}{\\sigma(\\hat \\beta_j)} < c) = 0.95\\]\n\\[ \\Longrightarrow (\\hat \\beta_j - c \\times \\sigma(\\hat \\beta_j) < \\beta_j < \\hat \\beta_j + c \\times \\sigma(\\hat \\beta_j)) = 0.95\\]\n\\[\\hat \\beta_j \\pm t_{n - p -1, 1 - \\alpha/2} \\sqrt{\\hat \\sigma^2 D_{jj}}\\]"
  },
  {
    "objectID": "week3/week3.html#model-estimated-expected-value",
    "href": "week3/week3.html#model-estimated-expected-value",
    "title": "Week 3",
    "section": "Model Estimated Expected Value",
    "text": "Model Estimated Expected Value\nA 100(1-\\(\\alpha)\\)% CI for \\(\\mu(x) = x'\\beta\\) is \\[\\hat \\mu(x) \\pm t_{n-p-1, 1-\\alpha/2} \\sqrt{\\hat \\sigma^2 x' (X' X)^{-1} x}\\]\n\nPrediction Intervals\nA 100(1-\\(\\alpha\\))% prediction interval for a single new observation with covariate values \\(x_{new}\\) is constructed by noting that\n\\[y_{new} = x_{new}' \\beta  + \\varepsilon_{new}\\]\nand \\(\\text{Var}(\\hat y_{new}) = \\sigma^2 x_{new}' (X'X)^{-1}x_{new} + \\sigma^2\\). Then\n\\[\\hat y_{new} \\pm t_{n-p-1, 1 -\\alpha} \\sqrt{ \\hat \\sigma^2 \\left(1 + x_{new}' (X'X)^{-1} x_{new}\\right)}\\]\nwhere \\(\\hat y_{new} = x_{new}' \\hat \\beta\\).\nThus the predictions for \\(y_{new}\\) have have uncertainty both from the estimate of \\(\\hat \\beta\\) and the estimated error variance \\(\\hat \\sigma^2\\).\n\nlm.sbp.age <- lm(SBP ~ age, data = hers)\n\npred_with_ci <- predict(lm.sbp.age, interval = 'confidence')\npred_with_pi <- predict(lm.sbp.age, interval = 'prediction')\n\nWarning in predict.lm(lm.sbp.age, interval = \"prediction\"): predictions on current data refer to _future_ responses\n\nintervals <- data.frame(hers$age, pred_with_ci, pred_with_pi[,2:3])\n\nnames(intervals) <- c('age', 'yhat', 'lwr_ci', 'upr_ci',\n                      'lwr_pi', 'upr_pi')\n\nintervals <- intervals %>% arrange(age)\n\nggplot(intervals, aes(age)) + \n  geom_ribbon(aes(ymin= lwr_pi, ymax = upr_pi, fill = 'prediction interval'), alpha = 0.5) + \n  geom_ribbon(aes(ymin= lwr_ci, ymax = upr_ci, fill = 'confidence interval'), alpha = 0.8) + \n  geom_line(aes(y = yhat)) + \n  geom_jitter(data = hers, aes(age, SBP), size = .75, width = .5, height = 0, alpha = 0.15) + \n  scale_fill_manual(values = c('prediction interval' = 'orange', 'confidence interval' = 'cornflowerblue')) + \n  theme_bw() +\n  labs(y = 'SBP', fill = 'interval type') + \n  ggtitle(\"Regression Prediction and Confidence Intervals\") + \n  theme(legend.position = 'bottom')\n\n\n\n\nNote that we have assumed \\(\\varepsilon \\sim \\mathcal N(0, \\sigma^2)\\) to construct the prediction interval. If the error terms are not close to normal, then the prediction interval could be misleading. This is not the case for the interval for the expected value, which only requires approximate normality for \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\)."
  },
  {
    "objectID": "week3/week3.html#r2-and-adjusted-r2",
    "href": "week3/week3.html#r2-and-adjusted-r2",
    "title": "Week 3",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\\[R^2 = 1 - \\frac{SSE}{SST} = \\frac{SSR}{SST}\\]\nThe proportion of the total variation in \\(Y_i\\) explained by \\(X_i\\).\nBecause \\(0 \\leq SSE \\leq SST\\), \\(0 \\leq R^2 \\leq 1\\).\n\\(R^2\\) increases whenever new terms are added to the model.\nTherefore for model comparison, most people often use a version of the \\(R^2\\) that is adjusted for the number of predictors in the model. This is the adjusted \\(R^2\\), defined as \\[R^2 = 1 - \\left( \\frac{n-1}{\\text{Error df}} \\right) \\frac{SSE}{SST}  = 1 - \\frac{MSE}{SST/(n-1)}\\]\nUsing the MSE is essentially a penalization on wasting unused parameters since \\[MSE = \\frac{\\sum_{i=1}^n (x_i - \\bar x_i)^2}{n - p - 1}.\\]"
  },
  {
    "objectID": "week3/week3.html#variance-estimates-to-confidence-intervals",
    "href": "week3/week3.html#variance-estimates-to-confidence-intervals",
    "title": "Week 3",
    "section": "Variance estimates to confidence intervals",
    "text": "Variance estimates to confidence intervals\nWe know that if \\(Z\\) is a random vector and \\(A\\) is a matrix of constants, then\n\\[\\text{Var}(AZ) = A \\text{Var}(Z) A^T.\\]\nThis implies that\n\\[\\text{Var}(\\hat \\beta) = \\text{Var}[(X^TX)^{-1} X^TY] = \\sigma^2 (X^TX)^{-1}\\]\nThis is a quadratic form. If we were in a scalar world with a variable \\(a \\in \\mathbb R\\) and \\(z\\) a random variable. Then \\(\\text{Var}(az) = a^2 \\text{Var}(z)\\). When we work with matrices, the square of a matrix is analogous to writing \\(AA^T\\)\nRemember that \\(\\hat \\beta = \\underbrace{(X^TX)^{-1} X^T}y\\).\n\\[\\text{Var}(\\beta) = \\left( \\begin{array}{cccc}\n\\text{Var}(\\beta_1) & \\text{Cov}(\\beta_1,\\beta_2) & ... & ... \\\\\n\\text{Cov}(\\beta_2, \\beta_1) & \\text{Var}(\\beta_2) & ... & ... \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\n\\end{array}\\right).\\]\n\\[95\\%CI(\\hat \\beta_j) = \\hat \\beta_j \\pm t_{n-p-1, 1-\\alpha/2} \\hat \\sigma \\sqrt{[(X^TX)^{-1}]_{j,j}}.\\]\nBut we can use the same idea to obtain variance and CI estimates for the prediction of a new/future observation \\(x_{new}\\). Given \\(x_{new}\\) we can predict their mean response \\(\\mathbb E(\\hat y|x_{new}) = x^T_{new}\\hat \\beta\\).\nNote that we cannot predict the response itself since we don’t know what \\(\\varepsilon_{new}\\) is.\nWhen it comes to confidence intervals, we can indeed get two different types of intervals.\n\\[\\text{Predicted mean response: } \\quad \\text{Var}(\\hat{\\mathbb E}(\\hat y|x_{new})) =\n\\text{Var}(x_{new}^T \\hat \\beta) = \\sigma^2 x^T_{new} (X^TX)^{-1} x_{new},\\]\nwhich gives the 95% confidence interval:\n\\[x_{new}^T \\hat \\beta \\pm t_{n-p-1,1-\\alpha/2} \\hat \\sigma \\sqrt{x_{new}^T (X^TX)^{-1} x_{new}}.\\]\n\\[\\text{The predicted response: }\n\\quad \\text{Var}(\\hat y|x_{new}) = \\text{Var}(x_{new}^T \\hat \\beta + \\varepsilon_{new}) = \\sigma^2 x_{new}^T (X^TX)^{-1}x_{new} + \\sigma^2,\\]\nwhich gives the 95% prediction interval:\n\\[x_{new}^T \\hat \\beta \\pm t_{n-p-1, 1-\\alpha/2} \\hat \\sigma \\sqrt{1 + x_{new}^T(X^TX)^{-1}x_{new}}.\\]\nIn general, we can’t predict \\(y_new = \\hat \\beta_0 + \\hat \\beta_1 x_1 + ... + \\varepsilon_{new}\\) because we don’t know \\(\\varepsilon_{new}\\). However, we can predict \\[\\text{Var}(\\hat y_{new}|x_{new}) = \\text{Var}(\\hat \\beta_0 + \\hat \\beta_1 x_1 + ...) + \\underbrace{\\text{Var}(\\varepsilon_{new})}_{=0,\\, \\text{ by assumption}}.\\]\nWhy should these values be \\(t\\)-distributed? Because finite samples of the \\(\\beta\\) distributed \nOften we just write \\(\\mathbb E(y) = X\\beta\\), but this isn’t really the full model. It’s missing an assumption: \\(\\text{Var}(y) = \\sigma^2I_n\\) where \\(I_n\\) is the \\(n \\times n\\) identity matrix.\nWe get \\(\\hat \\beta\\) often from either \\(OLS\\) or \\(MLE\\), and we get the \\(\\hat \\sigma^2\\) from the MSE.\nWhy do we care about these values? One way is to just say “center and spread” — but a more sophisticated way is to realize that these two statistics completely characterize a normal distribution.\nThe moment generating function says that \\[D(Y) = 1 + \\mathbb Ey + \\text{Var}(y) + \\text{Skew}(y) + \\text{Kurtosis}(y) + ...\\]\nConfidence intervals tell us what are the plausible range of model parameters."
  },
  {
    "objectID": "week3/week3.html#hypothesis-testing",
    "href": "week3/week3.html#hypothesis-testing",
    "title": "Week 3",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nHypothesis testing is about having a hunch and seeing if we’re right.\nFor multiple linear regression, there are many options for testing the significance of predictor variables. These tests fall under two main categories: F tests and Wald tests. Both are asymptotically equivalent, so yield comparable results in hypothesis testing.\nThe F-test is looking at the variances. The Wald test is looking at the behavior of the means.\n\nF-tests (comparison of variances)\nRecall that\n\nSST (Total Sum of Squares) is a measure of the total variance in the outcome from the given sample.\nSSR (Regression Sum of Squares) represents the total variance in the outcome explained by the regression model.\nSSE (Sum of Squared Errors) represents the remaining total variance in the outcome that is not captured by the regression model. We use the SSE to estimate the true (unobserved) variance \\(\\sigma^2\\) of the residuals. Let \\(\\hat \\sigma^2 = MSE = SSE/(n-p-1)\\) for a model with \\(p\\) predictors and an intercept.\n\n\\[SST \\stackrel{def}{=} \\sum_{i=1}^n (Y_i - \\bar Y)^2 = \\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2 + \\sum_{i=1}^n (Y_i - \\hat Y_i)^2 \\stackrel{def}{=} SSR + SST\\]\nThe \\(F\\)-tests are:\n\nTest for no covariate effect \\(H_0: \\beta_1 = ... = \\beta_p = 0\\)\n\n\\[F = \\frac{SSR/p}{SSE/(n-p-1)} = \\frac{SSR/p}{MSE} \\sim F_{p,n-p-1}.\\] 2. Test for a single variable \\(x_j\\). \\(H_0 : \\beta_j = 0\\).\n\\[F = \\frac{[SSR - SSR(\\text{model without } x_j)]/1}{SSE/(n-p-1)}\\] \\[ =\n  \\frac{SSR - SSR(\\text{model without } x_j)}{MSE} \\sim F_{1, n-p-1}.\\]\n\nTest for a group of \\(r\\) variables. \\(H_0: \\beta_{j_1} = ... = \\beta_{j_r} = 0\\).\n\n\\[F = \\frac{[SSR - SSR(\\text{model without } r \\text{ variables})]/r}{SSE/(n-p-1)}\\] \\[ = \\frac{[SSR - SSR(\\text{model without } r \\text{ variables})]/r}{MSE} \\sim F_{1,n-p-1}.\\]\n\nTest of a general linear hypothesis, \\(H_0: C\\beta= 0\\) where \\(C\\) is an \\(r \\times (p+1)\\) matrix of linearly independent contrasts. The test requires calculation of the sum of squares for the reduced model, where we parameterize according to the null hypothesis.\n\n\\[F = \\frac{(SSR - SSR(\\text{reduced model}))/r}{MSE} \\sim F_{r,n-p-1}\\]\nThe MSE is from the full model.\nNote: the reduced model must be nested within the full model for us to use the \\(F\\)-test.\n\nA great question is to look at this formula and say that the numerator is a marginal quantity, so why shouldn’t the denominator also be marginal?\nOne reason might be that if it were, the denominator would no longer have the same degrees of freedom as the numerator.\nAnother reason, arguably more important, is that this quantity would no longer be \\(F\\)-distributed, and we really want to have a quantity with nice asymptotic distributional properties.\n\nLet’s look at an example of situation 4:\nThe full model might be \\[\\mathbb E(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\]\nAnd model 2 is given as \\[\\mathbb E(y) = \\gamma_0 + \\gamma_1 x_1\\]\nThen\n\\[\\left( \\begin{array}{cccc} 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{array} \\right) \\left( \\begin{array}{c} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\end{array} \\right) = \\left( \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right)\\]\nIn general, when we say that the reduced model must be nested within the full model, what we’re saying is that the reduced model can be expressed as a linear constraint imposed on the full model.\nFor example, we might have a scenario where \\(Age \\in \\{ 57, 58, 59 \\}\\) and our reduced model is \\[y = \\beta_0 + \\beta_1 Age\\]\nAnd our full model is \\[y = \\gamma_0 + \\gamma_1 \\mathbb 1(Age = 57) + \\gamma_2 \\mathbb 1(Age = 58) + \\gamma_3 \\mathbb 1 (Age = 59)\\]\nIf we make a new variable that is \\[57 \\mathbb 1(Age = 57) + 58 \\mathbb 1(Age = 58) + 59 \\mathbb 1 (Age = 59)\\], and this recovers the original age variable.\n\nThe \\(F\\)-test can be thought of as a cost-benefit ratio."
  },
  {
    "objectID": "week3/week3.html#wald-test-comparison-of-means",
    "href": "week3/week3.html#wald-test-comparison-of-means",
    "title": "Week 3",
    "section": "Wald test (comparison of means)",
    "text": "Wald test (comparison of means)\nAs we touched on in lecture, we can also consider an alternative hypothesis test formulation for linear regression—this will also help motivate the form of our confidence intervals below. Recall the following properties: For an \\(r \\times (p+1)\\) matrix \\(C\\) and a random vector \\(y\\), then\n\\[\\mathbb E(Cy) = C\\mathbb E(y) \\quad \\text{and} \\quad \\text{Var}(Cy) = C\\text{Var}(y)C^T\\]\nAs we have shown previously, \\[\\hat \\beta \\sim MVN_{p+1} (\\beta, \\sigma^2 C(X^TX)^{-1}C^T)\\]\nSo, to test the general linear hypothesis \\(H_0: C\\beta = 0\\), we have the following statistic (which generalizes the familiar univariate Wald statistic \\(W = \\hat \\beta^2 / \\widehat{\\text{Var}}(\\hat \\beta) \\sim \\chi^2_1\\), asypmptotically):\n\\[W = (C\\hat \\beta)^T\\underbrace{[\\hat \\sigma^2 C(X^TX)^{-1}C^T]^{-1}}_{\\text{variance of } C\\beta}(C\\hat \\beta)\\]\nIf we think of what we’d do in a standard intro stats class, we’d do one of two things.\n\nWe’d look at \\(\\hat \\beta_1 / \\hat \\sigma(\\hat \\beta_1)\\), which is \\(t\\)-distributed.\nOr we’d look in \\(\\hat \\beta_1^2/\\widehat{Var}(\\hat \\beta_1)\\) which is \\(F\\)-distributed and asymptotically \\(\\chi^2\\) distributed.\n\nThen by properties of the \\(F\\)-statistic, \\(W/r \\sim F_{r,n-p-1}\\), and also asymptotically \\(W \\sim \\chi_r^2\\)."
  },
  {
    "objectID": "week3/week3.html#additional-remarks",
    "href": "week3/week3.html#additional-remarks",
    "title": "Week 3",
    "section": "Additional Remarks",
    "text": "Additional Remarks\nNote that the univariate Wald Statistic \\(W = \\hat \\beta^2 / \\widehat{\\text{Var}}(\\hat \\beta) \\sim \\chi_1^2\\) is equivalent to the \\(t\\)-test statistic\n\\[t_{obs} = \\frac{\\hat \\beta_1}{s.e.(\\hat\\beta_1)} \\sim t_{n-2,(1-\\alpha/2)}.\\]\nWhile we will not go into this in detail, the theoretical motivation for all of these tests is that they are ratios of \\(\\chi^2\\)-distributed random variables that are scaled by their degrees of freedom, which in turn defines the \\(F\\)-distribution.\nThe \\(F\\)-test is valid if and only if the \\(\\chi^2\\) assumption for the numerator and denominator holds true. Yet, \\(\\chi^2\\) random variables are obtained from the sums of squared normal random variables. This is why it is necessary for the residuals \\(\\varepsilon_i\\) to follow a normal distribution and/or 2) large sample theory to hold (so that the \\(\\varepsilon_i\\) may be approximately normal).\nThese \\(F\\)-tests are sometimes given in the context of an analysis of variance (ANOVA) model and table, which presents the sum of squares explained by each variable, conditional on all previous variables in the model, and then the sum of squares for the error terms. Each test compares two models, one nested within the other. Nested models will come up again when we discuss likeli- hood ratio tests, of which the F -test is a special case under the assumption of normally-distributed outcomes.\nA lack of significance of the effect of a covariate or group of covariates does not necessarily indicate the absence of an effect. Whether to remove these covariates from the model will depend on the scientific goal of the study."
  },
  {
    "objectID": "week3/week3.html#hypothesis-testing-for-nested-models-with-ordinalinteger-variables",
    "href": "week3/week3.html#hypothesis-testing-for-nested-models-with-ordinalinteger-variables",
    "title": "Week 3",
    "section": "Hypothesis Testing for Nested Models with Ordinal/Integer Variables",
    "text": "Hypothesis Testing for Nested Models with Ordinal/Integer Variables\nSuppose we want to model the relationship between year in a PhD program and some outcome where \\(X \\in \\{ 1, 2, 3, 4, 5 \\}\\) and \\(Y\\) is some continuous outcome, and we have reason to believe the association might be linear.\nWe might model that as \\(Y \\sim \\gamma_0 + \\gamma_1 X\\) (model 1). Another way we could model it that is more flexible (not linear) is:\n\\[Y = \\beta_0 + \\beta_1 \\mathbb 1(X = 1) + \\beta_2 \\mathbb 1(X=2)  + \\beta_3 \\mathbb 1(X=3)  + \\beta_4 \\mathbb 1(X=4)  + \\beta_5 \\mathbb 1(X=5) \\quad \\text{(model 2)}\\]\nWhat may not be so obvious is that model 1 is nested inside model 2. The crucial insight is that\n\\[X = \\mathbb 1(X = 1) + 2 \\mathbb 1(X=2) + 3 \\mathbb 1(X=3) +2 \\mathbb 3(X=3) +5 \\mathbb 1(X=5).\\]\nIf we were to substitute this into model 1, we have that \\[\\begin{aligned} Y & = \\gamma_0 + \\gamma_1 (\\mathbb 1(X = 1) + 2 \\mathbb 1(X=2) + 3 \\mathbb 1(X=3) +2 \\mathbb 3(X=3) +5 \\mathbb 1(X=5)) \\\\\n& = \\gamma_0 + \\gamma_1 \\mathbb 1(X = 1) + 2 \\gamma_1 \\mathbb 1(X=2) + 3 \\gamma_1 \\mathbb 1(X=3) + 4 \\gamma_1 \\mathbb 1(X=4) +5 \\gamma_1 \\mathbb 1(X=5)\n\\end{aligned}\\]\nComparing this to model 2, we see that we need to construct a constraint matrix where \\[\\beta_2 = 2 \\beta_1\\] \\[\\beta_3 = 3 \\beta_1\\] \\[\\beta_4 = 4 \\beta_1\\] \\[\\beta_5 = 5 \\beta_1\\]\nWriting out our constraint matrix, we would have that\n\\[\\begin{pmatrix}\n0 & 2 & -1 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & -1 & 0 & 0 \\\\\n0 & 4 & 0 & 0 & -1 & 0 \\\\\n0 & 5 & 0 & 0 & 0 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5\n\\end{pmatrix} =\n\\begin{pmatrix}\n0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\\]\nApplying this constraint to model 2 would reduce it to model 1, showing that model 1 is nested within model 2."
  },
  {
    "objectID": "week14/week14.html#binomial-count-data",
    "href": "week14/week14.html#binomial-count-data",
    "title": "Week 14",
    "section": "Binomial count data",
    "text": "Binomial count data\nRecall that a binomial random variable is the number of successes from \\(N\\) independent (Bernoulli) trials with a common probability of success.\nIn applications with binomial count outcomes, data are often of the form:\n\n\n\n# Trials\n# Successes\nCovariates\n\n\n\n\n\\(N_1\\)\n\\(Y_1\\)\n\\(x_1\\)\n\n\n\\(N_2\\)\n\\(Y_2\\)\n\\(x_2\\)\n\n\n…\n…\n…\n\n\n\\(N_n\\)\n\\(Y_n\\)$\n\\(x_n\\)\n\n\n\n\nAn example might be a study investigating factors associated with medication adherence where each individual’s outcome is the count of medication doses taken (out of a fixed number of doses prescribed).\nIf we assume that each individual’s \\(N_i\\) trials are independent and have probability of success \\(\\pi_i\\), then\n\\[Y_i \\sim \\text{Binomial}(N_i, \\pi_i)\\]\n\\(\\mathbb E[Y_i] = \\mu_i = N_i \\pi_i\\) and \\(\\text{Var}[Y_i] = N_i \\pi_i(1-\\pi_i)\\).\nSince \\(N_i\\) is fixed, we can structure a model for \\(\\pi_i\\) (which implies a model for \\(\\mu_i\\)).\n\\[g(\\pi_i) = x_i' \\beta \\quad \\Longrightarrow \\quad \\mu_i = \\underbrace{g^{-1}(x_i \\beta)}_{=\\pi_i} \\times N_i\\]\nWe can choose \\(g(\\cdot)\\) from among the usual candidates.\n\nLogit is the most common\nAnd coefficient estimates have analagous interpretations to the binary outcome setting.\n\nIn many settings, rather than \\(N_i\\) trials on the \\(i\\)th study unit, the count data we observe are collapsed binary data.\nConsider this Ohio lung cancer death data:\n\n\n\nAge\nSex\nRace\nN\nDeath\n\n\n\n\n55-64\nM\nWhite\n429617\n1025\n\n\n55-64\nM\nNon-White\n48382\n172\n\n\n55-64\nF\nWhite\n476170\n507\n\n\n55-64\nF\nNon-White\n54662\n81\n\n\n…\n…\n…\n…\n…\n\n\n\nThese data are disguised as binomial counts, but they are technically collapsed binary outcomes.\nThis dataset consists of a total of \\(n=12\\) rows, but represents outcomes for \\(N = 2,220,177\\) individuals.\nLet \\(Y_{ij} = 0/1\\) be a binary indicator of death due to lung cancer for the \\(j\\)th individual in the \\(i\\)th age/sex/race-stratum.\n\\(i = 1,...,12\\), \\(j = 1,...,N_i\\), and let \\(\\pi_{ij} = P(Y_{ij} = 1)\\) and suppose\n\\[\n\\begin{aligned}\n\\text{logit}(\\pi_{ij}) = \\beta_0 + &\n\\beta_{A1} \\mathbb{1}(\\text{Age}_{ij} = \"65-74\")\n+ \\beta_{A2} \\mathbb{2}(\\text{Age}_{ij} = \"75-84\") + \\\\\n\\beta_S \\mathbb {1}(\\text{Sex}_{ij} = \"F\") +\n\\beta_R \\mathbb 1 (\\text{Race} = \\text{\"Non-White\"})\n\\end{aligned}\\]\nBut under this model, \\(\\pi_{ij} = \\pi_i\\) for all individuals in age/sex/race stratum \\(i\\). So for individuals in the 65-74 age range who are women and non-white:\n\\[\\pi_{ij} = \\pi_i = \\text{expit}\\{ \\beta_0 + \\beta_{A1} + \\beta_S + \\beta_R \\}\\]\nAssuming independence across individuals, the likelihood is:\n\\[\\begin{aligned}\n\\mathcal L(\\beta) = & \\prod_{i=1}^{12} \\prod_{j=1}^{N_i} \\pi_i^{y_{ij}} (1-\\pi_i)^{1-y_{ij}} \\\\\n& = \\prod_{i=1}^{12} \\pi_{i}^{\\sum_j y_{ij}} (1-\\pi_i)^{\\sum_j (1-y_{ij})} \\\\\n& = \\prod_{i=1}^{12} \\pi_{i}^{Y_i}(1-\\pi_i)^{N_i - Y_i}\n\\end{aligned}\\]\nNote that this only differs from a binomial likelihood for the collapsed outcmoes in that there’s no \\({N_i \\choose Y_i}\\). But the \\({N_i \\choose Y_i}\\) term doesn’t involve \\(\\beta\\) anyway.\nThus we can treat the 12 collapsed observations as if they were binomial counts and model them as described in the collapsed table.\nIn R we can fit GLMs to binomial count data with a slight modification to the syntax used to fit GLMs to binary data. The only thing that looks different is that we change the left-hand-side:\n\nglm(cbind(Death, N-Death) ~ factor(Age) + Sex + Race,\n  data = Ohio,\n  family = binomial())\n\nAs with logistic regression analyses of binary data, exponentiate to report contrasts on the odds ratio scale:\n\njtools::export_summs(fit0, exp=T, error_pos = 'right',\n  error_format = \"[{conf.low}, {conf.high}]\")"
  },
  {
    "objectID": "week14/week14.html#poisson-count-data",
    "href": "week14/week14.html#poisson-count-data",
    "title": "Week 14",
    "section": "Poisson Count Data",
    "text": "Poisson Count Data\nIn many settings, we’re interested in a count of events but there’s no notion of a fixed number of trials. E.g., the number of major complications during a surgery.\nA counting process \\(Y(t)\\), counts the number of events in some population by time \\(t\\) (starting from a designated time 0).\nAs we move forward in time, \\(Y(t)\\) counts the number of events or ‘jumps’.\n\\[Y(0) = 0 \\; \\longrightarrow \\; Y(t_1) = 1 \\; \\longrightarrow \\; Y(t_2) = 2 ...\\]\nA counting process is a time-homogeneous Poisson process under certain assumptions.\nCrucially, the rate at which events occur per unit time, \\(\\lambda\\), should be constant (not time-dependent). Then, for fixed \\(t\\), \\(Y(t) \\sim \\text{Poisson}(\\lambda t)\\).\nFor Poisson count data, the data are (typically) of the form:\n\n\n\nFollow-up time/person-time\nNumber of events\nCovariates\n\n\n\n\n\\(t_1\\)\n\\(Y_1\\)\n\\(x_1\\)\n\n\n\\(t_2\\)\n\\(Y_2\\)\n\\(x_2\\)\n\n\n…\n…\n…\n\n\n\nAssuming each \\(Y_i\\) arises from a time-homogeneous Poisson process with rate \\(\\lambda_i\\),\n\\[Y_i \\sim \\text{Poisson}(\\lambda_i t_i).\\]\n\\[\\mathbb E[Y_i] = \\text{Var}[Y_i] = \\lambda_i t_i\\]\n\\(t_i\\) is often “person-time” to allow for the fact that each unit in the data could represent populations of different sizes.\nIn Poisson regression, we assume that\n\\[Y_i \\sim \\text{Poisson}(\\mu_i)\\] \\[\\mu_i = \\lambda_i t_i\\]\nSimilar to \\(N_i\\) in the binomial count setting, \\(t_i\\) is assumed to be fixed and known, so we build the systematic component of the model around the rate parameter \\(\\lambda_i\\).\nSince \\(\\lambda_i\\) is strictly positive, Poisson regression uses the log link:\n\\[\\log(\\lambda_i) = \\log \\left( \\frac{\\mu_i}{t_i} \\right) = x_i' \\beta\\]\nThis can be equivalently written as \\[\\log(\\mu_i) = x_i' \\beta + \\log(t_i)\\]"
  },
  {
    "objectID": "week14/week14.html#closed-form-logistic-binary-outcome-binary-treatment",
    "href": "week14/week14.html#closed-form-logistic-binary-outcome-binary-treatment",
    "title": "Week 14",
    "section": "Closed Form Logistic (Binary Outcome, Binary Treatment)",
    "text": "Closed Form Logistic (Binary Outcome, Binary Treatment)\n\\[\\text{logit}(\\mu_i) = \\beta_0 + \\beta_1 x_i\\]\nIn general we cannot obtain closed-form expressions for \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) or for higher dimension \\(\\beta\\) by solving the logistic regression score equations analytically. However, let’s consider a scenario in which we can obtain closed-form solutions. In particular, suppose our covariate \\(x_i\\) is binary and we want to fit a model to our data:\n\n\n\n\nY = 1\n0\n\n\n\n\n\nX = 1\n\\(O_{11}\\)\n\\(O_{12}\\)\n\\(O_{1\\cdot}\\)\n\n\n0\n\\(O_{21}\\)\n\\(O_{22}\\)\n\\(O_{2\\cdot}\\)\n\n\n\n\\(O_{\\cdot 1}\\)\n\\(O_{2 \\cdot}\\)\n\\(O_{\\cdot \\cdot}\\)\n\n\n\nLet \\(\\mu_0 = P(Y_i = 1 | X_i = 0)\\) and \\(\\mu_1 = P(Y_i = 1 | X_i = 1)\\).\nWrite an expression for the log-likelihood in terms of \\(\\mu_0\\), \\(\\mu_1\\) and the observed cell counts.\n\\[L(\\mu_0, \\mu_1 \\mid y ) = \\prod_{i=1}^n \\mu_i^{y_i}(1-\\mu_i)^{1-y_i}\\]\n\\[ = \\prod_{i=1}^{O_{2\\cdot}} \\mu_0^{y_i}(1-\\mu_0)^{1-y_i} \\cdot \\prod_{j=1}^{O_{1\\cdot}} \\mu_1^{y_i} (1-\\mu_1)^{1-y_i}\\]\n\\[ = \\mu_0^{\\sum_{i=1}^{O_{2 \\cdot }} y_i} (1 - \\mu_0)^{\\sum_{i=1}^{O_{2\\cdot}} (1-y_i)}\n\\cdot\n\\mu_1^{\\sum_{i=1}^{O_{1 \\cdot }} y_i} (1 - \\mu_1)^{\\sum_{i=1}^{O_{1\\cdot}} (1-y_i)}\\]\n\\[ = \\mu_0^{O_{21}} (1-\\mu_0)^{O_{22}} \\mu_1^{O_{11}} (1-\\mu_1)^{O_{12}}\\]\n(Note that the indexing is a little bit messed up; really we should have been writing \\(\\sum_{i : X_i = 0}^n y_i = O_{21}\\) and such.)\n\\[\\ell (\\mu_0, \\mu_1 \\mid y ) =\n  O_{21} \\log (\\mu_0) + O_{22} \\log (1-\\mu_0) +\n  O_{11} \\log (\\mu_1) + O_{12} \\log (1-\\mu_1) \\]\nBased on this new log-likelihood, let’s obtain the score equations \\(U(\\mu_0)\\) and \\(U(\\mu_1)\\) and solve to obtain closed-form expressions for \\(\\hat \\mu_0\\) and \\(\\hat \\mu_1\\).\nSo the score equations are\n\\[\n\\begin{aligned}\nU(\\mu_0) & = \\frac{d}{d \\mu_0} \\ell (\\mu_0, \\mu_1 \\mid y) \\\\\n& = \\frac{d}{d\\mu_0} \\left(\n   O_{21} \\log (\\mu_0) + O_{22} \\log (1-\\mu_0) +\n  O_{11} \\log (\\mu_1) + O_{12} \\log (1-\\mu_1)\n\\right) \\\\\n& = \\frac{O_{21}}{\\mu_0} - \\frac{O_{22}}{1-\\mu_0}\n\\end{aligned}\n\\]\nand \\[\n\\begin{aligned}\nU(\\mu_1) & = \\frac{d}{d \\mu_0} \\ell (\\mu_0, \\mu_1 \\mid y) \\\\\n& = \\frac{d}{d\\mu_0} \\left(\n   O_{21} \\log (\\mu_0) + O_{22} \\log (1-\\mu_0) +\n  O_{11} \\log (\\mu_1) + O_{12} \\log (1-\\mu_1)\n\\right) \\\\\n& = \\frac{O_{11}}{\\mu_1} - \\frac{O_{12}}{1-\\mu_1}\n\\end{aligned}\n\\]\nTo solve to obtain closed form expressions of \\(\\hat \\mu_0\\) and \\(\\hat \\mu_1\\), we take these derivatives and set them equal to zero.\nSo \\[0 = \\frac{O_{11}}{\\hat\\mu_1} - \\frac{O_{12}}{1-\\hat\\mu_1} \\Longrightarrow\n\\frac{O_{12}}{1-\\hat\\mu_1}  = \\frac{O_{11}}{\\hat\\mu_1}\n\\]\n\\[ \\Longleftrightarrow (1-\\hat\\mu_1)O_{11} = \\hat\\mu_1 O_{12} = O_{11} - \\hat\\mu_1 O_{11}\\]\n\\[ \\hat \\mu_1 (O_{12} + O_{11}) = O_{11} \\Longleftrightarrow \\mu_1 = \\frac{O_{11}}{O_{1\\cdot}}\\]\nAnd similar for \\(\\hat \\mu_0 = \\frac{O_{21}}{O_{2\\cdot}}\\).\nWhat we’re really after are the \\(\\hat \\beta\\) values.\n\\[\\text{logit}(\\mu_0) = \\beta_0 \\quad \\text{or} \\quad \\text{logit}(\\mu_1) = \\beta_0 + \\beta_1\\]\nSo \\[\n\\begin{aligned}\n\\hat \\beta_0 = \\text{logit}(\\hat \\mu_0) & = \\log\\left(\\frac{O_{21} / O_{2\\cdot}}{1 - O_{21}/O_{2\\cdot} } \\right) \\\\\n& = \\log \\left( \\frac{O_{21} / \\cancel{O_{2\\cdot}}}{O_{22}/\\cancel{O_{2\\cdot}}}\\right) \\\\\n& = \\log \\left( \\frac{O_{21}}{O_{22}} \\right) \\\\\n& = \\text{ the log odds.}\n\\end{aligned}\n\\]\nSimilarly,\n\\[\n\\begin{aligned}\n\\hat \\beta_1 = \\text{logit}(\\hat \\mu_1) - \\text{logit}(\\hat \\mu_0) = \\log\\left( \\frac{O_{11}}{O_{12}} \\right) -\n\\log \\left( \\frac{O_{21}}{O_{22}}\\right) \\\\\n& = \\log \\left( \\frac{O_{11}O_{22}}{O_{21}O_{12}}\\right)\n\\end{aligned}\n\\]\n\nHow do we know that these are MLEs? If we plug in MLEs for the \\(\\hat \\mu\\) values into the formula for the \\(\\hat \\beta\\) values, how do we know we get MLEs?\nMLEs have nice asymptotic properties that if you plug in an MLE into a function, you get an MLE for the function output.\n\nWe showed that the Fisher information matrix \\(\\mathcal I(\\beta)\\) takes the following form for a general logistic regression:\n\\[\n\\mathcal I(\\beta) = \\begin{pmatrix} \\sum_{i=1}^n \\mu_i ( 1- \\mu_i) & \\sum_{i=1}^n x_i \\mu_i ( 1- \\mu_i) \\\\\n\\sum_{i=1}^n x_i \\mu_i ( 1- \\mu_i) & \\sum_{i=1}^n x_i^2 \\mu_i ( 1- \\mu_i) \\end{pmatrix}\n\\]\nDerive an asymptotic variance estimator for \\(\\hat \\beta_1\\) by:\n\nSimplifying \\(\\mathcal I(\\beta)\\) from the above in terms of \\(\\mu_0\\) and \\(\\mu_1\\) and the observed cell counts.\nCalculating \\(\\mathcal I(\\beta)^{-1} \\vert_{\\mu = \\hat \\mu}\\).\n\nHint: \\[\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix}\n   d & -b \\\\ -c & a \\end{pmatrix}\n   \\]\n\n\\[\\mathcal I(\\beta) = \\begin{bmatrix}\n\\sum_{i:X_i = 0}^{n} \\mu_i(1-\\mu_i) +  \\sum_{i:X_i = 1}^{n} \\mu_i(1-\\mu_i) & ... \\\\\n... & ...\n\\end{bmatrix}\\]\n\\[\\mathcal I(\\beta) = \\begin{bmatrix}\nO_{2\\cdot} \\mu_0 (1-\\mu_0) + O_{1\\cdot} \\mu_1(1-\\mu_1) & O_{1\\cdot} \\mu_1(1-\\mu_1) \\\\\nO_{1\\cdot} \\mu_1(1-\\mu_1) & O_{1\\cdot} \\mu_1(1-\\mu_1)\n\\end{bmatrix}\\]\nSo we get that\n\\[\\mathcal I^{-1}(\\beta) = \\frac{1}{(O_{2\\cdot} \\mu_0 (1-\\mu_0) + O_{1\\cdot} \\mu_1(1-\\mu_1))(O_{1\\cdot} \\mu_1(1-\\mu_1)) - (O_{1\\cdot} \\mu_1(1-\\mu_1))^2} \\begin{bmatrix}\nO_{1\\cdot} \\mu_1(1-\\mu_1) & -O_{1\\cdot} \\mu_1(1-\\mu_1) \\\\\n- O_{1\\cdot} \\mu_1(1-\\mu_1) & O_{2\\cdot} \\mu_0 (1-\\mu_0) + O_{1\\cdot} \\mu_1(1-\\mu_1)\n\\end{bmatrix}\\]\nNote the simplification we can make in \\(a\\).\n\\[\\begin{aligned}\n\\text{Var}(\\hat \\beta_1) & = \\mathcal I^{-1}(\\beta)_{22} \\vert_{\\mu = \\hat \\mu} \\\\\n& = \\frac{a}{ad-bc} = \\frac{O_{2\\cdot} \\mu_0 (1-\\mu_0) + O_{1 \\cdot } \\mu_1 (1-\\mu_1)}{\n  O_{2\\cdot} \\mu_0 (1-\\mu_0) \\times O_{1 \\cdot } \\mu_1 (1-\\mu_1)\n} \\\\\n& = \\frac{1}{O_{1\\cdot} \\mu_{1}(1-\\mu_1)} + \\frac{1}{O_{2\\cdot} \\mu_0 (1-\\mu_0)} \\\\\n& = \\frac{1}{O_{1\\cdot} \\frac{O_{11}O_{12}}{O_{1\\cdot} O_{1\\cdot}}} + \\frac{1}{O_{2\\cdot} \\frac{O_{21}O_{22}}{O_{2\\cdot} O_{2\\cdot}}}  \\\\\n& = \\frac{1}{O_{12}} + \\frac{1}{O_{11}} + \\frac{1}{O_{22}} + \\frac{1}{O_{21}}\n\\end{aligned}\n\\]\nThis matches what we derived using the delta method"
  },
  {
    "objectID": "week14/week14.html#conditional-logistic-regression-1",
    "href": "week14/week14.html#conditional-logistic-regression-1",
    "title": "Week 14",
    "section": "Conditional Logistic Regression",
    "text": "Conditional Logistic Regression\nSuppose by experimental design, or by sampling from the data we have, we’ve matched cases and controls by their characteristics.\n\n\n\nStrata\n\\(Y\\)\nMatched \\(Z\\)\n\\(X\\)\n\n\n\n\n1\n1\nFemale, 29\n\n\n\n1\n0\nFemale, 29\n\n\n\n1\n0\nFemale, 29\n\n\n\n2\n1\nMale, 32\n\n\n\n2\n0\nMale, 32\n\n\n\n\nThe likelihood is instead retrospective now:\n\\[\\mathcal L_R = \\prod_{k=1}^{K} \\prod_{i=1}^{N_k} P(Y_{ki} = y_{ki} \\mid X_{ki}, \\text{Strata} = k)\\]\nSo one could just treat the strata as another variable, and write that\n\\[\\text{logit}(Y) = \\beta_0 + \\beta_1 X + \\beta_2 \\text{Strata}\\]\nThe problem with this is that the more strata we have, the more parameters we have, we can’t do inference because we’re not increasing the number of samples relative to the number of parameters that we have.\nInstead we’ll use conditional logistic regression to analyze matched case-control studies.\nA challenging question is in what situation conditioning doesn’t really change anything.\nThe full proof requires the principle of sufficiency, which will be seen in later classes.\nFor \\(K\\) matched sets, where \\(T_k\\) is the number of cases in set \\(k\\) and \\(N_k\\) is the number of observations in set \\(k\\). Let \\(X_{ki}\\) be th eexposure for observation \\(i\\) in set \\(k\\) and \\(y_{ki}\\) the outcome for observation \\(i\\) in set \\(k\\). Then the conditional likelihood is given as\n\\[\\mathcal L_C(\\beta_1) = \\prod_{k=1}^K \\frac{I(T_k = t_k) \\exp \\left( \\beta_1 \\sum_{i=1}^{N_k} y_{ki} X_{ki} \\right)}{\\sum_{u_k : T_k = t_k} \\exp \\left( \\beta_1 \\sum_{i=1}^{N_k} u_{ki} X_{ki} \\right)}\\]\nWe will show that for a 1:1 matched case-control study with a binary exposure and outcome, the conditional odds ratio is \\(\\frac{n_{10}}{n_{01}}\\)$ where \\(n_{10}\\) is the number of discordant pairs where the case is exposed and the control is not, and \\(n_{01}\\) is the number of discordant pairs where the case is unexposed and the control is exposed. \\[u_{k} = (u_{k1}, u_{k2}) = \\{ (0,1), (1, 0) \\}.\\]\n\\(u\\) encodes the sampling schema, so \\((0,1)\\) represents sampling the control first, then the case, and \\((1,0)\\) vice versa.\nDerive the conditional log-likelihood of \\(\\beta_1\\).\n\\[\\begin{aligned}\n\\mathcal L_C(\\beta_1) & = \\prod_{k=1}^K \\frac{\\exp \\left( \\beta_1 (1 \\cdot X_{k1} + 0 \\cdot X_{k1} \\right)}{\\exp \\left( \\beta_1 (0 \\cdot X_{k1} + 1 \\cdot X_{k2}) \\right) + \\exp \\left( \\beta_1(1 \\cdot X_{k1} + 0 \\cdot X_{k2})\\right)} \\\\\n& = \\prod_{k=1}^K \\frac{\\exp(\\beta_1 X_{k1})}{\\exp(\\beta_1 X_{k1}) + \\exp (\\beta_1 X_{k2})} \\\\\n\\ell_C(\\beta_1) & = \\sum_{k=1}^K \\log \\left( \\frac{\\exp(\\beta_1 X_{k1})}{\\exp(\\beta_1 X_{k1}) + \\exp (\\beta_1 X_{k2})} \\right) \\\\\n& = \\sum_{k=1}^K \\beta_1 X_{k1} - \\log(\\exp(\\beta_1 X_{k1}) + \\exp(\\beta_1 X_{k2})) \\\\\n& = \\sum_{k : X_{k1} = X_{k2}} \\underbrace{...}_{\\small =-\\log(2)} + \\sum_{k : X_{k1} = 1, X_{k2} = 0} \\underbrace{...}_{\\small =\\beta_1 - \\log(1 + \\exp(\\beta_1))} + \\sum_{k : X_{k1} = 0, X_{k2} = 1} \\underbrace{...}_{\\small -\\log(1 + \\exp(\\beta_1))} \\\\\n& = (n_{00} + n_{11})(-\\log 2) + n_{10}(\\beta_1 - \\log(1 + \\exp(\\beta_1))) + n_{01}(-\\log(1+\\exp(\\beta_1))) \\\\\n& = (n_{00} + n_{11})(-\\log 2) + n_{10} \\beta_1 - (n_{10} + n_{01})(\\log(1 + \\exp(\\beta_1)))\n\\end{aligned}\n\\]\nShow that \\(\\exp(\\hat \\beta_1) = \\frac{n_{10}}{n_{01}}.\\)$\nNow we just solve the score equation.\n\\[\\frac{d}{d \\beta_1} \\ell_C(\\beta_1) = n_{10} - (n_{10} + n_{01}) \\frac{\\exp(\\beta_1)}{1 + \\exp(\\beta_1)} \\stackrel{\\text{set}}{=} 0\\]\n\n\\[n_{10} + n_{10} \\exp(\\beta_1) = n_{10} \\exp(\\beta_1) + n_{01} \\exp(\\beta_1)\\] \\[\\Rightarrow \\exp (\\hat \\beta_1) = \\frac{n_{10}}{n_{01}}\\]\nFind the observed and estimated information \\(\\mathcal I (\\beta_1)\\) and \\(\\hat{\\mathcal I} (\\hat \\beta_1)\\):\n\\[\\mathcal I(\\beta_1) = -\\frac{\\partial^2}{\\partial \\beta_1^2} \\ell_C(\\beta_1) = (n_{10} + n_{01}) \\frac{\\exp(\\beta_1)}{(1+\\exp(\\beta_1))^2}\\]\n\\[\\hat{ \\mathcal I} (\\hat \\beta_1) = (n_{10} + n_{01}) \\frac{n_{10}/n_{01}}{(1+n_{10}/n_{01})^2} = \\frac{n_{10}n_{01}}{n_{10} + n_{01}} \\]"
  },
  {
    "objectID": "week15/week15.html",
    "href": "week15/week15.html",
    "title": "Week 15",
    "section": "",
    "text": "Poisson Count Outcomes"
  }
]