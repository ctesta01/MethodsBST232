[
  {
    "objectID": "week3/week3.html#variance-of-hat-beta",
    "href": "week3/week3.html#variance-of-hat-beta",
    "title": "Week 3",
    "section": "Variance of \\(\\hat \\beta\\)",
    "text": "Variance of \\(\\hat \\beta\\)\nThe variance of \\(\\hat \\beta\\) is expressed as the variance-covariance matrix\n\\[\\text{Var}(\\hat \\beta) = (X'X)^{-1} X' \\text{Var}(Y) X (X'X)^{-1}\\] \\[ = \\sigma^2 (X'X)^{-1}\\]\nIf we let \\(D = (X'X)^{-1}\\), the variance of \\(\\hat \\beta_j = \\sigma D_{jj}\\) and the covariance between \\(\\hat \\beta_i\\) and \\(\\hat \\beta_j\\) is \\(\\sigma^2 D_{ij}\\).\nHow would we get to this result?\nWe are using a shorthand where we denote \\((X'X)^TX' = A\\), and now we’re just looking at the \\(\\text{Var}(AY)\\). When we are working with the matrix-variance formula, we can rewrite \\(\\text{Var}(AY) = A\\text{Var}(Y)A'\\).\nPlugging in the formula for \\(A\\), we get to the above.\nRemember we said that \\(\\text{Var}(\\epsilon) = \\sigma^2\\) and \\(Y = X\\beta + \\epsilon\\) where the only randomness comes from \\(\\epsilon\\). In other words\n\\[\\text{Var}(\\hat \\beta) = (X'X)^{-1} X' \\text{Var}(Y) X (X'X)^{-1}\\] \\[ = (X'X)^{-1} X' \\text{Var}(X\\beta + \\epsilon) X (X'X)^{-1}\\] \\[ = (X'X)^{-1} X' \\text{Var}(\\epsilon) X (X'X)^{-1}\\] \\[ = (X'X)^{-1} X' \\sigma^2 I X (X'X)^{-1}\\] \\[ = \\sigma^2 (X'X)^{-1} X' I X (X'X)^{-1}\\] \\[ = \\sigma^2 \\cancel{(X'X)^{-1} X' X }\\underbrace{(X'X)^{-1}}_{\\stackrel{set}{=}D}.\\]\n\nGauss-Markov Theorem\n\nUnder the standard linear assumptions, \\(\\hat \\beta_{OLS}\\) is the best linear unbiased estimator (BLUE) for \\(\\beta\\).\nLinear unbiased estimator: \\(\\hat \\beta_{OLS}\\) is a linear combination of the observed \\(y\\) values (given that \\(\\hat \\beta = (X'X)^{-1}X'y\\) is a matrix of constants times a vector \\(y\\)) and is an unbiased estimator.\nIt’s “best” in the sense that it is the lowest variance (most precise).\n\nSo the Gauss Markov Theorem tells us that among all linear, unbiased estimators of \\(\\beta\\), \\(\\hat \\beta_{OLS}\\) has the lowest variance.\n\n\nSimple Linear Regression as a Special Case\nThe least squares estimators \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) can be expressed as\n\\[\\hat \\beta_0 = \\sum_{i=1}^n l_i y_i, \\quad \\hat \\beta_1 = \\sum_{i=1}^n k_i y_i,\\]\nwhere \\(l_i = \\frac{1}{n} - \\frac{\\bar x(x_i - \\bar x)}{\\sum_{i=1}^n (x_i - \\hat x)^2},\\) and \\(k_i = \\frac{(x_i - \\bar x)}{\\sum_{i=1}^n (x_i - \\bar x)^2}\\).\n\nVariance of LS Estimators\n\\[\\text{Var}(\\hat \\beta_0) = \\sigma \\left\\{ \\frac{1}{n} + \\frac{\\bar x^2}{\\sum_{i=1}^n (x_i-\\bar x)^2}\\right\\},\\] \\[\\text{Var}(\\hat \\beta_1) = \\sigma \\left\\{\\frac{1}{\\sum_{i=1}^n (x_i-\\bar x)^2}\\right\\},\\] \\[\\text{Cov}(\\hat \\beta_0, \\hat \\beta_1) = \\sigma \\left\\{ - \\frac{\\bar x}{\\sum_{i=1}^n (x_i-\\bar x)^2}\\right\\}.\\]\nThe variance-covariance matrix is\n\\[\\text{Var}(\\hat \\beta) = \\sigma(X'X)^{-1} = \\left[ \\begin{array}{cc} \\text{Var}(\\hat \\beta_0) & \\text{Cov}(\\hat \\beta_0, \\hat \\beta_1) \\\\ \\text{Cov}(\\hat \\beta_0, \\hat \\beta_1) & \\text{Var}(\\hat \\beta_1) \\end{array} \\right].\\]\n\n\nEstimation of \\(\\sigma^2\\)\nIn order to estimate \\(\\text{Var}(\\hat \\beta)\\), we need an estimator of \\(\\sigma^2\\):\nWe base this on the sum of squared errors (SSE):\n\\[SSE = (y - X\\hat\\beta)'(y-X\\hat \\beta)\\] \\[ = \\sum_{i=1}^n(y_i-x'_i\\hat\\beta)^2\\] \\[ = \\sum_{i=1}^n (\\hat \\epsilon_i)^2\\]\n\\[\\hat \\sigma^2 = MSE = \\frac{SSE}{n - p - 1}.\\]\nThis estimator \\(\\hat \\sigma^2\\) is an unbiased estimator.\nThe \\(n-p-1\\) in the denominator is because we estimate \\(p+1\\) parameters and we divide by the degrees of freedom, which is \\(n - \\text{\\# things we had to estimate}\\). The Kutner book has a more rigorous presentation of why this is the right amount to divide by.\n\n\n\nNormality assumption\nIf we are willing to make the stronger assumption that \\(\\epsilon_i \\stackrel{iid}{\\sim} \\mathcal N(0, \\sigma^2)\\), then we can perform inference on \\(\\beta\\).\nFirst note that \\(\\epsilon_i \\stackrel{iid}{\\sim} \\mathcal N(0, \\sigma^2) \\Longrightarrow Y_i \\stackrel{ind}{\\sim} \\mathcal N(x_i'\\beta, \\sigma^2)\\), such that\n\\[f_Y(y_i|\\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left[ -\\frac{1}{2\\sigma^2} (y_i - x_i'\\beta)^2\\right]\\]\nNotice that the \\(Y_i\\) values are independent but not identically distributed.\nBefore we were just assuming that the \\(\\epsilon\\) values were uncorrelated, which in the special case of the normal distribution implies independence, but this isn’t necessarily so for other distributions.\nWe can then use maximum likelihood techniques to obtain \\[\\hat \\beta_{MLE} \\sim MVN_{p+1} \\left[ \\beta, \\sigma^2 (X'X)^{-1} \\right].\\]\n\n\nJoint Density\nRecap of maximum likelihood estimation.\nIn general, suppose we have data \\(Y_1, ..., Y_n\\), which are independent random variables with \\(Y_i\\) having probability density function \\[f_Y(y_i|\\theta)\\] where \\(\\theta\\) is a vector of unknown parameters.\nThen the joint density function of all the \\(y_i\\) given \\(\\theta\\) is the product of the individual densities\n\\[f(y_1, ..., y_n|\\theta) = \\prod_{i=1}^n f_Y(y_i|\\theta).\\]\n\nLikelihood Functions\nThe likelihood function of \\(\\theta\\) given the data has the same form as the joint pdf:\n\\[\\mathcal L(\\theta|y_1,...,y_n) = f(y_1, ..., y_n|\\theta) = \\prod_{i=1}^n f_Y(y_i|\\theta).\\]\nOf course this looks exactly the same as the joint density of the \\(Y_i\\) values, but instead this is a function of \\(\\theta\\) instead of a function of the \\(y_i\\) values.\nOnce you take a random sample of size \\(n\\), the \\(y_i\\) values are known, and the likelihood is considered as a function of unknown parameter \\(\\theta\\).\nThe likelihood function should still integrate to 1.\nThe MLE of \\(\\theta\\) is the value \\(\\hat \\theta\\) that maximizes the likelihood\n\\[\\mathcal L(\\theta | y_1, ..., y_n)\\]\nas a function of \\(\\theta\\).\nThe value \\(\\hat \\theta\\) that maximizes \\(\\mathcal L(\\theta)\\) also maximizes\n\\[\\ell(\\theta | y_1, ..., y_n) = \\log \\mathcal L(\\theta | y_1, ..., y_n).\\]\n\n\nSolving for MLE\n\\[ \\frac{\\partial \\ell}{\\partial \\theta} \\stackrel{set}{=} 0,\\]\nand technically we’re going to need to check that this is a maximum as opposed to a minimum, and we’ll do so by checking that\n\\[\\left[ \\frac{\\partial^2 \\ell}{\\partial \\theta^2} \\right]_{\\theta = \\hat \\theta} < 0.\\]\nIf we were in a matrix setting instead of a vector setting, we’d need to check that the matrix is negative definite for a maximum, or positive definite for a minimum.\nThe negative of the second derivative,\n\\[\\frac{-\\partial^2 \\ell(\\theta | y_1, ..., y_n)}{\\partial \\theta^2},\\]\nis called the information.\n\n\nReturning to MLE for Regression\nThus in the linear regression setting if we assume \\(\\epsilon_i \\stackrel{iid}{\\sim} \\mathcal N(0, \\sigma^2),\\) then \\(Y_i \\stackrel{ind}{\\sim} \\mathcal N(x'_i\\beta, \\sigma^2)\\) and \\[f_Y(y_i|\\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left[ -\\frac{1}{2\\sigma^2} (y_i - x_i'\\beta)^2\\right]\\]\n\\[\\mathcal L(\\beta, \\sigma^2 | y_1, ..., y_n) = \\prod_{i=1}^n f_Y(y_i|\\beta, \\sigma^2),\\]\nand\n\\[\\mathcal L(\\beta, \\sigma^2|y_1, ..., y_n) = \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n\n\\exp \\left[ - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i'\\beta)^2 \\right]\\] \\[= \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n\n\\exp \\left[ - \\frac{1}{2\\sigma^2} (y - X\\beta)'(y-X\\beta) \\right].\\]\nTurning to the log-likelihood function:\n\\[\\ell(\\beta, \\sigma^2 | y_1, ..., y_n) \\propto \\cancel{-n/2\\log(\\sigma^2)} \\underbrace{- \\frac{1}{2\\sigma^2} (y-X\\beta)'(y-X\\beta)}_{= \\frac{-1}{2\\sigma^2} S(\\beta)}.\\]\nThe values that maximize this log-likelihood with respect to \\(\\beta\\), call them \\(\\hat \\beta_{MLE}\\) are the same as those that minimize \\(S(\\hat \\beta)\\), i.e.,\n\\[\\hat \\beta_{MLE} = (X'X)^{-1}X'y\\]\nand it’s straightforward to show that\n\\[\\hat \\beta_{MLE} \\sim MVN_{p+1}\\left[ \\beta, \\sigma^2(X'X)^{-1} \\right].\\]\n\n\n\\(\\sigma^2_{MLE}\\)\nWhile the estimates for \\(\\hat \\beta\\) are the same for OLS vs. MLE, we have that \\[\\hat \\sigma^2_{MLE} = \\frac{1}{n}(y-X\\hat\\beta)'(y-X\\hat\\beta) = \\frac{(n-p-1)}{n}MSE\\]\nSo of note, the MLE for \\(\\beta\\) are the same as the least squares estimator. However the MLE for \\(\\sigma^2\\) is not.\nRecall that the least squares estimator of \\(\\sigma^2\\) is unbiased. The MLE of \\(\\sigma^2\\) is biased, although it is consistent: \\[\\lim_{n\\to\\infty} P(|\\hat\\sigma^2 - \\sigma^2| \\leq \\epsilon) \\to 1, \\, \\forall \\epsilon > 0.\\]"
  },
  {
    "objectID": "week3/week3.html#inference-in-linear-regression",
    "href": "week3/week3.html#inference-in-linear-regression",
    "title": "Week 3",
    "section": "Inference in Linear Regression",
    "text": "Inference in Linear Regression\nOften it’s of interest to determine if, collectively, a group of predictors significantly contribute to the variability in \\(y\\) given another group of predictors are in the model.\nCommon examples are:\n\nIs a categorical variable, represented by dummy variables, significant (analagous to the overall ANOVA F-test)?\nCan the effect of a predictor be represented as a linear effect or is a higher-level polynomial (i.e., using \\(x^2\\), \\(x^3\\), etc.) necessary?\nIs a model that contains only main effects adequate or do we need to incorporate a set of interactions between variables in the models?\n\n\nSum of squares decomposition\n\\[(y_i - \\bar y)^2 = ((y_i - \\hat y_i) + (\\hat y_i - \\bar y))^2\\]\nThen, when computing the sums of squares, we get\n\\[\\sum(y_i - \\bar y)^2 = \\sum_{i=1}^n (\\hat y_i - \\bar y)^2 + \\sum_{i=1}^n (y_i - \\hat y_i)^2,\\]\nwhich happily features a “freshman’s dream”.\nWe thus have that \\[SST = \\underbrace{SSR}_{\\text{explained by regression}} + \\underbrace{SSE}_{\\text{left over}},\\]\nwhere \\(SST = \\text{Sums of Squares Total}\\), \\(SSR = \\text{Sums of Squares Regression}\\), and \\(SSE = \\text{Sums of Squares Error}\\).\n\n\n\n\n\n\n\n\n\n\nThe ANOVA-like table\nWe often will write something like this type of table:\n\n\n\n\n\n\n\n\n\n\nSource\n\\(SS\\)\n\\(\\text{df}\\)\n\\(\\text{MS}\\)\n\\(\\mathbb E[\\text{MS}]\\)\n\n\n\n\nRegression\n\\(SSR = \\hat \\beta'X'y-n\\bar y^2\\)\n\\(p\\)\n\\(\\frac{SSR}{p}\\)\n\\(\\sigma^2 + \\frac{\\beta'_Rx'_Cx_C\\beta_R}{p}\\)\n\n\nError\n\\(SSE = y'y - \\hat \\beta' X'y\\)\n\\(n-(p+1)\\)\n\\(\\frac{SSE}{n-(p+1)}\\)\n\\(\\sigma^2\\)\n\n\nTotal\n\\(SST=y'y - n \\bar y^2\\)\n\\(n-1\\)\n\n\n\n\n\nwhere \\(MS = \\text{Mean Square Error}\\), and \\[X_c = \\left( \\begin{array}{cccc}\nx_{11}-\\bar{x_1} & x_{12}- \\bar{x_2} & \\cdots & x_{1p}-\\bar{x_p} \\\\\nx_{21}-\\bar{x_1} & x_{22}- \\bar{x_2} & \\cdots & x_{2p}-\\bar{x_p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1}-\\bar{x_1} & x_{n2}- \\bar{x_2} & \\cdots & x_{np}-\\bar{x_p}\n\\end{array}\\right)\\]\n\n\nTesting for Groups of Predictors\nHow do we use this decomposition to test for a group of coefficients?\nThe hypothesis can be formulated as\n\\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_q = 0, q \\leq p\\] \\[H_1: \\text{ at least one of } \\beta_1, ..., \\beta_q \\neq 0.\\]\nAs an aside, tests of the overall regression and tests for a single variable fall within this framework as well:\nThe overall test:\n\\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_p = 0\\] \\[H_1: \\beta_j \\neq 0 \\text{ for at least one } j, j = 1,...,p\\]\nFor a single predictor:\n\\[H_0: \\beta_j = 0\\] \\[H_1: \\beta_j \\neq 0\\]\nWe like the property that testing for significance among a “group of coefficients” reduces in two special cases to either the overall test or a test for an individual coefficient.\nIf we consider the model in matrix form:\n\\[Y = X\\beta + \\epsilon,\\]\nto construct a test based on sums of squares, partition \\(\\beta\\) accordingly:\n\\[\\beta = (\\beta_1^, \\beta_2')',\\]\nwhere \\(\\beta_1\\) is a \\(q \\times 1\\) and \\(\\beta_2\\) is \\((p+1-q) \\times 1\\). We want to test the null hypothesis \\[H_0: \\beta_1 = 0\\] \\[H_1: \\beta_1 \\neq 1\\] and \\(\\beta_2\\) is left unspecified.\nDefining \\(X = \\left[ X_1, X_2 \\right]\\), rewrite the model as\n\\[Y = X_1 \\beta_1 + X_2 + \\beta_2 + \\epsilon.\\]\nNow our model is partitioned so we’re ready to test for significance among the predictors and \\(\\beta\\) coefficients of interest.\nThe full model has SSR expressed as\n\\[SSR(X) = \\hat \\beta' X' y - n \\bar y^2\\]\nand Mean Square Error\n\\[MSE(X) = \\frac{y'y - \\hat \\beta' X' y}{n-p-1}.\\]\nTo find the contribution of \\(X_1\\), fit the model assuming \\(H_0\\) is true. The reduced model is \\[Y = X_2 \\beta_2 + \\epsilon,\\] which yields \\[\\hat \\beta_2 = (X_2'X_2)^{-1}X_2'y \\quad \\text{ and } \\quad SSR(X_2) = \\hat \\beta_2' X_2' y - n' \\bar y^2.\\]\nThe regression sums of squares due to \\(X_1\\) given \\(X_2\\) is in the model is\n\\[SSR(X_1|X_2) = SSR(X) - SSR(X_2)\\]\nwith \\(q\\) degrees of freedom. This is known as the extra sum of squares due to \\(X_1\\) given \\(X_2\\).\nUnder the null hypothesis,\n\\(SSR(X_1|X_2)/\\sigma^2 \\sim \\chi_q^2\\) and \\(SSE/\\sigma^2 \\sim \\chi_{(n-p-1)}^2\\), and these quantities are independent.\nIn general, if one \\(\\chi^2\\) distribution has degrees of freedom \\(d_1\\) and another has \\(d_2\\), then \\((\\chi_{d_1}^2/d_2)/(\\chi_{d_2}^2/d_2) \\sim F_{d_1,d_2}\\) if the two are independent.\nSo we can test \\(H_0: \\beta_1 = 0\\) with the statistic \\[F = \\frac{SSR(X_1|X_2)/q}{MSE(X)} \\stackrel{H_0}{\\sim} F_{q,n-p-1}\\]\nThis \\(F\\) distributional result requires either\n\nnormality of errors \\(\\epsilon_i \\sim \\mathcal N(0,\\sigma^2)\\)\nlarge sample theory\n\nThere’s a handful of things above that we just have to take for granted assumed from a probability & inference class and don’t have time to re-prove here.\nThe \\(F\\) written above is an \\(F\\)-statistic (or \\(F\\)-distributed) because it is the quotient of two \\(\\chi^2\\)-distributed variables divided by their degrees of freedom.\nWith reasonably large sample size, \\(\\mathbb E[F_{q,n-p-1}] \\approx 1\\).\nFor example, one can see that if one runs the regressions:\n\nlm(data = mtcars, hp ~ rnorm(n = nrow(mtcars)))\nsummary(.Last.value)\n#> ... \n#> F-statistic: 0.06081 on 1 and 30 DF\n\nlm(data = mtcars, hp ~ mpg)\n#> ... \n#> F-statistic: 45.46 on 1 and 30 DF\n\nWe can think of this procedure as asking: Is the increase in the regression sums of squares associated with adding \\(q\\) additional predictors, given the presence of the remaining variables in the model, sufficient to warrant removing \\(q\\) additional degrees of freedom from the denominator of MSE?\nAdding an unimportant predictor may increase the MSE, which will increase the uncertainty in the regression coefficient estimates and the variance of \\(\\hat y\\) - so we should include only predictors that explain the response.\nNote however that for the purpose of explanation confounders may not reach significance at given level (e.g. \\(\\alpha = 0.05\\)) but still have a clinically relevant effect on both outcome and exposure and therefore affect the regression coefficients of interest.\n\n\nExample: Test for 2 BMI Terms, HERS Data\n\nlibrary(gt)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nhers <- readr::read_csv(here::here(\"data/hers.csv\"))\n\nRows: 2763 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): HT, raceth, nonwhite, smoking, drinkany, exercise, physact, globra...\ndbl (24): age, medcond, weight, BMI, waist, WHR, glucose, weight1, BMI1, wai...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhers$BMIc <- hers$BMI - mean(hers$BMI, na.rm=TRUE)\n\nlm.ldl.interact <- \n  lm(data = hers %>% filter(! is.na(BMIc)), LDL ~ BMIc*statins + age + nonwhite + drinkany + smoking)\n\nlm.ldl.noBMI <- \n  lm(data = hers %>% filter(! is.na(BMIc)), LDL ~ statins + age + nonwhite + drinkany + smoking)\n\n# perform f-test using anova(reducedModel, fullModel)\nbmi.test <- broom::tidy(anova(lm.ldl.noBMI, lm.ldl.interact))\n\n## format and print results table \ngt(bmi.test) %>% \n  tab_header(title = md(\"**Test of significance of BMI**\"),\n             subtitle = md(\"From LDL model with BMI * statin interaction\")) %>% \n  cols_width(term ~ px(375)) %>% sub_missing(missing_text = '') %>% \n  fmt_number(columns=c('statistic','p.value'),decimals=3) %>% \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Test of significance of BMI\n    \n    \n      From LDL model with BMI * statin interaction\n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LDL ~ statins + age + nonwhite + drinkany + smoking\n2739\n3725955\n\n\n\n\n    LDL ~ BMIc * statins + age + nonwhite + drinkany + smoking\n2737\n3707501\n2\n18454.31\n6.812\n0.001\n  \n  \n  \n\n\n\n\n\n\n\nOverall Test\nUnder the null hypothesis, \\(SSR/\\sigma^2 \\sim \\chi^2_p\\) and \\(SSE/\\sigma^2 \\sim \\chi^2_{n-(p+1)}\\) are independent.\nTherefore we have \\[F = \\frac{SSR/p}{SSE/[n-(p+1)]} = \\frac{MSR}{MSE} \\stackrel{H_0}{\\sim} F_{p,n-p-1}\\]\nWe note that this is reported automatically in a lm().\n\noverall.test <- broom::tidy(anova(lm(data = hers, LDL ~ BMI + age)))\n\ngt(overall.test) %>% \n  tab_header(title = md(\"**Overall test**\"),\n             subtitle = md(\"Model of LDL with BMI and age\")) %>% \n  sub_missing(missing_text = '') %>% \n  fmt_number(columns = c('statistic', 'p.value'), decimals = 3) %>% \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n      Overall test\n    \n    \n      Model of LDL with BMI and age\n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    BMI\n1\n14446.022\n14446.022\n10.155\n0.001\n    age\n1\n7567.195\n7567.195\n5.320\n0.021\n    Residuals\n2744\n3903361.455\n1422.508\n\n\n  \n  \n  \n\n\n\n\nWe can interpret the entries above in the sumsq column as \\(SSR(BMI)\\) and then \\(SSR(age|BMI)\\). These are called the “extra sums of squares” contributed by each variable, and sometimes called the “type 1 sums of squares” (no relation to Type 1 error, but more of a historical idiosyncrasy as a result of how old software [either SAS or S or S-plus] printed these out).\n\\[F = \\frac{(14446 + 7567)/2}{3903361/2744} = 7.74\\]\nUnder \\(H_0\\), \\(F \\sim F_{2, 2744}\\), yielding \\(p = 0.0004458\\).\nWe reject the null hypothesis at \\(\\alpha = 0.05\\) and conclude that at least one of \\(\\beta_1\\) or \\(\\beta_2\\) is not equal to zero.\nOne should note that the above table is one of the places in which order matters because each \\(SSR\\) is conditional on the inclusion of the previously listed variables."
  },
  {
    "objectID": "week3/week3.html#wald-tests",
    "href": "week3/week3.html#wald-tests",
    "title": "Week 3",
    "section": "Wald Tests",
    "text": "Wald Tests\nFor testing individual coefficients \\((H_0: \\beta_j = 0\\) vs \\(H_1: \\beta_j \\neq 0\\)) we can also use the conventional Wald test. To construct the test statistic, consider that\n\\[\\hat \\beta_j \\sim \\mathcal N(\\beta_j, \\sigma^2) D_{jj} \\quad \\text{ and } \\quad\n\\frac{\\hat{\\text{Var}} (\\hat \\beta_j)}{\\sigma^2 D_{jj}} \\sim \\frac{\\chi^2_{n-p-1}}{(n-p-1)}.\\]\nNote that if \\(Z \\sim \\mathcal N(0,1)\\) and \\(S \\sim \\chi^2_d\\) and \\(Z \\perp\\!\\!\\!\\perp S\\) then \\(\\frac{Z}{\\sqrt{S/d}} \\sim t_d\\).\n\\[\\left( \\frac{\\hat \\beta_j - \\beta_j}{\\sqrt{\\sigma^2 D_{jj}}} \\right) \\biggr /\n\\left( \\sqrt{\\frac{\\widehat{\\text{Var}}(\\hat \\beta_j)}{\\sigma^2D_{jj}}} \\right) = \\underbrace{\\boxed{\\frac{\\hat \\beta_j - \\beta_j}{\\sqrt{\\widehat{\\text{Var}}(\\hat \\beta_j)}}}}_{\\text{this should look like a t-statistic}} \\stackrel{H_0}{\\sim} t_{n-p-1}\\]\nIt should be noted that a \\(t^2\\) value where \\(t\\) is a \\(t\\)-statistic follows an \\(F\\)-distribution. This implies that in the case of testing a single coefficient, the \\(t\\)-test and the \\(F\\)-test give the exact same results.\nIn the summary() function, the \\(p\\)-values shown will be from \\(t\\)-tests for each \\(\\beta_j\\), while the \\(F\\)-statistic shown is for the overall model.\n\\[E(LDL_i) = \\beta_0 + \\beta_1 BMI_i + \\beta_2 Age_i\\]\n\nwald.test <- broom::tidy(lm(data = hers, LDL ~ BMI + age))\ngt(wald.test) |> \n  tab_header(title = \n               md(\"**Individual coefficient Wald test**\"),\n             subtitle = \"Test of BMI in model of LDL with age already included\") |> \n  fmt_number(decimals = 3) |> \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n      Individual coefficient Wald test\n    \n    \n      Test of BMI in model of LDL with age already included\n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n151.443\n8.774\n17.260\n0.000\n    BMI\n0.367\n0.132\n2.778\n0.006\n    age\n−0.253\n0.110\n−2.306\n0.021\n  \n  \n  \n\n\n\n\n\\(T = 0.366/0.132 = 2.78 \\quad p = 0.0006\\)\nThis Wald testing strategy extends to testing groups of cofficients\n\\[Y = X_1 \\beta_1 + X_2 \\beta_2 + \\epsilon\\]\nwhere \\(\\beta_1\\) is \\(q \\times 1\\) and \\(\\beta_2\\) is \\((p + 1 - q) \\times 1\\).\n\\[H_0: \\beta_1 = 0\\] \\[H_1: \\beta_1 \\neq 0\\]\nThe multivariate Wald test statistic is \\[W = \\hat \\beta_1' \\left[ \\widehat{\\text{Var}}(\\hat \\beta_1 ) \\right]^{-1}  \\hat \\beta_1\\]\nUnder the null,\n\n\\((1/q)W \\sim F_{q,n-p-1}\\)\nAsymptotically, \\(W \\sim \\chi_q^2\\)\n\n\nwald.test.group <- broom::tidy(lm.ldl.interact)\n\ngt(wald.test.group) |> \n  tab_header(title = \n               md(\"**LDL model with BMI * statin interaction**\")) |> \n  fmt_number(decimals = 3) |> \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n      LDL model with BMI * statin interaction\n    \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n162.405\n7.583\n21.416\n0.000\n    BMIc\n0.582\n0.160\n3.636\n0.000\n    statinsyes\n−16.253\n1.469\n−11.066\n0.000\n    age\n−0.173\n0.111\n−1.563\n0.118\n    nonwhiteyes\n4.073\n2.275\n1.790\n0.074\n    drinkanyyes\n−2.075\n1.467\n−1.415\n0.157\n    smokingyes\n3.110\n2.167\n1.435\n0.151\n    BMIc:statinsyes\n−0.702\n0.269\n−2.606\n0.009\n  \n  \n  \n\n\n\n\nIn this scenario, \\(H_0: \\beta_2 - \\beta_8 = 0\\).\n\n## Generic function for a Wald test from the output of lm()\n\nwaldTest <- function(fit, vec, digits=c(2,4)) {\n  \n  beta     <- coef(fit)[vec]\n  varMat   <- summary(fit)$cov.unscaled[vec,vec] * (summary(fit)$sigma^2)\n  testStat <- t(beta) %*% solve(varMat) %*% beta \n  pVal     <- 1 - pchisq(testStat, length(vec))\n  value    <- c(Fstat = round(testStat, digits=digits[1]),\n             p = round(pVal, digits=digits[2]))\n  return(value)\n}\n\nwaldTest(lm.ldl.interact, vec = c(2,8))\n\n  Fstat       p \n13.6200  0.0011"
  },
  {
    "objectID": "week3/week3.html#testing-general-linear-hypotheses",
    "href": "week3/week3.html#testing-general-linear-hypotheses",
    "title": "Week 3",
    "section": "Testing general linear hypotheses",
    "text": "Testing general linear hypotheses\nSuppose we are interested in testing linear combinations of the regression coefficients. For example, we may be interested in testing\n\\[H_0: \\beta_i = \\beta_j\\]\nequivalently \\(H_0: \\beta_i - \\beta_j = 0\\).\nSuch hypotheses can be expressed as \\(H_0: C \\beta = 0\\).\nWhere \\(C\\) is an \\(r \\times (p+1)\\) matrix of linearly independent contrasts with \\(r\\) the number of restrictions imposed by the null.\nFor example, consider the model \\[Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i3} + \\epsilon_i,\\]\nand testing the hypothesis \\[H_i : \\beta_1 = 0, \\beta_2 = \\beta_3\\]\nThis could also be written as \\[\\left( \\begin{array}{c} \\beta_1 \\\\ \\beta_2 - \\beta_3 \\end{array} \\right) =\n\\left( \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right)\\]\nThis null hypothesis is equivalent to \\[H_0: \\left( \\begin{array}{cccc} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & -1 \\end{array} \\right)\\beta = 0\\] were \\(\\beta = (\\beta_0, \\beta_1, \\beta_2, \\beta_3)'\\).\nWe can obtain the reduced model by solving \\(C\\beta\\) for \\(r\\) of the regression coefficients in terms of the remaining \\(p+1-r\\) regression coefficients. Substituting these values into the full model will yield a reduced model under the null hypothesis,\n\\[Y = Z \\gamma + \\epsilon,\\]\nwhere \\(\\dim(Z) = n \\times (p+1 -r)\\) matrix and \\(\\dim(\\gamma) = (p + 1 - r) \\times 1\\) vector of regression coefficients.\nThe residual SS for this reduced model is \\[SSE(RM) = y'y - \\hat\\gamma' Z'y \\quad \\quad (n - p - 1 + r \\, \\text{ degrees of freedom})\\]\n\\(SSR(\\text{Full Model}) - SSR(\\text{Reduced Model})\\) is called the sum of squares due to the hypothesis \\(C\\beta=0\\).\nWe can test this hypothesis using \\[F = \\frac{(SSR(FM)-SSR(RM))/r}{MSE} \\stackrel{H_0}{\\sim} F_{r,n-p-1}.\\]\n\nExample with HERS data\nConsider using the physical activity score (1-5):\n\n\n\nPhysact\nActivity\n\n\n\n\n1\nMuch less active\n\n\n2\nSomewhat less active\n\n\n3\nAbout as active\n\n\n4\nSomewhat more active\n\n\n5\nMuch more active\n\n\n\nAn ANOVA model for glucose level regressed on physical activity is\n\\[E(glucose_i) = \\beta_0 + \\beta_1D_{i1} + \\beta_2D_{i2} + \\beta_3D_{i3} + \\beta_4 D_{i4}\\]\nQuestion: For the purposes of predicting glucose level, is the cruder physical activity categorization below adequate?\n\n\n\nCollapsed Physact\nActivity\n\n\n\n\n1\nLess active\n\n\n2\nAbout as active\n\n\n3\nMore active\n\n\n\nRecall the full model is\n\\[E(glucose_i) = \\beta_0 + \\beta_1D_{i1} + \\beta_2D_{i2} + \\beta_3D_{i3} + \\beta_4 D_{i4}\\]\nand this question corresponds to the null hypothesis .\nRemember that we can also write \\(H_0\\) as \\(\\beta_1 - \\beta_2 = 0, \\beta_4 = 0\\). We can think about this as having solved for \\(\\beta_1\\) in terms of \\(\\beta_2\\) or vice-versa.\n\\[H_0: \\beta_1 = \\beta_2, \\beta_4 = 0\\]\nand the reduced model \\[E(glucose_i) = \\beta_0 + \\beta_1(D_{i1} + D_{i2}) + \\beta_3D_{i3}\\]\n\nlm.glucose.pa <- lm(glucose ~ factor(physact), data = hers)\npa.test.fine <- broom::tidy(anova(lm.glucose.pa))\n\ngt(pa.test.fine) |> \n   tab_header(title = \n               md(\"**Overall test of 5-level physical activity**\"),\n                  subtitle = \"Model of glucose with 5 PA categories\") |> \n  fmt_number(decimals = 1) |> \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n      Overall test of 5-level physical activity\n    \n    \n      Model of glucose with 5 PA categories\n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    factor(physact)\n4.0\n87,696.5\n21,924.1\n16.5\n0.0\n    Residuals\n2,758.0\n3,662,765.0\n1,328.1\nNA\nNA\n  \n  \n  \n\n\n\nhers$collapsed_physact <- \n  case_when(hers$physact %in% c(\"much less active\", 'somewhat less active') ~ 'less',\n            hers$physact %in% c(\"much more active\", 'somewhat more active') ~ 'more',\n            TRUE ~ hers$physact)\n\nlm.glucose.pacoarse <- lm(glucose ~ factor(collapsed_physact), data = hers)\n\npa.test.coarse <- broom::tidy(anova(lm.glucose.pacoarse))\n\ngt(pa.test.coarse) |> \n  tab_header(\n    title = md(\"**Overall test of 3-level physical activity**\"),\n    subtitle = \"Model of glucose with 3 PA categories\") |> \n  fmt_number(columns = df:meansq,\n             decimals=0) |> \n  fmt_number(columns = statistic:p.value,\n             decimals=2) |> \n  tab_options(table.align = 'left')\n\n\n\n\n\n  \n    \n      Overall test of 3-level physical activity\n    \n    \n      Model of glucose with 3 PA categories\n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    factor(collapsed_physact)\n2\n76,501\n38,250\n28.73\n0.00\n    Residuals\n2,760\n3,673,961\n1,331\nNA\nNA\n  \n  \n  \n\n\n\n\nOur \\(F\\)-test then becomes:\n\\[ \\frac{(87,697 - 76,501)/2}{1330} = 4.21 \\stackrel{H_0}{=} F_{2,2760} \\, \\, (p = 0.0149)\\]\nHow would we get that p-value?\nIn our case we can run:\n\nF_stat <- ((pa.test.fine$sumsq[[1]]-pa.test.coarse$sumsq[[1]])/2) / \n  pa.test.fine$meansq[[2]]\n\nprint(F_stat)\n\n[1] 4.215159\n\npf( # the distribution function (cdf) of the F distribution \n  q = F_stat,\n  df1 = 2,\n  df2 = pa.test.fine$df[[2]],\n  lower.tail = FALSE)\n\n[1] 0.01486524\n\n# compare to the anova table p-value\nanova(lm.glucose.pacoarse, lm.glucose.pa) |> \n  broom::tidy() |> \n  gt() |> \n  tab_header(\n    title = md(\"**ANOVA table comparing the 5-level to 3-level model**\"),\n    subtitle = \"Glucose regressed on physical activity\") |> \n  fmt_number(\n    columns = df.residual:df,\n    decimals = 0) |> \n  fmt_number(\n    columns = statistic:`p.value`,\n    decimals = 3\n    )\n\n\n\n\n\n  \n    \n      ANOVA table comparing the 5-level to 3-level model\n    \n    \n      Glucose regressed on physical activity\n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    glucose ~ factor(collapsed_physact)\n2,760\n3,673,961\nNA\nNA\nNA\nNA\n    glucose ~ factor(physact)\n2,758\n3,662,765\n2\n11195.89\n4.215\n0.015\n  \n  \n  \n\n\n\n\n\nWhat is the multivariate Wald test for a general linear hypothesis?\n\\[H_0 : C\\beta = 0\\]\nAnd thus \\[W = (C\\hat \\beta)'(\\widehat{\\text{Var}}(C\\hat\\beta))^{-1}(C\\beta)\\] \\[ = (C\\hat\\beta)'[C \\widehat{\\text{Var}}(\\hat\\beta) C']^{-1}(C\\hat\\beta)\\] \\[ = (C\\hat\\beta)'[C \\hat \\sigma^2 (X'X)^{-1} C']^{-1} (C\\hat\\beta)\\]\nand \\(W/r \\sim F_{r,n-p-1}\\) or asymptotically \\(W \\sim \\chi^2_r\\)."
  },
  {
    "objectID": "week3/week3.html#confidence-intervals",
    "href": "week3/week3.html#confidence-intervals",
    "title": "Week 3",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nRecall that often we obtain CIs by inverting test statistics.\nThus we can construct a confidence interval for \\(\\beta_j\\) by inverting the univariate \\(t\\)-test.\nFirst, letting \\(c\\) denote \\(t_{n-p-1,1-\\alpha/2}\\), note that \\[P(-c < \\frac{\\hat \\beta_j - \\beta_j}{\\sigma(\\hat \\beta_j)} < c) = 0.95\\]\n\\[ \\Longrightarrow (\\hat \\beta_j - c \\times \\sigma(\\hat \\beta_j) < \\beta_j < \\hat \\beta_j + c \\times \\sigma(\\hat \\beta_j)) = 0.95\\]\n\\[\\hat \\beta_j \\pm t_{n - p -1, 1 - \\alpha/2} \\sqrt{\\hat \\sigma^2 D_{jj}}\\]"
  },
  {
    "objectID": "week3/week3.html#model-estimated-expected-value",
    "href": "week3/week3.html#model-estimated-expected-value",
    "title": "Week 3",
    "section": "Model Estimated Expected Value",
    "text": "Model Estimated Expected Value\nA 100(1-\\(\\alpha)\\)% CI for \\(\\mu(x) = x'\\beta\\) is \\[\\hat \\mu(x) \\pm t_{n-p-1, 1-\\alpha/2} \\sqrt{\\hat \\sigma^2 x' (X' X)^{-1} x}\\]\n\nPrediction Intervals\nA 100(1-\\(\\alpha\\))% prediction interval for a single new observation with covariate values \\(x_{new}\\) is constructed by noting that\n\\[y_{new} = x_{new}' \\beta  + \\varepsilon_{new}\\]\nand \\(\\text{Var}(\\hat y_{new}) = \\sigma^2 x_{new}' (X'X)^{-1}x_{new} + \\sigma^2\\). Then\n\\[\\hat y_{new} \\pm t_{n-p-1, 1 -\\alpha} \\sqrt{ \\hat \\sigma^2 \\left(1 + x_{new}' (X'X)^{-1} x_{new}\\right)}\\]\nwhere \\(\\hat y_{new} = x_{new}' \\hat \\beta\\).\nThus the predictions for \\(y_{new}\\) have have uncertainty both from the estimate of \\(\\hat \\beta\\) and the estimated error variance \\(\\hat \\sigma^2\\).\n\nlm.sbp.age <- lm(SBP ~ age, data = hers)\n\npred_with_ci <- predict(lm.sbp.age, interval = 'confidence')\npred_with_pi <- predict(lm.sbp.age, interval = 'prediction')\n\nWarning in predict.lm(lm.sbp.age, interval = \"prediction\"): predictions on current data refer to _future_ responses\n\nintervals <- data.frame(hers$age, pred_with_ci, pred_with_pi[,2:3])\n\nnames(intervals) <- c('age', 'yhat', 'lwr_ci', 'upr_ci',\n                      'lwr_pi', 'upr_pi')\n\nintervals <- intervals %>% arrange(age)\n\nggplot(intervals, aes(age)) + \n  geom_ribbon(aes(ymin= lwr_pi, ymax = upr_pi, fill = 'prediction interval'), alpha = 0.5) + \n  geom_ribbon(aes(ymin= lwr_ci, ymax = upr_ci, fill = 'confidence interval'), alpha = 0.8) + \n  geom_line(aes(y = yhat)) + \n  geom_jitter(data = hers, aes(age, SBP), size = .75, width = .5, height = 0, alpha = 0.15) + \n  scale_fill_manual(values = c('prediction interval' = 'orange', 'confidence interval' = 'cornflowerblue')) + \n  theme_bw() +\n  labs(y = 'SBP', fill = 'interval type') + \n  ggtitle(\"Regression Prediction and Confidence Intervals\") + \n  theme(legend.position = 'bottom')\n\n\n\n\nNote that we have assumed \\(\\varepsilon \\sim \\mathcal N(0, \\sigma^2)\\) to construct the prediction interval. If the error terms are not close to normal, then the prediction interval could be misleading. This is not the case for the interval for the expected value, which only requires approximate normality for \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\)."
  },
  {
    "objectID": "week3/week3.html#r2-and-adjusted-r2",
    "href": "week3/week3.html#r2-and-adjusted-r2",
    "title": "Week 3",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\\[R^2 = 1 - \\frac{SSE}{SST} = \\frac{SSR}{SST}\\]\nThe proportion of the total variation in \\(Y_i\\) explained by \\(X_i\\).\nBecause \\(0 \\leq SSE \\leq SST\\), \\(0 \\leq R^2 \\leq 1\\).\n\\(R^2\\) increases whenever new terms are added to the model.\nTherefore for model comparison, most people often use a version of the \\(R^2\\) that is adjusted for the number of predictors in the model. This is the adjusted \\(R^2\\), defined as \\[R^2 = 1 - \\left( \\frac{n-1}{\\text{Error df}} \\right) \\frac{SSE}{SST}  = 1 - \\frac{MSE}{SST/(n-1)}\\]\nUsing the MSE is essentially a penalization on wasting unused parameters since \\[MSE = \\frac{\\sum_{i=1}^n (x_i - \\bar x_i)^2}{n - p - 1}.\\]"
  },
  {
    "objectID": "week3/week3.html",
    "href": "week3/week3.html",
    "title": "Week 3",
    "section": "",
    "text": "Lab\nOne perspective is that statistics is really only good for two things:"
  },
  {
    "objectID": "week3/week3.html#variance-estimates-to-confidence-intervals",
    "href": "week3/week3.html#variance-estimates-to-confidence-intervals",
    "title": "Week 3",
    "section": "Variance estimates to confidence intervals",
    "text": "Variance estimates to confidence intervals\nWe know that if \\(Z\\) is a random vector and \\(A\\) is a matrix of constants, then\n\\[\\text{Var}(AZ) = A \\text{Var}(Z) A^T.\\]\nThis implies that\n\\[\\text{Var}(\\hat \\beta) = \\text{Var}[(X^TX)^{-1} X^TY] = \\sigma^2 (X^TX)^{-1}\\]\nThis is a quadratic form. If we were in a scalar world with a variable \\(a \\in \\mathbb R\\) and \\(z\\) a random variable. Then \\(\\text{Var}(az) = a^2 \\text{Var}(z)\\). When we work with matrices, the square of a matrix is analogous to writing \\(AA^T\\)\nRemember that \\(\\hat \\beta = \\underbrace{(X^TX)^{-1} X^T}y\\).\n\\[\\text{Var}(\\beta) = \\left( \\begin{array}{cccc}\n\\text{Var}(\\beta_1) & \\text{Cov}(\\beta_1,\\beta_2) & ... & ... \\\\\n\\text{Cov}(\\beta_2, \\beta_1) & \\text{Var}(\\beta_2) & ... & ... \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\n\\end{array}\\right).\\]\n\\[95\\%CI(\\hat \\beta_j) = \\hat \\beta_j \\pm t_{n-p-1, 1-\\alpha/2} \\hat \\sigma \\sqrt{[(X^TX)^{-1}]_{j,j}}.\\]\nBut we can use the same idea to obtain variance and CI estimates for the prediction of a new/future observation \\(x_{new}\\). Given \\(x_{new}\\) we can predict their mean response \\(\\mathbb E(\\hat y|x_{new}) = x^T_{new}\\hat \\beta\\).\nNote that we cannot predict the response itself since we don’t know what \\(\\varepsilon_{new}\\) is.\nWhen it comes to confidence intervals, we can indeed get two different types of intervals.\n\\[\\text{Predicted mean response: } \\quad \\text{Var}(\\hat{\\mathbb E}(\\hat y|x_{new})) =\n\\text{Var}(x_{new}^T \\hat \\beta) = \\sigma^2 x^T_{new} (X^TX)^{-1} x_{new},\\]\nwhich gives the 95% confidence interval:\n\\[x_{new}^T \\hat \\beta \\pm t_{n-p-1,1-\\alpha/2} \\hat \\sigma \\sqrt{x_{new}^T (X^TX)^{-1} x_{new}}.\\]\n\\[\\text{The predicted response: }\n\\quad \\text{Var}(\\hat y|x_{new}) = \\text{Var}(x_{new}^T \\hat \\beta + \\varepsilon_{new}) = \\sigma^2 x_{new}^T (X^TX)^{-1}x_{new} + \\sigma^2,\\]\nwhich gives the 95% prediction interval:\n\\[x_{new}^T \\hat \\beta \\pm t_{n-p-1, 1-\\alpha/2} \\hat \\sigma \\sqrt{1 + x_{new}^T(X^TX)^{-1}x_{new}}.\\]\nIn general, we can’t predict \\(y_new = \\hat \\beta_0 + \\hat \\beta_1 x_1 + ... + \\varepsilon_{new}\\) because we don’t know \\(\\varepsilon_{new}\\). However, we can predict \\[\\text{Var}(\\hat y_{new}|x_{new}) = \\text{Var}(\\hat \\beta_0 + \\hat \\beta_1 x_1 + ...) + \\underbrace{\\text{Var}(\\varepsilon_{new})}_{=0,\\, \\text{ by assumption}}.\\]\nWhy should these values be \\(t\\)-distributed? Because finite samples of the \\(\\beta\\) distributed \nOften we just write \\(\\mathbb E(y) = X\\beta\\), but this isn’t really the full model. It’s missing an assumption: \\(\\text{Var}(y) = \\sigma^2I_n\\) where \\(I_n\\) is the \\(n \\times n\\) identity matrix.\nWe get \\(\\hat \\beta\\) often from either \\(OLS\\) or \\(MLE\\), and we get the \\(\\hat \\sigma^2\\) from the MSE.\nWhy do we care about these values? One way is to just say “center and spread” — but a more sophisticated way is to realize that these two statistics completely characterize a normal distribution.\nThe moment generating function says that \\[D(Y) = 1 + \\mathbb Ey + \\text{Var}(y) + \\text{Skew}(y) + \\text{Kurtosis}(y) + ...\\]\nConfidence intervals tell us what are the plausible range of model parameters."
  },
  {
    "objectID": "week3/week3.html#hypothesis-testing",
    "href": "week3/week3.html#hypothesis-testing",
    "title": "Week 3",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nHypothesis testing is about having a hunch and seeing if we’re right.\nFor multiple linear regression, there are many options for testing the significance of predictor variables. These tests fall under two main categories: F tests and Wald tests. Both are asymptotically equivalent, so yield comparable results in hypothesis testing.\nThe F-test is looking at the variances. The Wald test is looking at the behavior of the means.\n\nF-tests (comparison of variances)\nRecall that\n\nSST (Total Sum of Squares) is a measure of the total variance in the outcome from the given sample.\nSSR (Regression Sum of Squares) represents the total variance in the outcome explained by the regression model.\nSSE (Sum of Squared Errors) represents the remaining total variance in the outcome that is not captured by the regression model. We use the SSE to estimate the true (unobserved) variance \\(\\sigma^2\\) of the residuals. Let \\(\\hat \\sigma^2 = MSE = SSE/(n-p-1)\\) for a model with \\(p\\) predictors and an intercept.\n\n\\[SST \\stackrel{def}{=} \\sum_{i=1}^n (Y_i - \\bar Y)^2 = \\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2 + \\sum_{i=1}^n (Y_i - \\hat Y_i)^2 \\stackrel{def}{=} SSR + SST\\]\nThe \\(F\\)-tests are:\n\nTest for no covariate effect \\(H_0: \\beta_1 = ... = \\beta_p = 0\\)\n\n\\[F = \\frac{SSR/p}{SSE/(n-p-1)} = \\frac{SSR/p}{MSE} \\sim F_{p,n-p-1}.\\] 2. Test for a single variable \\(x_j\\). \\(H_0 : \\beta_j = 0\\).\n\\[F = \\frac{[SSR - SSR(\\text{model without } x_j)]/1}{SSE/(n-p-1)}\\] \\[ =\n  \\frac{SSR - SSR(\\text{model without } x_j)}{MSE} \\sim F_{1, n-p-1}.\\]\n\nTest for a group of \\(r\\) variables. \\(H_0: \\beta_{j_1} = ... = \\beta_{j_r} = 0\\).\n\n\\[F = \\frac{[SSR - SSR(\\text{model without } r \\text{ variables})]/r}{SSE/(n-p-1)}\\] \\[ = \\frac{[SSR - SSR(\\text{model without } r \\text{ variables})]/r}{MSE} \\sim F_{1,n-p-1}.\\]\n\nTest of a general linear hypothesis, \\(H_0: C\\beta= 0\\) where \\(C\\) is an \\(r \\times (p+1)\\) matrix of linearly independent contrasts. The test requires calculation of the sum of squares for the reduced model, where we parameterize according to the null hypothesis.\n\n\\[F = \\frac{(SSR - SSR(\\text{reduced model}))/r}{MSE} \\sim F_{r,n-p-1}\\]\nThe MSE is from the full model.\nNote: the reduced model must be nested within the full model for us to use the \\(F\\)-test.\n\nA great question is to look at this formula and say that the numerator is a marginal quantity, so why shouldn’t the denominator also be marginal?\nOne reason might be that if it were, the denominator would no longer have the same degrees of freedom as the numerator.\nAnother reason, arguably more important, is that this quantity would no longer be \\(F\\)-distributed, and we really want to have a quantity with nice asymptotic distributional properties.\n\nLet’s look at an example of situation 4:\nThe full model might be \\[\\mathbb E(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\]\nAnd model 2 is given as \\[\\mathbb E(y) = \\gamma_0 + \\gamma_1 x_1\\]\nThen\n\\[\\left( \\begin{array}{cccc} 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{array} \\right) \\left( \\begin{array}{c} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\end{array} \\right) = \\left( \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right)\\]\nIn general, when we say that the reduced model must be nested within the full model, what we’re saying is that the reduced model can be expressed as a linear constraint imposed on the full model.\nFor example, we might have a scenario where \\(Age \\in \\{ 57, 58, 59 \\}\\) and our reduced model is \\[y = \\beta_0 + \\beta_1 Age\\]\nAnd our full model is \\[y = \\gamma_0 + \\gamma_1 \\mathbb 1(Age = 57) + \\gamma_2 \\mathbb 1(Age = 58) + \\gamma_3 \\mathbb 1 (Age = 59)\\]\nIf we make a new variable that is \\[57 \\mathbb 1(Age = 57) + 58 \\mathbb 1(Age = 58) + 59 \\mathbb 1 (Age = 59)\\], and this recovers the original age variable.\n\nThe \\(F\\)-test can be thought of as a cost-benefit ratio."
  },
  {
    "objectID": "week3/week3.html#wald-test-comparison-of-means",
    "href": "week3/week3.html#wald-test-comparison-of-means",
    "title": "Week 3",
    "section": "Wald test (comparison of means)",
    "text": "Wald test (comparison of means)\nAs we touched on in lecture, we can also consider an alternative hypothesis test formulation for linear regression—this will also help motivate the form of our confidence intervals below. Recall the following properties: For an \\(r \\times (p+1)\\) matrix \\(C\\) and a random vector \\(y\\), then\n\\[\\mathbb E(Cy) = C\\mathbb E(y) \\quad \\text{and} \\quad \\text{Var}(Cy) = C\\text{Var}(y)C^T\\]\nAs we have shown previously, \\[\\hat \\beta \\sim MVN_{p+1} (\\beta, \\sigma^2 C(X^TX)^{-1}C^T)\\]\nSo, to test the general linear hypothesis \\(H_0: C\\beta = 0\\), we have the following statistic (which generalizes the familiar univariate Wald statistic \\(W = \\hat \\beta^2 / \\widehat{\\text{Var}}(\\hat \\beta) \\sim \\chi^2_1\\), asypmptotically):\n\\[W = (C\\hat \\beta)^T\\underbrace{[\\hat \\sigma^2 C(X^TX)^{-1}C^T]^{-1}}_{\\text{variance of } C\\beta}(C\\hat \\beta)\\]\nIf we think of what we’d do in a standard intro stats class, we’d do one of two things.\n\nWe’d look at \\(\\hat \\beta_1 / \\hat \\sigma(\\hat \\beta_1)\\), which is \\(t\\)-distributed.\nOr we’d look in \\(\\hat \\beta_1^2/\\widehat{Var}(\\hat \\beta_1)\\) which is \\(F\\)-distributed and asymptotically \\(\\chi^2\\) distributed.\n\nThen by properties of the \\(F\\)-statistic, \\(W/r \\sim F_{r,n-p-1}\\), and also asymptotically \\(W \\sim \\chi_r^2\\)."
  },
  {
    "objectID": "week3/week3.html#additional-remarks",
    "href": "week3/week3.html#additional-remarks",
    "title": "Week 3",
    "section": "Additional Remarks",
    "text": "Additional Remarks\nNote that the univariate Wald Statistic \\(W = \\hat \\beta^2 / \\widehat{\\text{Var}}(\\hat \\beta) \\sim \\chi_1^2\\) is equivalent to the \\(t\\)-test statistic\n\\[t_{obs} = \\frac{\\hat \\beta_1}{s.e.(\\hat\\beta_1)} \\sim t_{n-2,(1-\\alpha/2)}.\\]\nWhile we will not go into this in detail, the theoretical motivation for all of these tests is that they are ratios of \\(\\chi^2\\)-distributed random variables that are scaled by their degrees of freedom, which in turn defines the \\(F\\)-distribution.\nThe \\(F\\)-test is valid if and only if the \\(\\chi^2\\) assumption for the numerator and denominator holds true. Yet, \\(\\chi^2\\) random variables are obtained from the sums of squared normal random variables. This is why it is necessary for the residuals \\(\\varepsilon_i\\) to follow a normal distribution and/or 2) large sample theory to hold (so that the \\(\\varepsilon_i\\) may be approximately normal).\nThese \\(F\\)-tests are sometimes given in the context of an analysis of variance (ANOVA) model and table, which presents the sum of squares explained by each variable, conditional on all previous variables in the model, and then the sum of squares for the error terms. Each test compares two models, one nested within the other. Nested models will come up again when we discuss likeli- hood ratio tests, of which the F -test is a special case under the assumption of normally-distributed outcomes.\nA lack of significance of the effect of a covariate or group of covariates does not necessarily indicate the absence of an effect. Whether to remove these covariates from the model will depend on the scientific goal of the study."
  },
  {
    "objectID": "week2/week2.html",
    "href": "week2/week2.html",
    "title": "Week 2",
    "section": "",
    "text": "Estimation\nProperties of estimators that we want:\nRecommended reading: the Matrix Cookbook"
  },
  {
    "objectID": "week2/week2.html#recap",
    "href": "week2/week2.html#recap",
    "title": "Week 2",
    "section": "Recap",
    "text": "Recap\nEquivalent forms of the canonical linear model:\nAssuming \\(\\mathbb E[\\epsilon_i] = 0\\), \\(\\text{Var}(\\epsilon_i) = \\sigma^2\\), \\(\\text{Cov}(\\epsilon_i, \\epsilon_i') = 0\\).\nScalar notation: \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_p x_{ip} + \\epsilon_i\\]\nVector notation\n\\[ Y_i = x_i' \\beta + \\epsilon_i\\]\nExpectation representations\n\\[\\mathbb E(Y_i) = \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_p x_{ip}\\] \\[\\mathbb E(Y_i) = x_i' \\beta\\]\nParameter interpretation: the \\(\\beta\\) values represent the change in expected value per unit change in \\(x\\) holding the remaining predictors constant.\nBest practices:\n\nAlways include an intercept, even if it’s not interpretable or of substantive interest.\nFor \\(K\\)-category predictors, include \\(K-1\\) indicators in regression (leads to ANOVA-like situations).\nIf relationships are suspected to be nonlinear, can include polynomial or other nonlinear terms."
  },
  {
    "objectID": "week2/week2.html#confounding",
    "href": "week2/week2.html#confounding",
    "title": "Week 2",
    "section": "Confounding",
    "text": "Confounding\nGiven an outcome \\(Y\\) and two covariates \\(x1\\) and \\(x2\\), should we include \\(x1\\) in the model?\nSuppose we didn’t see the data generation process:\n\nx2 <- rnorm(n = 100, mean = 3.25, sd = .15)\nY <- x2*1.5 + rnorm(n = 100)\nx1 <- (x2 + Y > 8)\n\nBut we could see marginal associations:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nggplot(data.frame(), aes(x = x2, y = Y, color = x1, shape = x1)) + \n  geom_point() + \n  theme_bw() + \n  theme(legend.position = 'bottom') + \n  ggtitle(expression(paste(x[1], \" confounds the relationship between \",\n                           x[2], \" and \", Y)))\n\n\n\n\nWhich would show us that \\(x_1\\) is associated with both \\(x_2\\) and \\(Y\\).\nFitting a model with both \\(x_1\\) and \\(x_2\\), we would call the prediction of \\(Y\\) given \\(x_2\\) the conditional association between \\(x_2\\) conditional on (or within levels of) \\(x_1\\).\n\n# observe that the association flips\njtools::summ(lm(Y ~ x2))\n\n\n\n\n  \n    Observations \n    100 \n  \n  \n    Dependent variable \n    Y \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(1,98) \n    0.83 \n  \n  \n    R² \n    0.01 \n  \n  \n    Adj. R² \n    -0.00 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    3.16 \n    1.92 \n    1.64 \n    0.10 \n  \n  \n    x2 \n    0.54 \n    0.59 \n    0.91 \n    0.36 \n  \n\n\n Standard errors: OLS\n\n\njtools::summ(lm(Y ~ x1 + x2))\n\n\n\n\n  \n    Observations \n    100 \n  \n  \n    Dependent variable \n    Y \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(2,97) \n    83.18 \n  \n  \n    R² \n    0.63 \n  \n  \n    Adj. R² \n    0.62 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    5.83 \n    1.20 \n    4.87 \n    0.00 \n  \n  \n    x1TRUE \n    1.48 \n    0.12 \n    12.81 \n    0.00 \n  \n  \n    x2 \n    -0.55 \n    0.37 \n    -1.49 \n    0.14 \n  \n\n\n Standard errors: OLS\n\n\n\nThis figure shows some types of bivariate confounding:\n\n\n\n\n\nTypes of confounding from Faraway, page 50\n\n\n\n\nExamples of confounding in the HERS data:\nHow is a woman’s LDL cholesterol level associated with her body mass index (BMI)?\nIn the HERS data, women with higher BMI tend to have higher LDL levels. However, interpreting this simple marginal association as causal might be misleading because\n\nOlder women in HERS have both lower BMI and lower LDL levels;\nEthnic background, as well as whether a woman smokes or drinks, also predict both higher BMI and higher LDL levels.\n\nThus BMI, the risk factor of interest, is associated with a number of other factors, or potential confounders, which also predict the outcome.\n\nlibrary(here)\nhers <- readr::read_csv(here(\"data/hers.csv\"))\n\n# just visualized to see how difficult it is to observe confounding\nGGally::ggpairs(hers, columns = c('LDL', 'BMI', 'age'), alpha = .5)\n\n\n\n\nOften we will just fit separate models for each of the pairwise models:\n\nsimple_summ <-\n  function(model) {\n    jtools::summ(model, model.info = F, model.fit = F)\n  }\nsimple_summ(lm(data = hers, formula = LDL ~ age))\n\n\n\n\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n\n\n\n\n(Intercept)\n\n\n164.83\n\n\n7.25\n\n\n22.73\n\n\n0.00\n\n\n\n\nage\n\n\n-0.30\n\n\n0.11\n\n\n-2.74\n\n\n0.01\n\n\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\nsimple_summ(lm(data = hers, formula = LDL ~ BMI))\n\n\n\n\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n\n\n\n\n(Intercept)\n\n\n133.19\n\n\n3.79\n\n\n35.11\n\n\n0.00\n\n\n\n\nBMI\n\n\n0.42\n\n\n0.13\n\n\n3.18\n\n\n0.00\n\n\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\nsimple_summ(lm(data = hers, formula = BMI ~ age))\n\n\n\n\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n\n\n\n\n(Intercept)\n\n\n37.40\n\n\n1.05\n\n\n35.78\n\n\n0.00\n\n\n\n\nage\n\n\n-0.13\n\n\n0.02\n\n\n-8.48\n\n\n0.00\n\n\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\nsimple_summ(lm(data = hers, formula = LDL ~ BMI + age))\n\n\n\n\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n\n\n\n\n(Intercept)\n\n\n151.44\n\n\n8.77\n\n\n17.26\n\n\n0.00\n\n\n\n\nBMI\n\n\n0.37\n\n\n0.13\n\n\n2.78\n\n\n0.01\n\n\n\n\nage\n\n\n-0.25\n\n\n0.11\n\n\n-2.31\n\n\n0.02\n\n\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\nIn the marginal model \\(LDL ~ BMI\\), we had an effect estimate for BMI of 0.42 vs. 0.37 in the model with both BMI and age. So we could say there was a 12% decrease in \\(\\hat \\beta_{BMI}\\).\nA commonly used rule of thumb is that a variable is a confounder if it changes the estimated associations of interest by >10%. However, this is a really arbitrary threshold, so when available substantive knowledge should be the primary consideration for selecting confounders a priori to any analyses. Moreover, these types of heuristic criteria are specific to linear regression and they change for other types of models (e.g., logistic models for binary outcomes).\nWe could condition on a few more variables that we might suspect are possible confounders:\n\nsimple_summ(lm(LDL ~ BMI + age + nonwhite + drinkany + smoking, data = hers))\n\n\n\n\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n\n\n\n\n(Intercept)\n\n\n147.32\n\n\n9.26\n\n\n15.91\n\n\n0.00\n\n\n\n\nBMI\n\n\n0.36\n\n\n0.13\n\n\n2.68\n\n\n0.01\n\n\n\n\nage\n\n\n-0.19\n\n\n0.11\n\n\n-1.68\n\n\n0.09\n\n\n\n\nnonwhiteyes\n\n\n5.22\n\n\n2.32\n\n\n2.25\n\n\n0.02\n\n\n\n\ndrinkanyyes\n\n\n-2.72\n\n\n1.50\n\n\n-1.82\n\n\n0.07\n\n\n\n\nsmokingyes\n\n\n4.75\n\n\n2.21\n\n\n2.15\n\n\n0.03\n\n\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\nThe effect for \\(\\hat \\beta_{BMI}\\) didn’t change very much, so we can presume that these additional variables are not meaningful confounders of the \\(LDL ~ BMI\\) relationship."
  },
  {
    "objectID": "week2/week2.html#interaction",
    "href": "week2/week2.html#interaction",
    "title": "Week 2",
    "section": "Interaction",
    "text": "Interaction\nWe may be interested in a model with interaction effects:\n\\[ \\mathbb E(Y_i) = \\beta_0 + \\beta_1 x_{i1} + \\beta_3 x_{i1} x_{i2}\\]\nWe can alternatively view this model as\n\\[ \\mathbb E(Y_i) = (\\beta_0 + \\beta_2 x_{i2}) + (\\beta_1 + \\beta_3 x_{i2})x_{i1} + \\epsilon_i.\\]\n(or switch the roles of \\(x_{i1}\\) and \\(x_{i2}\\). Interactions are also sometimes referred to as effect-modification.\nGenerally it’s considered best practice whenever including interaction terms to include the main-effects for any interacted variables as well. Sometimes in economics literature, main effects may be referred to as “constitutive effects”.\n\nStatin-Use Example\nFor example, we might ask if the association between LDL and BMI differ between those who take statins (cholesterol lowering medications) vs. those who do not?\nWe center BMI so that the Statin coefficient is meaningful.\n\nhers$BMI_centered <- hers$BMI - mean(hers$BMI, na.rm=TRUE)\nsimple_summ(lm(\n  LDL ~ BMI_centered * statins + \n    age + smoking + drinkany + nonwhite,\n  data = hers\n))\n\n\n\n\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n\n\n\n\n(Intercept)\n\n\n162.41\n\n\n7.58\n\n\n21.42\n\n\n0.00\n\n\n\n\nBMI_centered\n\n\n0.58\n\n\n0.16\n\n\n3.64\n\n\n0.00\n\n\n\n\nstatinsyes\n\n\n-16.25\n\n\n1.47\n\n\n-11.07\n\n\n0.00\n\n\n\n\nage\n\n\n-0.17\n\n\n0.11\n\n\n-1.56\n\n\n0.12\n\n\n\n\nsmokingyes\n\n\n3.11\n\n\n2.17\n\n\n1.44\n\n\n0.15\n\n\n\n\ndrinkanyyes\n\n\n-2.08\n\n\n1.47\n\n\n-1.42\n\n\n0.16\n\n\n\n\nnonwhiteyes\n\n\n4.07\n\n\n2.28\n\n\n1.79\n\n\n0.07\n\n\n\n\nBMI_centered:statinsyes\n\n\n-0.70\n\n\n0.27\n\n\n-2.61\n\n\n0.01\n\n\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\nAnother option not-covered in class is to use the / operator to create a BMI effect within the yes/no levels of statins:\ncheck ?formula.terms for an explanation of the / operator:\n“The / operator provides a shorthand, so that a / b is equivalent to a + b %in% a.”\n\nlm.ldl.interact <- lm(\n  LDL ~ statins / BMI_centered + \n    age + smoking + drinkany + nonwhite,\n  data = hers\n)\nsimple_summ(lm.ldl.interact)\n\n\n\n\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n\n\n\n\n(Intercept)\n\n\n162.41\n\n\n7.58\n\n\n21.42\n\n\n0.00\n\n\n\n\nstatinsyes\n\n\n-16.25\n\n\n1.47\n\n\n-11.07\n\n\n0.00\n\n\n\n\nage\n\n\n-0.17\n\n\n0.11\n\n\n-1.56\n\n\n0.12\n\n\n\n\nsmokingyes\n\n\n3.11\n\n\n2.17\n\n\n1.44\n\n\n0.15\n\n\n\n\ndrinkanyyes\n\n\n-2.08\n\n\n1.47\n\n\n-1.42\n\n\n0.16\n\n\n\n\nnonwhiteyes\n\n\n4.07\n\n\n2.28\n\n\n1.79\n\n\n0.07\n\n\n\n\nstatinsno:BMI_centered\n\n\n0.58\n\n\n0.16\n\n\n3.64\n\n\n0.00\n\n\n\n\nstatinsyes:BMI_centered\n\n\n-0.12\n\n\n0.22\n\n\n-0.54\n\n\n0.59\n\n\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\nHow do we interpret them? Often the simplest way is to just visualize them.\n\n# install.packages(\"interactions\")\ninteractions::interact_plot(lm.ldl.interact,\n                            pred = BMI_centered, modx = statins,\n                            interval = TRUE)\n\n\n\n\n\nQuestion: How would we interpret the magnitude of an interaction between 2 continuous variables?\nThe coefficient estimate for an interaction with two continuous effects is the change in the \\(x_1 \\sim Y\\) slope corresponding to a 1-unit change in \\(x_2\\)."
  },
  {
    "objectID": "week2/week2.html#matrix-representation-of-multiple-linear-regression",
    "href": "week2/week2.html#matrix-representation-of-multiple-linear-regression",
    "title": "Week 2",
    "section": "Matrix Representation of Multiple Linear Regression",
    "text": "Matrix Representation of Multiple Linear Regression\nI will use math bolding once and then give up on it. Do not expect more from me. It is too much of a pain to write in every line.\n\\[ \\pmb Y = \\pmb X \\pmb \\beta + \\pmb \\epsilon \\]\n\\[ \\pmb Y = \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{array}  \\right],\n\\quad \\pmb X = \\left[ \\begin{array}{ccccc} 1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{np} \\\\\n\\end{array} \\right], \\]\n\\[ \\pmb \\beta = \\left[ \\begin{array}{c} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_n \\end{array}  \\right],\n\\quad \\pmb \\epsilon = \\left[ \\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}  \\right].\\]\nNow we can write (under the usual assumptions on \\(\\epsilon_i\\)), we can alternatively write \\(\\mathbb E(\\pmb Y) = \\pmb X \\pmb \\beta\\).\nThis marks the spot when I shall give up on bolding vectors and matrices."
  },
  {
    "objectID": "week2/week2.html#how-should-we-estimate-hat-beta",
    "href": "week2/week2.html#how-should-we-estimate-hat-beta",
    "title": "Week 2",
    "section": "How should we estimate \\(\\hat \\beta\\)?",
    "text": "How should we estimate \\(\\hat \\beta\\)?\nThus far we haven’t made any distributional assumptions.\nWithout distributional assumptions, one way forward is to simply find the estimate \\(\\hat \\beta\\) that results in \\(X \\hat \\beta\\) as close as possible to the observed \\(y\\). (Note teh change to a lowercase \\(y\\) when referring to observed values in the sample rather than a random variable).\nUsing Euclidean distance, the distance between the vectors \\(y\\) and \\(X \\beta\\) is\n\\[ d(y, X\\beta) = \\sqrt{(y-X\\beta)'(y-X\\beta)}.\\]\nWe generally prefer not to work with square roots and since squaring is a momotone transformation for values on \\(\\mathbb R^+\\), the value that minimizes \\(d(y, X\\beta)\\) will also minimize \\(d(y,X\\beta)^2\\).\n\\[S(\\beta) = SSE = d(y,X\\beta)^2 = (y-X\\beta)'(y-X\\beta)\\] \\[ = y'y - 2y'X\\beta + \\beta' X' X \\beta\\]\nThe values of \\(\\beta\\) that minimize \\(S(\\beta)\\) are called least squares estimates or ordinatry least squares (OLS) estimates.\nOLS has been around since at least the early 1800s and variously attributed to Gauss, Laplace, Legendre, etc.. A lot of the properties of estimators obtained in this way were proven by Gauss.\nSee Stephen Stigler’s Gauss and the Invention of Least Squares https://www.jstor.org/stable/2240811.\nTo find OLS estimates, we (1) compute the gradient of \\(S(\\beta)\\), (2) set the equation to zero, and (3) solve for \\(\\beta\\).\n\\[ \\frac{\\partial S(\\beta)}{\\partial \\beta } = -2X'y + 2X'X\\beta \\stackrel{set}{=} 0\\] \\[ = -2X'(y-X\\beta) = 0\\]\nThis gives us the least squares normal equations:\n\\[ X'X \\hat \\beta = X' y\\]\nNormal equations have a relationship to geometry that “we” won’t expand on further. Except I will! See https://stats.stackexchange.com/a/305748/174809\n\n\n\n\n\n\n\n\n\n\nMultiply each side by \\((X'X)^{-1}\\) to obtain: \\[\\hat \\beta = (X'X)^{-1}X'y\\] provided \\((X'X)^{-1}\\) exists (it will if predictors are linearly independent.\nThis is a good equation to commit to memory.\n\n(About now is a good time to note that \\(X\\) is capitalized because it’s a matrix, not because it’s a random variable)."
  },
  {
    "objectID": "week2/week2.html#simple-linear-regression-setting",
    "href": "week2/week2.html#simple-linear-regression-setting",
    "title": "Week 2",
    "section": "Simple Linear Regression Setting",
    "text": "Simple Linear Regression Setting\n\\[\\hat \\beta_1 =\n\\frac{\\sum_{i=1}^n (x_i - \\bar x)(y_i - \\bar y}{\\sum_{i=1}^n (x_i - \\bar x)^2} = \\frac{\\widehat{\\text{Cov}} (x,y)}{\\left(\\widehat{\\text{sd}}(x)\\right)^2} =\n\\hat \\rho_{xy} \\left( \\frac{\\widehat{\\text{sd}}(y)}{\\widehat{\\text{sd}}(x)} \\right)\\]\n\\[\\hat{\\beta_0} = \\bar y - \\hat \\beta_1 \\bar x.\\]\nUsing these estimates, we get a fitted value for the \\(i\\)th observation and a residual for it."
  },
  {
    "objectID": "week2/week2.html#properties-of-least-squares-estimates",
    "href": "week2/week2.html#properties-of-least-squares-estimates",
    "title": "Week 2",
    "section": "Properties of Least Squares Estimates",
    "text": "Properties of Least Squares Estimates\n\\(\\hat \\beta\\) is an unbiased estimator of \\(\\beta\\).\n\\[\\mathbb E(\\hat \\beta) = \\mathbb E[\\underbrace{(X'X)^{-1} X'}_{A} y] = (X'X)^{-1} X'X \\beta = \\beta\\]\nThe variance of \\(\\hat \\beta\\) is expressed by the variance-covariance matrix.\n\\[\\text{Var}(\\hat \\beta) = (X'X)^{-1} X' \\text{Var}(Y) X (X'X)^{-1}\\] \\[ = \\sigma^2 (X'X)^{-1}\\]\nIf we let \\(D = (X'X)^{-1}\\), the variance of \\(\\hat \\beta_j = \\sigma D_{jj}\\) and the covariance between \\(\\hat \\beta_i\\) and \\(\\hat \\beta_j\\) is \\(\\sigma^2 D_{ij}\\)."
  },
  {
    "objectID": "week1/week1.html",
    "href": "week1/week1.html",
    "title": "Week 1",
    "section": "",
    "text": "Linear Regression\nWe will generally be intereseted in the relationship between an outcome \\(Y\\) and \\(p\\) covariates or predictors denoted \\((x_1, ..., x_p)\\).\nWe generally say that a statistical model has a systematic component and a random component.\nWe often hypothesize that the “real” relationship between our outcome and predictors might be “super-complex”. Like cancer and environmental exposures might have complex dependencies on genetics, etc.. So sometimes instead of having a systematic component that captures variables in their full complexity, we might prioritize interpretability and we might be satisfied with an imperfect (but more intuitive, simpler) model that might give us some intuition about reality.\nThe random component may provide both a means to explain everything uncaptured by our predictors, as well as “real” randomness. For example, we might theorize that every time cells divide, there’s some small chance due to genetic drift that cells become metastatic and cancerous which is just random (did a mutation happen that was harmful in the right way in the right spot?)."
  },
  {
    "objectID": "week1/week1.html#il-famoso-smoking-ra-fisher",
    "href": "week1/week1.html#il-famoso-smoking-ra-fisher",
    "title": "Week 1",
    "section": "il famoso Smoking RA Fisher",
    "text": "il famoso Smoking RA Fisher\nWe’ll talk about a lot of the methods that Ronald A. Fisher developed. Already in the 1900s it was being observed that there was a strong association between smoking and lung cancer. However, Fisher was a smoker himself and posited that the association between lung cancer and smoking could be explained away by some genetic or biological difference between the smoking and non-smoking population (positing some genes that caused people to desire to smoke).\n\n\n\n\n\nRonald Fisher’s unsupported theory of genetics confounding the smoking-lung cancer relationship\n\n\n\n\nWe’re pretty sure that this was driven not by any substance matter expertise, but rather by Fisher’s love of smoking."
  },
  {
    "objectID": "week1/week1.html#hormone-replacement-therapy",
    "href": "week1/week1.html#hormone-replacement-therapy",
    "title": "Week 1",
    "section": "Hormone Replacement Therapy",
    "text": "Hormone Replacement Therapy\nIn the mid- to late- 20th century there were a ton of studies linking hormone replacement therapy for older women to better cardiovascular outcomes (lack of coronary heart disease).\nHowever, thankfully due to the Heart and Estrogen/Progestin Study (HERS, in the early 90s) we now know that a lot of those studies were not controlling for socioeconomic status. It turns out that socioeconomic status was highly associated with HRT usage, and associated at least in the US with a lot of better health outcomes across the board.\nIt turned out that HRT when applied at certain times for some people can actually be harmful — but the point is the picture is much muddier than was initially thought and recommendations were rolled back. Later randomized studies were performed that produced reliable bodies of evidence demonstrating either no effect or in some cases harmful effects.\nWe’ll use the baseline data from HERS (not so much interested in the HRT treatment effect), but to investigate the research question:\n\nHow is systolic blood pressure related to age, independently of other well-known cardiovascular risk factors? (Age, diabetes, smoking, etc.)"
  },
  {
    "objectID": "week1/week1.html#prediction-studies",
    "href": "week1/week1.html#prediction-studies",
    "title": "Week 1",
    "section": "Prediction Studies",
    "text": "Prediction Studies\nTypically in prediction settings, there’s no single exposure of particular interest; mechanisms and confounding is treated as less of a concern (if at all), and the main challenge is that we need to take care to not overfit the data.\nA major theme of this class will be that different tasks require different analysis strategies and diffrent statistical tools."
  },
  {
    "objectID": "week1/week1.html#quantifying-uncertainty",
    "href": "week1/week1.html#quantifying-uncertainty",
    "title": "Week 1",
    "section": "Quantifying Uncertainty",
    "text": "Quantifying Uncertainty\nTypically standard statistical models have nice theoretical properties because years-and-years ago, we didn’t have much data so people spent their time studying theory instead of data. As a result, we have a lot of nice theories about the uncertainty represented in statistical models.\nAn example of the kind of uncertainty we might be interested in is shown in this figure relating Alzheimer’s disease rates and exposure to PM2.5.\n\n\n\n\n\n\n\n\n\nThis figure is taken from the article Long-term effects of PM2·5 on neurological disorders in the American Medicare population: a longitudinal cohort study by Shi et al, Lancet Planetary Health (2020)."
  },
  {
    "objectID": "week1/week1.html#why-learn-methods-before-study-design",
    "href": "week1/week1.html#why-learn-methods-before-study-design",
    "title": "Week 1",
    "section": "Why Learn Methods Before Study Design",
    "text": "Why Learn Methods Before Study Design\nAn interesting point made is that it’s important to understand the limitations, strengths of methods, what they can and can’t do, and how to use them before designing a study."
  },
  {
    "objectID": "week1/week1.html#recommended-reading",
    "href": "week1/week1.html#recommended-reading",
    "title": "Week 1",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nKutner M, Nachtsheim C, Neter J, Li W. Applied Linear Statistical Model. 5th edition. chapters 1-3\nShmueli, G. (2010). To explain or to predict? Statistical Science. https://www.stat.berkeley.edu/~aldous/157/Papers/shmueli.pdf"
  },
  {
    "objectID": "week1/week1.html#hers-study-sbp-in-post-menopausal-women",
    "href": "week1/week1.html#hers-study-sbp-in-post-menopausal-women",
    "title": "Week 1",
    "section": "HERS Study: SBP in Post-Menopausal Women",
    "text": "HERS Study: SBP in Post-Menopausal Women\nIn a clinical trial of hormone therapy for preventing heart attacks and deaths among 2,763 post-menopausal women with existing coronary heart disease:\n\nthe outcome variable is systolic blood pressure\nthe data collected on covariates included age, diabetes diagnosis, smoking status, etc.\nthe research question was how is systolic blood pressure jointly related to age, statin use, and other risk factors in this cohort.\n\nReference: Vittinghoff et al., Regression methods in Biostatistics 2005 https://regression.ucsf.edu/regression-methods-biostatistics-linear-logistic-survival-and-repeated-measures-models"
  },
  {
    "objectID": "week1/week1.html#multiple-linear-regression-scalar-notation",
    "href": "week1/week1.html#multiple-linear-regression-scalar-notation",
    "title": "Week 1",
    "section": "Multiple Linear Regression: Scalar Notation",
    "text": "Multiple Linear Regression: Scalar Notation\nWe consider the model\n\\[Y_i = \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_p x_{ip} + \\epsilon_i, \\quad i = 1, ..., n.\\]\n\\[\\mathbb E(\\epsilon_i) = 0, \\, \\underbrace{\\text{Var}(\\epsilon_i) = \\sigma^2}_{\\text{homoscedasticity assumption}}, \\, \\text{ and Cov}(\\epsilon_i, \\epsilon_j) = 0.\\]\nTypically we assume that the predictors \\(x_i\\) are fixed and measured without error.\nWe require that \\(p < n\\).\nTJ notes that \\(\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0\\) doesn’t imply independence. This is the assumption we’re making for the time-being, but independence will be implied after we later assume that the errors are normally distributed.\nRoman asks if we need a conditional mean zero assumption — i.e., \\(\\mathbb E(\\epsilon | x) = 0\\)? Rachel notes that \\(\\epsilon\\) and \\(x\\) are assumed to be independent, so \\(\\mathbb E(\\epsilon | x) = \\epsilon\\). We will make stronger assumptions later, but we aren’t introducing those yet.\nBy taking the expected value of both sides, we can equivalently write that\n\\[\\mathbb E(Y_i) = \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_p x_{ip}.\\]\nThe parameters \\(\\beta_j\\) for \\(j = 1, ..., p\\) represent the change in the expected value \\(\\mathbb E(Y_i)\\) per unit change in \\(x_j\\) holding the remaining predictors \\(x_k \\, (k \\neq j)\\) constant.\nOne can see this by observing:\n\\[ \\mathbb E(Y_i | x_{i1} = x^* + 1) = \\beta_0 + \\beta_1 (x^* + 1) + ...\\] \\[ \\mathbb E(Y_i | x_{i1} = x^*) = \\beta_0 + \\beta_1 x^* + ...\\] \\[ \\mathbb E(Y_i | x_{i1} = x^* + 1) - \\mathbb E(Y_i | x_{i1} = x^*) = \\beta_1.\\]\nWe most often interested in testing whether \\(\\beta_j = 0\\), interpreted as a test of whether there’s an association between \\(x_j\\) and \\(Y\\)."
  },
  {
    "objectID": "week1/week1.html#vector-notation",
    "href": "week1/week1.html#vector-notation",
    "title": "Week 1",
    "section": "Vector Notation",
    "text": "Vector Notation\nWe can write this model more succinctly by writing:\n\\[ Y_i = x_i' \\beta + \\epsilon_i, \\quad i = 1, ..., n,\\]\nwhere \\(x_i = (1, x_{i1}, ..., x_{ip})',\\) and \\(\\beta = (\\beta_0, \\beta_1, ..., \\beta_p)'\\), \\(\\mathbb E(\\epsilon_i) = 0\\), \\(\\text{Var}(\\epsilon_i) = \\sigma^2\\), and \\(\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0\\)."
  },
  {
    "objectID": "week1/week1.html#examples",
    "href": "week1/week1.html#examples",
    "title": "Week 1",
    "section": "Examples",
    "text": "Examples\n\nlibrary(here)\n\nhere() starts at /Users/cht180/Documents/2023/BST232 Methods Quarto\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nhers <- readr::read_csv(here(\"data/hers.csv\"))\n\nRows: 2763 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): HT, raceth, nonwhite, smoking, drinkany, exercise, physact, globra...\ndbl (24): age, medcond, weight, BMI, waist, WHR, glucose, weight1, BMI1, wai...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nplt1 <- ggplot(hers %>% dplyr::sample_frac(.1), aes(x = age, y = SBP)) + \n  geom_point() + \n  theme_bw() + \n  ggtitle(\"Scatterplot of a 10% sample of our data\") \nplt1 \n\n\n\nplt2 <- plt1 + \n  stat_summary_bin(bins = 10, breaks = quantile(hers$age, seq(0,1, 0.1)), geom = 'line', fun = mean, color = 'cadetblue', size = 1.5) + \n  stat_summary_bin(bins = 10, breaks = quantile(hers$age, seq(0,1, 0.1)), geom = 'point', shape = 23, fun = mean, color = 'black', size = 3.5, fill = 'white') + \n  ggtitle(\"Now with overlain means for decile groups by age\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nplt2 \n\n\n\nplt3 <- \n  plt2 + \n  geom_smooth(method = 'lm', se = FALSE) + \n  ggtitle(\"Now with a linear model\") \nplt3 \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nlm.sbp.age <- lm(SBP ~ age, data = hers)\njtools::summ(lm.sbp.age)\n\n\n\n\n  \n    Observations \n    2763 \n  \n  \n    Dependent variable \n    SBP \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(1,2761) \n    77.21 \n  \n  \n    R² \n    0.03 \n  \n  \n    Adj. R² \n    0.03 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    103.63 \n    3.60 \n    28.82 \n    0.00 \n  \n  \n    age \n    0.47 \n    0.05 \n    8.79 \n    0.00 \n  \n\n\n Standard errors: OLS\n\n\n\nSo now we would say that \\(\\hat{\\mathbb E}(SBP_i) = 103.63 + 103.63 Age_i\\). Though, the intercept is kind of useless since our model doesn’t have any observations for systolic blood pressure for age 0 infants. We might want to fit an age centered model.\n\nlm.sbp.agec <- lm(SBP ~ I(age - mean(age)), data = hers)\njtools::summ(lm.sbp.agec)\n\n\n\n\n  \n    Observations \n    2763 \n  \n  \n    Dependent variable \n    SBP \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(1,2761) \n    77.21 \n  \n  \n    R² \n    0.03 \n  \n  \n    Adj. R² \n    0.03 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    135.07 \n    0.36 \n    378.24 \n    0.00 \n  \n  \n    I(age - mean(age)) \n    0.47 \n    0.05 \n    8.79 \n    0.00 \n  \n\n\n Standard errors: OLS\n\n\n\nIn which case we’d say that \\(\\hat{\\mathbb E}(SBP_i) = 135.07 + 135.07 Age_i\\)."
  },
  {
    "objectID": "week1/week1.html#multiple-linear-regression",
    "href": "week1/week1.html#multiple-linear-regression",
    "title": "Week 1",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\n# fitting a p=2 model\nlm.sbp.age.weight <- lm(SBP ~ age + weight, data = hers)\nx <- y <- seq(0, 100, length= 30)\nf <- function(x,y){ z <- x*coef(lm.sbp.age.weight)[2] + y*coef(lm.sbp.age.weight)[3] + coef(lm.sbp.age.weight)[1] }\nz <- outer(x,y,f)\npersp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = \"lightblue\", xlab = \"age\", ylab = \"weight\", zlab = \"expected SBP\", ticktype = 'detailed', nticks = 4)"
  },
  {
    "objectID": "week1/week1.html#indicator-dummy-variables",
    "href": "week1/week1.html#indicator-dummy-variables",
    "title": "Week 1",
    "section": "Indicator / Dummy Variables",
    "text": "Indicator / Dummy Variables\nWe might be interested in modeling categorical variables as well. To do so, we would include dummy variables.\nIf a categorical variable has \\(K\\) levels, then we’ll need to compute \\(K-1\\) dummy variables where the omitted variable is called the “reference level”.\n\nunique(hers$physact)\n\n[1] \"much more active\"     \"much less active\"     \"about as active\"     \n[4] \"somewhat less active\" \"somewhat more active\"\n\nhers$physact <- as.factor(hers$physact) \n# the reference category will be \"much more active\" since it's the \n# first factor level — \n# if we wanted to change it, we could run: \n# hers$raceth <- relevel(hers$raceth, \"much less active\") \n# for example. \nlm.sbp.physact <- lm(SBP ~ physact, data = hers)\nhead(model.matrix(lm.sbp.physact))\n\n  (Intercept) physactmuch less active physactmuch more active\n1           1                       0                       1\n2           1                       1                       0\n3           1                       0                       0\n4           1                       1                       0\n5           1                       0                       0\n6           1                       0                       0\n  physactsomewhat less active physactsomewhat more active\n1                           0                           0\n2                           0                           0\n3                           0                           0\n4                           0                           0\n5                           1                           0\n6                           0                           0\n\njtools::summ(lm.sbp.physact)\n\n\n\n\n  \n    Observations \n    2763 \n  \n  \n    Dependent variable \n    SBP \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(4,2758) \n    0.48 \n  \n  \n    R² \n    0.00 \n  \n  \n    Adj. R² \n    -0.00 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    135.00 \n    0.63 \n    215.01 \n    0.00 \n  \n  \n    physactmuch less active \n    0.07 \n    1.49 \n    0.05 \n    0.96 \n  \n  \n    physactmuch more active \n    -0.90 \n    1.26 \n    -0.72 \n    0.47 \n  \n  \n    physactsomewhat less active \n    0.96 \n    1.06 \n    0.91 \n    0.36 \n  \n  \n    physactsomewhat more active \n    -0.05 \n    0.91 \n    -0.05 \n    0.96 \n  \n\n\n Standard errors: OLS\n\n\n\nIn machine learning this is called “one-hot” encoding."
  },
  {
    "objectID": "week1/week1.html#polynomial-regressions",
    "href": "week1/week1.html#polynomial-regressions",
    "title": "Week 1",
    "section": "Polynomial Regressions",
    "text": "Polynomial Regressions\n\nlm.sbp.agepolynomial <- lm(SBP ~ poly(age), data = hers)\n\n# we could plug in coef(lm.sbp.agepolynomial)[1] through \n# coef(lm.sbp.agepolynomial)[4] into the following equation, \n# but it's kind of boring — instead here's a more fun \n# looking cubic polynomial we could imagine being the result of\n# a polynomial regression:\ncurve(1 - 2.8*x + 4*(x^2) - .65*(x^3), from = 0, to = 5,\n      xlab = expression(x[1]),\n      ylab = \"E(Y)\")\n\n\n\n\nIt’s worth emphasizing that usually we prefer splines or generalized additive models to polynomial regression these days since polynomial regression can act strangely."
  },
  {
    "objectID": "week1/week1.html#marginal-associations",
    "href": "week1/week1.html#marginal-associations",
    "title": "Week 1",
    "section": "Marginal Associations",
    "text": "Marginal Associations\nIf we have one binary predictor \\(x_1\\) and one continuous \\(x_2\\), we might be interested in the marginal association between \\(Y\\) and \\(x_2\\). This would look like a model fit with lm(Y ~ x2) and \\(x_1\\) is not included."
  },
  {
    "objectID": "week1/week1.html#conditional-association",
    "href": "week1/week1.html#conditional-association",
    "title": "Week 1",
    "section": "Conditional Association",
    "text": "Conditional Association\nWe might also want to fit models that include \\(x_1\\), so those could be fit with lm(Y ~ x1 + x2) and this would include an effect for \\(x_2\\) (one effect, not multiple) that is applied while also considering an effect for \\(x1\\)."
  },
  {
    "objectID": "week4/week4.html#recap",
    "href": "week4/week4.html#recap",
    "title": "Week 4",
    "section": "Recap",
    "text": "Recap\nOne can use either a sums-of-squares decomposition approach or a Wald test to test hypotheses about linear models. The sums-of-squares decomposition (equivalently called the F-test approach) is looking at if the amount of variability remaining is significantly increased compared to the amount of variation in the original model when reducing the model complexity. On the other hand, the Wald test is looking at the distance between the \\(\\hat \\beta\\)-parameters and the null hypothesis. Both of these approaches can do either individual or overall tests.\nIn the Wald test, we would write the Wald statistic as \\[W = (C\\hat\\beta)'[\\widehat{\\text{Var}}(C\\hat\\beta)]^{-1} (C\\hat\\beta) \\sim \\chi_r^2.\\]\nFor the sums-of-squares decomposition approach, we solve for the \\(r\\) coefficients in terms of remaining \\((p+1-r)\\), plug these into the full model to find the reduced model, and then use the \\(F\\)-test. Remember that in the \\(F\\)-test, the numerator can be thought of (or called) the “extra sums of squares.”"
  },
  {
    "objectID": "week4/week4.html#multicollinearity",
    "href": "week4/week4.html#multicollinearity",
    "title": "Week 4",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nHigh correlation among predictors is called multicollinearity.\nGenerally, multicollinearity increases the standard errors of the regression coefficient estimates.\n\\[\\widehat{\\text{Var}}(\\hat \\beta_j) = \\frac{\\hat \\sigma^2}{(n-1)s_{x_j}^2 (1-r^2_j)},\\]\nwhere \\(r^2_j\\) is the \\(R^2\\) value from a multiple linear model that regresses \\(x_j\\) on all other predictors.\nRecall that in simple linear regression, one has that \\(\\text{Var}(\\hat \\beta_j) = \\frac{\\hat \\sigma^2}{(n-1)s_x^2}\\) where \\(s_x^2\\) is the variance of the \\(x\\). (This is how we showed that we prefer to have more variation in the \\(x\\)-values, all else being equal, because it helps make our estimates of \\(\\beta\\)-coefficients more precise.)\nThe term \\(1/(1-r^2)\\) is known as the variance inflation factor (VIF), since \\(\\text{Var}(\\hat \\beta_j)\\) is increased when \\(x_j\\) is highly correlated with the space defined by the other \\(p-1\\) covariates.\n\\(r^2\\) will be large (close to 1) when there is multicollinearity in the \\(x\\)’s.\nThus we want to balance the advantage of putting in strong predictors to decrease \\(\\hat \\sigma^2\\) with adding predictors that are highly correlated with existing \\(x\\)’s in the model.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nhers <- readr::read_csv(here::here(\"data/hers.csv\"))\n\nRows: 2763 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): HT, raceth, nonwhite, smoking, drinkany, exercise, physact, globra...\ndbl (24): age, medcond, weight, BMI, waist, WHR, glucose, weight1, BMI1, wai...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlm.1 <- lm(data = hers, LDL ~ BMI)\nlm.2 <- lm(data = hers, LDL ~ BMI + weight)\n\njtools::summ(lm.1, model.info = FALSE, model.coef = TRUE, model.fit = FALSE)\n\n\n\n\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n\n\n\n\n(Intercept)\n\n\n133.19\n\n\n3.79\n\n\n35.11\n\n\n0.00\n\n\n\n\nBMI\n\n\n0.42\n\n\n0.13\n\n\n3.18\n\n\n0.00\n\n\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\njtools::summ(lm.2, model.info = FALSE, model.coef = TRUE, model.fit = FALSE)\n\n\n\n\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n\n\n\n\n(Intercept)\n\n\n133.16\n\n\n3.81\n\n\n34.96\n\n\n0.00\n\n\n\n\nBMI\n\n\n0.38\n\n\n0.33\n\n\n1.15\n\n\n0.25\n\n\n\n\nweight\n\n\n0.01\n\n\n0.13\n\n\n0.10\n\n\n0.92\n\n\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\nOne can see here that the standard error on the BMI coefficient is blowing up from model 1 to 2."
  },
  {
    "objectID": "week4/week4.html#diagnostics-using-residuals",
    "href": "week4/week4.html#diagnostics-using-residuals",
    "title": "Week 4",
    "section": "Diagnostics using Residuals",
    "text": "Diagnostics using Residuals\nAssumptions of linear regression\n\\[Y_i = \\beta_0 + \\beta_1 + x_{i1} + ... + \\beta_p x_{ip} + \\varepsilon_i\\]\n\nThe relationships between the outcomes and a predictor, controlling for the other covariates in the model, is linear.\nThe \\(\\varepsilon_i\\) have 0 mean, constant variance.\nThe \\(\\varepsilon_i\\) are uncorrelated.\nThe errors are normally distributed or we have an adequate sample size to rely on large-sample theory.\nIn addition, we hope that model results are not driven by a small subset of observations.\n\nWe should always check fitted models to make sure these assumptions have not been violated.\nViolations of these assumptions can\n\nbias regression coefficients, and\nbring the validity of \\(p\\)-values and confidence/prediction intervals into question.\n\nFor example, \\(\\text{Var}(\\hat \\beta) = \\sigma^2 (X'X)^{-1}\\), which relies on \\(\\text{Var}(\\varepsilon) = \\sigma^2 I_n\\).\nRecall that normality and uncorrelatedness imply independence in the case of the normally distributed error terms.\nIn generally, we may write \\(e_i\\) as estimates of \\(\\varepsilon_i\\).\n\\[\\hat \\varepsilon_i = e_i = y_i - \\hat y_i.\\]\n(We’ll use \\(e_i\\) because the hats get annoying for the upcoming content.)\nThe fitted values are \\(\\hat y_i = x_i'\\hat\\beta\\) where \\(x_i\\) is the \\((p+1)\\times 1\\) vector of covariates for subject \\(i\\).\nBecause \\(e_i\\) is an estimate of \\(\\varepsilon_i\\), if the model holds we expect the \\(e_i\\) to exhibit the properties consistent with our model assumptions.\n\nThe Hat Matrix\nNote that we can write \\[\\hat y = X \\hat \\beta = X(X'X)^{-1}X'y = Hy\\]\nwhere \\(H = X(X'X)^{-1}X'\\).\n\\(H\\) is an \\(n \\times n\\) matrix that is symmetric and idempotent (idempotency means \\(HH = H\\)).\n\\(H\\) is called the “hat matrix” or “projection matrix,” and it pops up a lot in concepts or proofs related to linear regression. We’ll use the fact that \\[e = y - \\hat y = y - Hy = (I - H)y.\\]\n\n\nGeometric Interpretation of OLS using \\(H\\)\n\n\n\n\n\nIn linear algebra terms, \\(H\\) is an orthogonal projection matrix.\nWhen multiplied by any vector, \\(H\\) orthogonally projects that vector onto the column-space of \\(X\\).\nThus since \\(\\hat y = X \\hat \\beta_{OLS} = Hy\\), OLS regression can be viewed as projecting \\(y\\) onto the column space of \\(X\\).\nBecause it’s an orthogonal projection, \\(\\hat y\\) will be the closest point to \\(y\\) on the column space of \\(X\\).\nIn the figure, the blue plane represents the column-space of \\(X\\), and the orthogonal components (or decomposition into a linear combination of basis elements) of \\(\\hat y\\) is \\((\\hat \\beta_1 X_1, \\hat \\beta_2 X_2)\\).\nThe mean of the \\(\\{ e_i \\}\\) is 0 as long as there is an intercept in the model:\n\\[ \\bar e = \\frac{1}{n} \\sum_{i=1}^n e_i = 0 \\]\nNote that to show this, start by writing \\(\\bar e = n^{-1} 1'(I-H)y\\) and replace vector \\(1\\) with \\(Xa\\) where \\(a\\) is a \\((p+1) \\times 1\\) matrix, \\(a = (1, 0, 0, ..., 0)'\\).\nThe estimate of the population variance computed from the mean squared error\n\\[MSE = \\frac{SSE}{n - p - 1} = \\frac{1}{n-p-1}\\sum_{i=1}^n e_i^2\\]\nAlthough we’ve assumed that \\(\\text{Var}(\\varepsilon_i) = \\sigma^2\\), note that the \\(e_i\\) are estimated quantities and \\(\\text{Var}(e_i) \\neq \\sigma^2\\) (!).\nWe have shown that \\[e = (I-H)y.\\]\nThus the covariance matrix of the residuals is\n\\[\\text{Var}(e) = \\sigma^2(I-H)\\]\nWe can see this by writing that\n\\[\\text{Var}(e) = \\text{Var}((I - H)y)\\]\nAnd \\((I-H)\\) is constant (only a function of the \\(X\\) values).\nSo \\[\\text{Var}(e) = (I-H)\\text{Var}(y)(I-H)'\\] \\[ = (I-H)\\sigma^2I_n(I-H)'\\] \\[ = \\sigma^2(I-H)(I-H)'\\] \\[ = \\sigma^2(I-H) \\quad \\text{ by the fact that } (I-H) \\text{ is symmetric and idempotent}\\]\nThe variance of the \\(i\\)th residual is\n\\[\\text{Var}(e_i) = \\sigma^2 (1-h_i).\\]\nwhere \\(h_i\\) is the \\(i\\)th element on the diagonal of the hat matrix and \\(0 \\leq h_i \\leq 1\\).\n\n\nStudentized Residuals\nThe quantity\n\\[r_i = \\frac{e_i}{\\sqrt{MSE(1-h_i)}} = \\frac{e_i}{\\sqrt{\\hat\\sigma^2(I-H)}}\\]\nis a studentized residual and approximately follows a \\(t\\) distribution with \\(n - p - 1\\) degrees of freedom, assuming the model assumptions are satisfied.\nIf \\(n\\) is sufficiently large, these are approximately \\(\\mathcal N(0,1)\\).\nResiduals are correlated, but this correlation is minimal as long as \\(n\\) is sufficiently large.\n\nIn general, we’ve seen a few times that estimates divided by their standard error are \\(t\\)-distrbuted.\n\nResidual (or studentized residual) scatterplot are useful for helping us identify\n\nNon-normal errors\nNon-constant variance\nViolations of the assumption of linearity\nPotential outliers\n\nPlots of the distribution of the residuals help us assess normality (using histograms, boxplots, smoothed density plots, and normal probability [or Q-Q plots]).\nIn a normal Q-Q plot, the y-axis shows the ordered residuals. On the x-axis has the expected values of the order statistics for a normally distributed sample of size \\(n\\).\n\nx <- rt(n = 1000, df = 15)\nhist(x, breaks = 100)\n\n\n\n\n\n\n\nqqnorm(x)\nabline(a = 0, b = 1)\n\n\n\n\n\n\n\nx <- rcauchy(n = 1000)\nhist(x, breaks = 100)\n\n\n\n\n\n\n\nqqnorm(x)\nabline(a = 0, b = 1)\n\n\n\n\n\n\n\nx <- rlnorm(n = 1000, meanlog = 2.5, sdlog = .1)\nx <- x - mean(x)\nhist(x, breaks = 100)\n\n\n\n\n\n\n\nqqnorm(x)\nabline(a = 0, b = 1)\n\n\n\n\n\n\n\n\n\nCheck out the ggResidpanel and olsrr packages!\n\n\n\nTransformations of the Outcome\nTransforming the outcome is often a successful way to reduce the skewness of the residuals.\nThe idea is that often large values of the outcome yield the largest residuals.\nPulling these values back towards the center will yield a more symmetric distribution.\nA popular transformation is to replace \\(Y\\) with \\(\\log (Y)\\) where \\(\\log = \\ln\\)).\nOther “power transformations” that are often effective are \\(Y^k\\) with smaller values of \\(k\\) pulling in the right tail more strongly.\n\nhist(hers$LDL)\n\n\n\nplot(lm(data = hers, LDL ~ BMI), which = 2)\n\n\n\nplot(lm(data = hers, log(LDL) ~ BMI), which = 2)\n\n\n\n\nThese changes alter the interpretation of the model coefficients.\n\\[\\log(Y_i) = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\]\n\\[\\Longrightarrow Y_i = \\exp(\\beta_0) \\exp(\\beta_1 x_i) \\exp(\\varepsilon_i)\\]\nHere, \\(\\exp(\\beta_1)\\) is the ratio of the expected value of \\(y\\) corresponding to a one-unit increase in \\(x\\). To see this note that\n\\[\\frac{\\mathbb E(Y_i | x_i = x^*+1)}{\\mathbb E(Y_i | x_i = x^*)} = \\frac{exp(\\beta_0)\\exp(\\beta_1(x^*+1)) \\mathbb E[\\exp(\\varepsilon_i)]}{\\exp(\\beta_0)\\exp(\\beta_1 x^*) \\mathbb E[\\exp(\\varepsilon_i)]}\\]\n\\[ = \\frac{\\exp(\\beta_1x^* + \\beta_1)}{\\exp(\\beta_1 x^*)} = exp(\\beta_1)\\]\nPeople often report \\(100(e^{\\hat \\beta_1} - 1)\\) as the percent change in \\(\\mathbb E(Y_i)\\) per unit increase in \\(x\\)."
  },
  {
    "objectID": "week4/week4.html",
    "href": "week4/week4.html",
    "title": "Week 4",
    "section": "",
    "text": "Lab"
  },
  {
    "objectID": "week4/week4.html#assessing-constant-variance",
    "href": "week4/week4.html#assessing-constant-variance",
    "title": "Week 4",
    "section": "Assessing Constant Variance",
    "text": "Assessing Constant Variance\nStandard multiple linear regression assumes constant variance, or homoscedasticity:\n\\[\\sigma^2 = \\text{Var}(\\varepsilon_i) = \\text{Var}(Y_i | x_i)\\]\nWhen the errors are heteroscedastic, this can invalidate inference in both small and large samples.\nWe can plot \\(e_i\\) vs. \\(\\hat y_i\\) and look for changes in variance of \\(e_i\\) to get a sense of whether homoscedasticity holds.\n\nSome people prefer to use the studentized residuals for this since technically \\(\\text{Var}(e_i) = \\sigma^2(1-h_i)\\).\nAs we will see later, \\(h_i\\) is small for most observations in the sample so in practice many argue this makes little difference.\n\n\nlm.ldl <- lm(LDL ~ BMI, data = hers)\n\nplot(residuals(lm.ldl), main = \"Satisfactory Residual Plot\")\n\n\n\ndf <- tibble::tibble(\n  x = runif(1000, min = 0, max = 10),\n  y = rnorm(n = 1000, sd = sqrt(x)))\n\nplot(df$x, residuals(lm(data = df, y ~ x)), main = \"Unsatisfactory Residual Plot\")"
  },
  {
    "objectID": "week4/week4.html#assessing-linearity",
    "href": "week4/week4.html#assessing-linearity",
    "title": "Week 4",
    "section": "Assessing Linearity",
    "text": "Assessing Linearity\nUnless we add non-linear terms, linear regression assumes the functional form relating the mean of \\(Y\\) to \\(x\\) is a straight line. Violations to this assumption may occur for a variety of reasons including:\n\nceiling or floor effects\nU or inverse-U shaped relationships (e.g., manganese is both a nutrient and toxin)\n\n\n\n\n\n\n\n\n\n\nClaus Henn B, Ettinger AS, Schwartz J, Téllez-Rojo MM, Lamadrid-Figueroa H, Hernández-Avila M, Schnaas L, Amarasiriwardena C, Bellinger DC, Hu H, Wright RO. Early postnatal blood manganese levels and children’s neurodevelopment. Epidemiology. 2010 Jul;21(4):433-9. doi: 10.1097/ede.0b013e3181df8e52. PMID: 20549838; PMCID: PMC3127440.\nhttps://pubmed.ncbi.nlm.nih.gov/20549838/\n\ndf <- tibble::tibble(\n  x = runif(n = 1000, min = -5, max = 10),\n  y = x^2 + 3*x + exp(x/10) + rnorm(n = 1000, sd = 10))\n\nplot(df$x, df$y, main = \"data with nonlinear pattern\")\n\n\n\nplot(df$x, residuals(lm(y~x, df)), main = \"nonlinear residuals\")\n\n\n\n\n\nhers.sample <- na.omit(hers[,c(\"HDL\", \"BMI\")]) |> \n  dplyr::sample_frac(.1)\nlm.hdl.bmi <- lm(HDL ~ BMI, data = hers.sample)\n\nplot(hers.sample$BMI, residuals(lm.hdl.bmi))\n\nlines(lowess(lm.hdl.bmi$residuals~hers.sample$BMI))\n\n\n\n\n\nAssessing Linearity in Multiple Linear Regression\nIn multiple linear regression, we want to assess whether it’s appropriate to assume linearity between a given \\(x_j\\) and \\(Y\\) *after conditioning on all the \\(x_{-j}\\).\nNeither scatterplots of \\(Y\\) vs. \\(x_j\\) nor scatterplots of \\(e\\) vs. \\(x_j\\) allow us to appropriately evaluate this.\nPartial regression plots are a tool that allow us to assess the form of the relationship between \\(x_j\\) and \\(Y\\) after conditioning on all the \\(x_{-j}\\). They are:\n\nA variation of the plot of residuals vs. predictors.\nAlso called “added variable plots” or “adjusted variable plots”.\n\nConsider the candidate model\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i3} + \\varepsilon_i\\]\nand we want to assess whether it is reasonable to assume linearity between \\(x_1\\) and \\(Y\\), conditional on \\(x_2\\) and \\(x_3\\).\nTo create a partial regression plot, we\n\nRegress \\(y\\) on only \\(x_2,\\) \\(x_3\\) and obtain the residuals \\(e_i(y|x_2,x_3)\\). * What’s leftover in \\(Y\\) after conditioning \\(Y\\) after conditioning on \\(x_2, x_3\\).\nRegress \\(x_1\\) on \\(x_2, x_3\\) and obtain the residuals \\(e_i(x_1|x_2,x_3)\\) * What’s leftover in \\(x_1\\) after conditioning \\(x_2, x_3\\)\nPlot \\(e_i(y|x_2,x_3)\\) against \\(e_i(x_1|x_2,x_3)\\) and look for any nonlinearity * This is the association between what’s leftover in \\(Y\\) and \\(x_1\\) after accounting for \\(x_2,x_3\\).\n\n\n# we can use car::av.plots(lm.hdl.bmi)\nlm.ldl.bmi.age <- lm(HDL ~ BMI + age, data = hers)\ncar::avPlots(lm.ldl.bmi.age, main = expression(paste(\"Partial Regression Plot for E(HDL) = \", beta[0] + beta[1], \"BMI + \", beta[2], \"Age\")))\n\n\n\n\nPerks of partial regression plots:\n\nIf we fit a linear model to the data in the plot for \\(x_j\\), the slope estimate would be exactly the same as the estimated linear coefficient for \\(x_j\\) in the full model of \\(Y\\) (conditional on all variables)\nThis tells us that the plots are visualizing the same relationships that are being quantified in the full model\n\nCaveats of partial regression plots\n\nCan’t detect interactions between regressors.\nThe presence of strong multicollinearity can cause partial regression plots to be misleading.\n\n\nRemedial measures for non-linearity\nCould include polynomial terms, but these can behave erratically, particularly in the tails of the distribution.\nModern approaches and advice are to use splines or generalized (GAMs) to capture non-linearity.\nSplines and GAMs can still be unreliable in regions of the predictor space with little data, but are generally more stable than polynomial models.\nWon’t cover them here, but they can be easily operationalized within the linear model framework.\nUsually need to visualize associations since we’re no longer fitting straight lines with a constant slope."
  },
  {
    "objectID": "week4/week4.html#outliers-and-influential-points",
    "href": "week4/week4.html#outliers-and-influential-points",
    "title": "Week 4",
    "section": "Outliers and Influential Points",
    "text": "Outliers and Influential Points\nOutlying observations with large residuals can have a disproportionately large effect on regression results.\nWe want to make sure regression results are not driven by only a few points.\nThese influential points could:\n\nCause associations between an \\(x\\) and \\(Y\\) that would be clearly significant with their inclusion to become non-significant.\nCause associations that would clearly not be significant without their inclusion to become significant.\n\nIf results are significantly impacted by the inclusion or exclusion of a small subset of points, this should be reported. Usually one should report results both with and without those influential points.\nPossible explanations for outliers include:\n\n“Bad” data that results from an explainable (or unexplained) event (e.g., misrecorded measurement, instrument malfunction, etc.). Only in the case when data are corrupted due to an explained event should one discard the observation. In the ideal case (unrealistic as it is), one would go back and try to retrieve the original data.\nInadequacies in the model. In this case, we would not want to exclude observations in any effort to make the data “agree overall with the model”.\nPoor sampling of observations in the tail of the distribution — the data generating process may actually be long-tailed!\n\nIt tends to be the case that large \\(x\\)-outliers have much more influence (or leverage) than \\(y\\)-outliers.\nWe’ll use the terminology influential points for terms we’ve confirmed have high influence on a given regression. Otherwise we’ll use the terms high leverage points or outliers for points that are unique in some regards but unconfirmed as having large influence on our model.\n\n# dataset with a low-influence outlier point \ndf <- tibble::tibble(\n  x = rnorm(n = 1000),\n  y = x*.25 + rnorm(n = 1000, sd = .5))\n\ndf2 <- bind_rows(df, c(x = 0, y = 10))\n\nlm1 <- lm(y ~ x, data = df)\nlm2 <- lm(y ~ x, data = df2)\n\nggplot(df2, aes(x = x, y = y)) + \n  geom_point(alpha = .15, size = 2) + \n  geom_abline(\n    mapping = aes(\n      color = 'w/o outlier',\n      linetype = 'w/o outlier',\n      intercept = coef(lm1)[1],\n      slope = coef(lm1)[2]\n    ),\n    size = 2\n  ) +\n  geom_point(aes(x = 0, y = 10, shape = 'low-influence outlier'), size = 2, color = 'red', alpha = .5) + \n  geom_abline(\n    mapping = aes(\n      color = 'with outlier',\n      linetype = 'with outlier',\n      intercept = coef(lm2)[1],\n      slope = coef(lm2)[2]\n    ),\n    size = 1.75\n  ) +\n  scale_color_manual(values = c(\n    'w/o outlier' = 'cornflowerblue',\n    'with outlier' = 'orange'\n  )) +\n  scale_linetype_manual(values = c('w/o outlier' = 'solid',\n                                   'with outlier' = 'dashed')) + \n  labs(color = 'model type', linetype = 'model type') + \n  theme_bw() + \n  scale_shape_manual(values = c('low-influence outlier' = 2)) + \n  ggtitle(\"Comparing Models with and without the Influential Point\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n# dataset with a high-influence outlier point \ndf <- tibble::tibble(\n  x = rnorm(n = 100),\n  y = x*.25 + rnorm(n = 100, sd = .5))\n\ndf2 <- bind_rows(df, c(x = 7, y = -1))\n\nlm1 <- lm(y ~ x, data = df)\nlm2 <- lm(y ~ x, data = df2)\n\nggplot(df2, aes(x = x, y = y)) + \n  geom_point(alpha = .25, size = 2) + \n  geom_abline(\n    mapping = aes(\n      color = 'w/o outlier',\n      linetype = 'w/o outlier',\n      intercept = coef(lm1)[1],\n      slope = coef(lm1)[2]\n    ),\n    size = 2\n  ) +\n  geom_point(aes(x = 7, y = -1, shape = 'influential point'), size = 2, color = 'red', alpha = .5) + \n  geom_abline(\n    mapping = aes(\n      color = 'with outlier',\n      linetype = 'with outlier',\n      intercept = coef(lm2)[1],\n      slope = coef(lm2)[2]\n    ),\n    size = 1.75\n  ) +\n  scale_color_manual(values = c(\n    'w/o outlier' = 'cornflowerblue',\n    'with outlier' = 'orange'\n  )) +\n  scale_linetype_manual(values = c('w/o outlier' = 'solid',\n                                   'with outlier' = 'dashed')) + \n  labs(color = 'model type', linetype = 'model type') + \n  theme_bw() + \n  scale_shape_manual(values = c('influential point' = 2)) + \n  ggtitle(\"Comparing Models with and without the Influential Point\")\n\n\n\n\n\nQuantifying Measure of Leverage\n\\(h_i\\) (\\(i\\)th diagonal element of \\(H\\)) is known as the leverage of observation \\(i\\).\n\\(h_i\\) is a measure of \\(x_i\\)’s distance from the center in the \\(x\\)-dimension. E.g., in SLR:\n\\[h_i = \\frac{1}{n} + \\frac{(x_i - \\bar x)^2}{\\sum_{i=1}^n (x_i - \\bar x)^2}\\]\n\\(h_i\\) is also the weight given to the observed outcome for unit \\(i\\) when computing its own fitted value.\nSince \\(\\hat y = Hy\\),\n\\[\\hat y_i = H_i y = h_{i1}y_1 + h_{i2} y_2 + \\cdots + h_{ii}y_i + \\cdots + h_{in}y_n\\]\nwhere \\(H_i\\) is the \\(i\\)th row of \\(H\\).\nThis is saying that units whose \\(x_i\\) is further from the mean tend to have the most influence on model fit/fitted values. The average value of \\(h_i\\) is \\(\\bar h = (p+1)/n\\) over all \\(i = 1, ..., n\\).\nA rule of thumb can be to classify observation \\(i\\) as a high leverage point if \\[h_i > 2 \\times \\bar h = 2 (p+1)/n.\\]\n\nleverage <- hat(model.matrix(lm(SBP ~ age, hers |> dplyr::sample_frac(.1))))\nplot(leverage, main = expression(paste(\"Leverage \", h[i], \" values and its rule of thumb threshold\")))\nabline(a = 2 * mean(leverage), b = 0)\n\n\n\n\n\nStudentized Residuals\nIn order to quantify \\(y\\)-outliers, one might use studentized residuals.\nRecall that if the model is correct, the studentized residuals are\n\\[r_i = \\frac{e_i}{\\sqrt{\\widehat{\\text{Var}} (e_i) }}\\] should behave as \\(\\mathcal N(0,1)\\) random variables and hence have constant variance. If the model is correct, we would expect \\(-3 < r_i < 3\\). Thus a residual satisfying \\(|r_i| > 3\\) warrants attention.\n\nhers.ldl.bmi.complete <- hers |> select(LDL, BMI) |> na.omit() |> dplyr::sample_frac(.1)\nstud.resid <- rstandard(lm(LDL~BMI, hers.ldl.bmi.complete))\nplot(hers.ldl.bmi.complete$BMI,stud.resid, ylab=\"Studentized Residuals\",\n  main=\"log(LDL) ~ BMI\", ylim=c(-3.5,3.5))\nabline(h=3, lty=3)\nabline(h=-3,lty=3)\n\n\n\nstud.resid <- rstandard(lm(LDL~BMI, hers.ldl.bmi.complete))\nplot(hers.ldl.bmi.complete$BMI,stud.resid, ylab=\"Studentized Residuals\",\n  main=\"log(LDL) ~ BMI\", ylim=c(-3.5,3.5))\nabline(h=3, lty=3)\nabline(h=-3,lty=3)\n\n\n\n\n\n\nLeave-one-out residuals\nStudentized residuals don’t always catch all outliers. Recall that the studentized residuals have the form\n\\[r_i = \\frac{e_i}{\\sqrt{MSE(1-h_i)}},\\]\nwhere \\(MSE = \\frac{\\sum e_i^2}{n-p-1}\\) and the \\(MSE(1-h_i)\\) term is equal to \\(\\widehat{\\text{Var}}(e_i) = \\hat\\sigma^2 (1-h_i)\\).\nThis formula makes it clear that outliers can conceal themselves by inflating MSE.\nThis motivates “leave-one-out” or “deleted residual”s that use the MSE calculated after removing each observation.\nLet \\(MSE_{(-1)}\\) be the MSE for a model fit to a dataset with the \\(i\\)th observation deleted. The leave one out or jackknife residuals are\n\\[r_{(-i)} = \\frac{e_i}{\\sqrt{MSE_{(-i)}(1-h_i)}}\\]\nBack in the day, it was a bit unthinkable that one would fit a model \\(n\\) times as computational power was lacking, so analytic methods were derived to calculate these leave-one-out and jackknife approaches quickly.\nBecause \\((n-p-1)MSE = (n-p-2)MSE(-i) + e_i^2/(1-h_i)\\) (see KNNL Ch. 10), we can show \\[r_{(-i)} = r_i \\sqrt{\\frac{MSE}{MSE_{(-i)}}} = e_i \\left[\n\\frac{n-p-2}{SSE_{(-i)}(1-h_i)}\n\\right]^{1/2}\\] \\[ = e_i \\left[ \\frac{n-p-2}{SSE(1-h_i) - e_i^2\\right]^{1/2}\\]\nThus we do not need to refit the model \\(n\\) times to come up with the leave-one-out statistics.\nJackknife residuals have a mean near 0 and a variance of\n\\[\\frac{1}{(n-p-1)-1} \\sum_{i=1}^n r^2_{(-i)}\\]\nthat is slightly greater than 1.\nOutliers could be identified using similar thresholds to those used for studentized residuals (-3,3).\nKNNL Ch. 10 also describes a statistical test for identifying outliers using jackknife residuals that are both probably overkill for most problems and perhaps a bit of a relic of the past now.\nBecause just one outlier can artificially inflate MSE, jackknife residuals are the preferred choice for identifying \\(Y\\)-outliers. In R, this is as simple as using rstudent(model.fit).\n\n\n\nInfluence on a Single Fitted Value: DFFITS\n\\[(DFFITS)_i = \\frac{\\hat y_i - \\hat y_{i(-i)}}{\\sqrt{MSE_{(-i)h_i}}}\\]\n\\(\\hat y_{i(-i)}\\) is the fitted value of \\(y_i\\) once observation \\(i\\) is removed from the dataset.\nThe denominator is the estimated standard deviation of \\(\\hat y_i\\) and is based on the MSE calculated from the regression model fit when the \\(i\\)th observation is deleted.\nAny obesrvation with \\(|(DFFITS)_i| > 2 \\sqrt{(p+1)/n}\\) should be inspected closely.\nKNNL Ch 10 shows that this measure can also be re-written to only use quantities from the model fit to the full data.\n\ndffits.sbp.age <- dffits(lm(SBP ~ age, hers |> dplyr::sample_frac(.1)))\nplot(dffits.sbp.age)\nabline(h = 2*sqrt(2/(nrow(hers)*.1)))\nabline(h = -2*sqrt(2/(nrow(hers)*.1)))\n\n\n\n\n\n\nDFBETAS\n\\[(DFBETAS)_{k(i)} = \\frac{\\hat \\beta_k - \\hat \\beta_{k(-i)}}{\\sqrt{\\text{Var}(\\hat \\beta_k}}\\]\nmeasures the influence of each observation on the estimated regression coefficients.\nAs a guideline for identifying influential cases, KNNL recommends considering a case influential if the absolute values of DFBETAS exceeds:\n\n1 for a small to medium dataset\n\\(2/\\sqrt{n}\\) for a large data set (the smaller threshold reflects the large number of DFBETAS).\n\n\nlm.sbp.age <- lm(SBP ~ age, hers |> dplyr::sample_frac(.1))\ndfbetas.sbp.age <- dfbetas(lm.sbp.age)\n\nplot(dfbetas.sbp.age[,2], type='h',xlab= \"Observation Number\", ylab=\"DFBETA BETA1\")\n\nshow.points <- order(-abs(dfbetas.sbp.age[,2]))[1:3]\n\ntext(seq(1,276)[show.points],\ndfbetas.sbp.age[show.points,2], show.points)\n\n\n\n\n\n\nCook’s Distance\nWe can measure the influence of a point on the overall fit using its Cook’s Distance:\n\\[D_i = \\frac{(\\hat y - \\hat y_{(-i)})'(\\hat y - \\hat y_{(-i)}}{(p+1)MSE}\\]\nwhere \\(\\hat y_{(-i)}\\) is the vector of fitted values obtained once observation \\(i\\) is removed from the dataset.\nIt can be shown that \\[D_i = \\underbrace{\\frac{\\overbrace{e_i^2}^{\\text{location in y}}}{(p+1)MSE}}_{\\text{looks like a } F\\text{-statistic}}\\underbrace{\\left( \\frac{h_i}{1-h_i} \\right)}_{\\text{location in x}}.\\]\nThus Cook’s D measures a point’s location both with respect to \\(X\\) and \\(Y\\) and can be computed using only the original model fit.\nKNNL (p. 403) recommend comparing values of Cook’s D to an \\(F(p+1, n-p-1)\\) distribution to determine whether it is “large”. The numerator is clearly a squared normally distributed variable (by assumptions about \\(e_i\\)), and we already know the MSE to be \\(\\chi^2\\) distribution.\nIf the percentile value is less than 10% or 20% then the \\(i\\)th observation has little influence on the fitted values. If it is up\n\nplot(lm.sbp.age, 4)"
  },
  {
    "objectID": "week4/week4.html#what-should-be-done-about-outliers",
    "href": "week4/week4.html#what-should-be-done-about-outliers",
    "title": "Week 4",
    "section": "What should be done about outliers?",
    "text": "What should be done about outliers?\nFaraway (2002) recommends:\n\nCheck data entry errors first.\nCheck physical context — why did it happen.\nRun analyses both with and without influential points. Always report if analyses are sensitive.\nIt is inappropriate to exclude outliers automatically.\n\nIn summary, residual diagnostics are useful for checking:\n\nnon-normality and heteroscedasticity of errors\nnon-linearity\noutliers\n\nTransformations are often useful for addressing apparent violations of the model assumptions.\nLeave-one-out residuals allow us to check to see whether an outlier is concealing itself.\nWe can check the leverage and influence of a given observation using\n\nLeverages \\(h_i\\)\nCook’s distance\nDFFITS\nDFBETAS"
  },
  {
    "objectID": "week4/week4.html#multiple-linear-regression-diagnostics",
    "href": "week4/week4.html#multiple-linear-regression-diagnostics",
    "title": "Week 4",
    "section": "Multiple Linear Regression Diagnostics",
    "text": "Multiple Linear Regression Diagnostics\nContinuing with the matrix notation for MLR used in lecture and last week’s lab. The residuals \\(e = (e_1, ..., e_n)'\\) can be computed using the hat matrix \\(H = X(X'X)^{-1}X'\\):\n\\[e = (I-H)y, \\, \\text{ with variance } \\text{Var}(e) = \\sigma^2 (I-H).\\]\nDefine \\(h_i\\) to be the \\((i,i)\\) value of \\(H\\). Then the residual for the \\(i\\)th observation has \\(\\text{Var}(e_i) = \\sigma^2 (1-h_i)\\). This leads to the studentized residual:\n\\[ r_i = \\frac{e_i}{\\sqrt{\\hat \\sigma^2 (1-h_i)}} = \\frac{e_i}{\\sqrt{MSE(1-h_i)}}\\]\nThese are asymptotically distributed according to a \\(t_{n-p-1}\\) distribution under the model assumptions, which can be approximated as \\(\\mathcal N(0,1)\\) for large \\(n\\).\nThe \\(h_i\\) values represent how far away from the centroid of the \\(X\\) values the \\(i\\)th observation is.\n\nResidual Plots\nResiduals are extremely useful to assessing model assumptions graphically.\n\nHistograms (residuals should appear normal)\nQ-Q Plots (residuals should follow a straight line, reflecting normality)\nScatter plots of residuals against \\(\\hat y\\) or \\(x_{ij}\\) (should be a fluffy, patternless cloud)\n\nPartial regression plots can also be used to assess the assumptions for each predictor on its own. To assess the assumptions for \\(x_{j}\\):\n\nRegress \\(y\\) on \\(\\{ x_1, ..., x_{j-1}, x_{j+1}, ..., x_p \\}\\) and obtain residuals \\(e_i(y|x_{-j})\\).\nRegress \\(x_j\\) on \\(\\{x_1, ..., x_{j-1}, x_{j+1}, ..., x_p \\}\\) and obtain residuals \\(e_i(x_j|x_{-j})\\).\nPlot the residuals \\(e_i(y|x_{-j})\\) on \\(e_i(x_j|x_{-j})\\)."
  },
  {
    "objectID": "week4/week4.html#outliers-leverage-influence",
    "href": "week4/week4.html#outliers-leverage-influence",
    "title": "Week 4",
    "section": "Outliers, Leverage, Influence",
    "text": "Outliers, Leverage, Influence\nOutlying data points have the potential to greatly affect the slope of our fitted regression line. We can calculate several quantities for each data point to assess which ones either potentially or actually exert strong influence on our fitted model."
  },
  {
    "objectID": "week4/week4.html#leverage",
    "href": "week4/week4.html#leverage",
    "title": "Week 4",
    "section": "Leverage",
    "text": "Leverage\nThe leverage of the \\(i\\)th observation is denoted with \\(h_i\\), and it measures how far observation \\(i\\)’s covariates are from the overall covariate average.\nThe average of \\(h_i\\) is \\(\\bar h = (p+1)/n\\) so a rule of thumb is that if $h_i > \nLeave-one-out or jackknife residuals can give us a better sense of the effect of leverage: let the \\((-i)\\) subscript denote a quantity from the model refit without the \\(i\\)th observation.\n\\[r_{(-i)} = \\frac{e_i}{\\sqrt{MSE_{(-i)}(1-h_i)}} = e_i \\left[ \\frac{n-p-2}{SSE(1-h_i) - e_i^2}\\right]^{1/2}.\\]\nThese residuals have mean near 0 and variance slightly greater than 1."
  },
  {
    "objectID": "week4/week4.html#cooks-distance-1",
    "href": "week4/week4.html#cooks-distance-1",
    "title": "Week 4",
    "section": "Cook’s Distance",
    "text": "Cook’s Distance\nHow much does inclusion/exclusion of one particular value affect all of the predictions?\n\\[D_i = \\frac{\\hat y - \\hat y_{(-i)}^T(\\hat y - \\hat y_{(-i)})}{(p+1)MSE} =\n\\frac{e^2_i}{(p+1)MSE}\\left( \\frac{h_i}{1-h_i}\\right) \\]\n\\(D_i\\) values can be compared to a \\(F_{p+1,n-p-1}\\) distribution; observations with percentiles on this distribution above 50% have high influence. We are not saying they should be distributed according to this \\(F\\) distribution. As a rule of thumb, a Cook’s distance value is considered large if \\(d_i > \\frac{4}{n-2}\\)."
  },
  {
    "objectID": "week4/week4.html#dffits",
    "href": "week4/week4.html#dffits",
    "title": "Week 4",
    "section": "DFFITS",
    "text": "DFFITS\nHow much does the inclusion or exclusion of a particular observation affect the prediction for that value?\n\\[(DFFITS)_i = \\frac{\\hat y_i - \\hat y_{(-i)i}}{\\sqrt{MSE_{(-i)}h_i}}\\]\n\\[(DFFITS)_i = \\left( \\frac{h_i}{1-h_i} \\right)^{1/2} \\times e_i \\left[ \\frac{n-p-2}{SSE(1-h_i) - e_i^2}\\right]^{1/2}\\]\nPoints with \\(|DFFITS_i|>2 \\sqrt{\\frac{p+1}{n}}\\) should be investigated. DFFITS value patterns should be close or identical to Cook’s distance since they are conceptually similar.\n##DFBETAS\nWhereas the previous quantities assessed each point’s impact on overall model fit, the \\(DFBETAS_i\\) statistic quantifies how much a particular coefficient \\(\\hat \\beta_k\\) changes if we delete a particular observation \\(i\\).\n\\[(DFBETAS)_{k(i)} = \\frac{\\hat \\beta_k - \\hat \\beta_{k(-i)}}{\\sqrt{\\text{Var}(\\hat\\beta_k)}}.\\]\nThe number is presented in standard error units of how far the “new” \\(\\hat \\beta_k\\) falls from the “old” \\(\\hat\\beta_k\\), so an observation can be considered influential if the absolute value of the \\(DFBETAS\\) exceed 1 (for smaller data sets) or \\(2 / \\sqrt{n}\\) for larger data sets.\n\nDealing with Outliers\n\nDo not just exclude outliers. There is no rule of thumb that can be used to automatically discard observations.\nConsider data measurement errors, data entry errors, coding oddities, etc.\nConsider physical plausibility of measurement and goal of study.\nRun the analyses with and without the observation; Report both if they differ substantially."
  },
  {
    "objectID": "week4/week4.html#exercises",
    "href": "week4/week4.html#exercises",
    "title": "Week 4",
    "section": "Exercises",
    "text": "Exercises\nProve that \\(\\text{Var}(e) = \\sigma^2(I-H)\\). Hint: derive the values of \\(I^T\\), \\(H^T\\), \\(II\\), and \\(HH\\) first.\n\\[HH = X\\cancel{(X'X)^{-1}X'X}(X'X)^{-1}X' = H,\\] \\[I^T = I, \\quad I^2 = I\\] \\[H' = (X(X'X)^{-1}X')' = X''(\\underbrace{(X'X)^{-1}}_{\\text{symmetric}})'X' = X(X'X)^{-1}X'\\]\n\\[\\begin{aligned}\\text{Var}(\\hat e) & = \\text{Var}(y - \\hat y) \\\\\n& = \\text{Var}(y - Hy) \\\\\n& = \\text{Var}((I-H)y) \\\\\n& = (I-H)\\text{Var}(y)(I-H)' \\\\\n& = (I-H)\\sigma^2I(I-H)' \\\\\n& = \\sigma^2(I-H)(I-H)' \\\\\n& = \\sigma^2(I-H)(I-H) \\\\\n& = \\sigma^2(I-H - H + H^2) \\\\\n& = \\sigma^2(I-H)\n\\end{aligned}\\]\n\nTrace\nDefine the trace of an \\(n \\times n\\) square matrix \\(S\\) as \\(tr(S)= \\sum_{i=1}^n s_{i,i}\\), the sum of the diagonal elements.\nProve that for any \\(m \\times n\\) matrix \\(A\\) and an \\(n \\times m\\) matrix \\(B\\), \\(tr(AB) = tr(BA)\\). Use that to prove that traces are invariant to (finite) cyclic permutations. Then conclude that \\(\\bar h = \\sum_{i=1}^n h_i/n = (p+1)/n\\).\n\\[tr(AB) = tr\\left[\n\\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{mn} & \\cdots & a_{mn} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nb_{11} & b_{12} & \\cdots & b_{1m} \\\\\nb_{21} & b_{22} & \\cdots & b_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{nm} & \\cdots & b_{nm} \\\\\n\\end{pmatrix}\n\\right]\\]\n\\[\n= tr\\left[\n\\begin{pmatrix}\n\\sum_{i=1}^n a_i b_{i1} & \\cdots & \\cdots & \\cdots \\\\\n\\cdots & \\sum_{i=1}^n a_{2i} b_i & \\cdots & \\cdots \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\n\\end{pmatrix}\n\\right]\\]\n\\[ \\bar h = \\frac{1}{n} tr(H)\\]\n\\[ = \\frac{1}{n} tr[X(X'X)^{-1}X']\\] \\[ = \\frac{1}{n} tr[X'X(X'X)^{-1}]\\] \\[ = \\frac{1}{n} tr[I_{p+1}] = \\frac{p+1}{n}\\]"
  },
  {
    "objectID": "week3/week3.html#hypothesis-testing-for-nested-models-with-ordinalinteger-variables",
    "href": "week3/week3.html#hypothesis-testing-for-nested-models-with-ordinalinteger-variables",
    "title": "Week 3",
    "section": "Hypothesis Testing for Nested Models with Ordinal/Integer Variables",
    "text": "Hypothesis Testing for Nested Models with Ordinal/Integer Variables\nSuppose we want to model the relationship between year in a PhD program and some outcome where \\(X \\in \\{ 1, 2, 3, 4, 5 \\}\\) and \\(Y\\) is some continuous outcome, and we have reason to believe the association might be linear.\nWe might model that as \\(Y \\sim \\gamma_0 + \\gamma_1 X\\) (model 1). Another way we could model it that is more flexible (not linear) is:\n\\[Y = \\beta_0 + \\beta_1 \\mathbb 1(X = 1) + \\beta_2 \\mathbb 1(X=2)  + \\beta_3 \\mathbb 1(X=3)  + \\beta_4 \\mathbb 1(X=4)  + \\beta_5 \\mathbb 1(X=5) \\quad \\text{(model 2)}\\]\nWhat may not be so obvious is that model 1 is nested inside model 2. The crucial insight is that\n\\[X = \\mathbb 1(X = 1) + 2 \\mathbb 1(X=2) + 3 \\mathbb 1(X=3) +2 \\mathbb 3(X=3) +5 \\mathbb 1(X=5).\\]\nIf we were to substitute this into model 1, we have that \\[\\begin{aligned} Y & = \\gamma_0 + \\gamma_1 (\\mathbb 1(X = 1) + 2 \\mathbb 1(X=2) + 3 \\mathbb 1(X=3) +2 \\mathbb 3(X=3) +5 \\mathbb 1(X=5)) \\\\\n& = \\gamma_0 + \\gamma_1 \\mathbb 1(X = 1) + 2 \\gamma_1 \\mathbb 1(X=2) + 3 \\gamma_1 \\mathbb 1(X=3) + 4 \\gamma_1 \\mathbb 1(X=4) +5 \\gamma_1 \\mathbb 1(X=5)\n\\end{aligned}\\]\nComparing this to model 2, we see that we need to construct a constraint matrix where \\[\\beta_2 = 2 \\beta_1\\] \\[\\beta_3 = 3 \\beta_1\\] \\[\\beta_4 = 4 \\beta_1\\] \\[\\beta_5 = 5 \\beta_1\\]\nWriting out our constraint matrix, we would have that\n\\[\\begin{pmatrix}\n0 & 2 & -1 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & -1 & 0 & 0 \\\\\n0 & 4 & 0 & 0 & -1 & 0 \\\\\n0 & 5 & 0 & 0 & 0 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5\n\\end{pmatrix} =\n\\begin{pmatrix}\n0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\\]\nApplying this constraint to model 2 would reduce it to model 1, showing that model 1 is nested within model 2."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods (BST 232) Notes",
    "section": "",
    "text": "Welcome to my notes on Methods for Simple, Multiple, and Generalized Linear Models!\n\n\n\n\n\nI hope you will enjoy them, but you do have to be prepared: the notation is pretty all over the place.\nI’ve been honing my \\(\\LaTeX\\) tikz and pgfplots skills throughout this course, so I hope you enjoy the diagrams I have diligently been creating throughout.\nBesides the above, here are a couple more figures I’m so thrilled to have been able to learn how to make in \\(\\LaTeX\\).\n\n\n\n\n\n\n\n\n\n\n\n\nYou may be asking, why on Earth would you spend your time learning tikz when it’s so convoluted, tortured, arcane, and generally a pain-in-the-ass?\nI have two reasons:\n\nI find it to be nicer for producing mathematical “infographics” (explanatory graphics) than R’s {ggplot2} which I love to death for data visualization.\n[left to the reader as an exercise]"
  },
  {
    "objectID": "week5/week5.html",
    "href": "week5/week5.html",
    "title": "Week 5",
    "section": "",
    "text": "Model Selection: Substantive, Statistical, and Predictive Criteria\nMain reading for this week is Sections 5.1, 6.1-6.2 from Introduction to Statistical Learning.\nSo far we have assumed that we know what variables should be included in a regression model. We’ve focused on specification and interpretation of the linear regression model and regression diagnostics for testing if we have the correct functional form or verifying the underlying assumptions.\nHowever, we might not be sure which variables should be included in the model to begin with.\nModel selection will be dependent on the study design and objectives.\nInterest could be on:\nIn all situations, it is important that the model be as parsimonious as possible:\nHowever, we don’t want the model to be so overly simplistic that it yields biased estimates.\n\\[SSE = \\sum_{i=1}^n \\left(\ny_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\n\\right)^2\\]\nIn ridge regression the coefficients are estimated by minimizing the SSE while constraining the sum of the squared coefficients:\n\\[\\min_{\\beta} \\left\\{\n\\sum_{i=1}^n \\left(\ny_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\n\\right)\n\\right\\} \\quad s.t. \\quad\n\\sum_{j=1}^p \\beta_j^2 \\leq s.\n\\]\nBy the theory of Lagrange multipliers (Joseph-Louis Lagrange, 1735-1813) for constrained optimization, this is equivalent to minimizing the SSE with a penalty.\nIn particular the ridge regression coefficient estimates \\(\\beta_{\\lambda}^R\\) are the values that minimize\n\\[\n\\sum_{i=1}^n \\left(\ny_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\n\\right)^2 +\n\\lambda \\sum_{j=1}^p \\beta_j^2\n\\]\nwhere \\(\\lambda \\geq 0\\) is a tuning parameter that relates to, but is not the same as \\(s\\).\nThe intuition is that the sum of squared terms drives the minimization to fit the data, but at the same time, the second summation penalizes the \\(\\beta_j\\) coefficients towards zero.\nFor fixed \\(\\lambda\\), the solution \\(\\beta_{\\lambda}^R\\) to the ridge regression problem is given by\n\\[\\hat{\\beta_{\\lambda}^R} = (X'X + \\lambda D)^{-1} X'y\\]\nwhere \\(D = \\text{Diag}(0,1_p)\\).\nInspecting this, we can see that\nTypically one uses cross-validation techniques to determine the optimal value of \\(\\lambda\\) according to pre-specified criteria.\nAs \\(\\lambda\\) increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias."
  },
  {
    "objectID": "week5/week5.html#causal-selection",
    "href": "week5/week5.html#causal-selection",
    "title": "Week 5",
    "section": "Causal Selection",
    "text": "Causal Selection\nIdeally, the primary piece of model building and model selection (choosing what terms to include) should be substantive knowledge.\nOn a causal DAG, an association between two variables (exposure/treatment) and Y (outcome) can arise in 3 ways:\n\n\\(A\\) causes \\(Y\\):\n\n\n\n\n\n\n\n\n\n\n\nConfounding: Common causes of variables considered in the model that are not conditioned on.\n\n\n\n\n\n\n\n\n\n\n\nCollider: Variables that have a common effect on another variable which is conditioned on.\n\n\n\n\n\n\n\n\n\n\nIn the case of confounders, we should adjust for them. On the other hand, we should not adjust for colliders (which creates collider stratification bias).\n\n\n\n\n\n\n\n\n\nAnother scenario we might be interested in is mediation.\n\n\n\n\n\n\n\n\n\nConditioning on a mediator will attenuate the observed effects.\nIf one fits a model like \\(Y \\sim A + M\\) the coefficient on \\(A\\) can be interpreted as the direct effect of \\(A\\) on \\(Y\\) not through \\(M\\)."
  },
  {
    "objectID": "week5/week5.html#statistical-criteria",
    "href": "week5/week5.html#statistical-criteria",
    "title": "Week 5",
    "section": "Statistical Criteria",
    "text": "Statistical Criteria\nSome model selection criteria we might use are \\(R^2\\), Adjusted \\(R^2\\), or the Akaike Information Criterion (AIC).\nRecall that \\[R^2 \\stackrel{def}{=} \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}.\\]\nThe SSE will never go down as we add more predictors. More particularly, the SSE is exactly equivalent to the measure we try to minimize with respect to \\(\\beta\\) (\\(S(\\beta)\\)). As we add more degrees of freedom (or from a linear algebra perspective, adding more orthogonal vectors to the span of our prediction space), we are able to more closely fit the \\(y\\) values. Or in other words, if we add another predictor to the regression that adds no additional explanatory value, then the \\(\\hat \\beta\\) coefficient on that term can be set to 0 and we won’t increase \\(S(\\beta)\\) at all.\nThus \\(R^2\\) has the undesirable property that adding more predictors will never decrease it.\nRecall that adjusted \\(R^2\\) is\n\\[R^2_{adj} = 1 - \\frac{MSE}{SST/(n-1)},\\]\nwhere using MSE instead of the SSE penalizes the addition of predictors that don’t improve model performance.\n\nAkaike Information Criteria\nAIC is a general variable selection criterion defined for any likelihood-based model. For a model with parameters \\(\\theta \\in \\mathbb R^d\\) and log-likelihood \\(\\ell (\\theta)\\)\n\\[\\begin{aligned}AIC & = -2 \\left(\\ell(\\hat \\theta_{MLE}) - d\\right) \\\\\n& = -2 \\ell (\\hat \\theta_{MLE}) + 2d \\end{aligned}\\]\n\\(\\ell(\\hat \\theta)\\) captures goodness of fit on the observed sample.\n\\(d\\) captures model complexity.\nLower AIC is preferred, thus AIC penalizes models with large numbers of parameters (high model complexity) by adding \\(2d\\).\nFor a regression model with \\(n\\) observations and normally distributed errors, the log-likelihood is\n\\[\\ell(\\beta, \\sigma^2 ; y) = c - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\n\\sum_{i=1}^n (y_i - x_i' \\beta)^2.\\]\nPlugging in the MLEs, we have that\n\\[\\ell(\\hat \\beta, \\hat \\sigma^2_{MLE}; y) = c - \\frac{n}{2} \\log(\\hat \\sigma^2_{MLE}) - \\frac{n}{2}\\] subtracting off the number of parameters \\((p+1)\\), and multiplying by \\(-2\\), we have that\n\\[AIC = n \\log(SSE/n) + 2(p+1) + c.\\]\nFrom a set of candidate models, we could then select the model that leads to the lowest AIC.\n\n\nBest subsets selection\nWe could use the best subset selection algorithm to fit separate models for each possible combination of the \\(p\\) predictors and try to pick the best.\n\nBest subsets selection algorithm\n--------------------------------\n  \nFor j = 1,...,p\n  (a) Fit all (p choose j) models containing exactly j predictors.\n  (b) Pick the best model from this set, i.e., the model with the highest R^2\n      or lowest SSE. Call it M_j.\nEnd\nAmong M_1,...,M_p, select the single best model as the one with the highest\nadjusted R^2 or lowest AIC.\n\nExhaustive search for best subsets can be performed in R using the {leaps} package with the function leaps::regsubsets.\nIn addition to considering all model subsets, people have traditionally used automated algorithms for model building. Most commonly these include forward selection, backward elimination, stepwise selection.\nThese typically inflate Type I error rates by doing tons and tons of hypothesis tests.\nFor a discussion of these, see Harrell F.E. (2015) Regression Modeling Strategies."
  },
  {
    "objectID": "week5/week5.html#predictive-performance-train-vs.-test-error",
    "href": "week5/week5.html#predictive-performance-train-vs.-test-error",
    "title": "Week 5",
    "section": "Predictive Performance: Train vs. Test Error",
    "text": "Predictive Performance: Train vs. Test Error\nIn machine learning, model selection is usually aimed at obtaining good predictive performance (small amount of error in predictions).\nVery flexible/complex models are common (not usually worried about interpretation!), so overfitting is a key concern.\nThey usually focus on a variant of the MSE defined as\n\\[MSE^* = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat y_i)^2 = SSE/n\\]\nWhen computed on the sample used to fit the model (or “training data”), by design MSE* always decreases as more predictors are added in linear models.\nThus we should evaluate and compare \\(MSE^*\\) of models when applied to data that is independent from the training data (“test data”).\n\n\n\n\n\nFigure 2.9 from the Introduction to Statistical Learning\n\n\n\n\nThe black line in the left panel shows the true mean \\(\\mathbb E[Y|X]\\). A linear model (orange), a spline model (blue) and a wildly wiggly model (green) are fit. On the right-hand-side, we see that the more complex models overfit the data and start to diverge in the test-data \\(MSE^*\\).\nBecause the data were generated, we know the “true” minimum possible MSE in the test dataset, indicated in the dashed line at \\(Y=1\\).\n\nModel Selection Using Test Error\nSuppose we have fitted candidate model(s) on the training data sample and collected new data. Let \\(m = 1, ..., M\\) index the observations in the test set and \\(y_m^{new}\\) and \\(\\hat y_m^{new}\\) be their observed outcomes and predicted outcomes from a given model fit on the training data, respectively.\nWe define the train and test \\(MSE^*\\) as\n\\[MSE_{train}^* = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat y_i)^2\\] \\[MSE_{test}^* = \\frac{1}{n} \\sum_{i=1}^n (y_{new} - \\hat y_{new})^2\\]\nUsually we don’t have the ability to collect entirely new data to evaluate our models. A simple way to estimate the test error in this setting is to randomly split your one observed dataset into a training and testing and then use the same procedures as in the previous slide.\nDisadvantages:\n\nYou lose samples from the training data.\nResults could depend heavily on the choice of points held out of the model.\n\n\nQuestions:\n\nHow, in general, should one choose what data to hold out? Should it match the prediction problem, somehow?\nShould one use test/train splitting to validate choice of model and then report on a model trained on the entire dataset?\n\n\n\n\nK-Fold Validation\nDivide the observations into \\(K\\) groups (“folds”) of roughly equal size.\nMake \\(K\\) passes, where in pass \\(k = 1, ..., K\\), fold \\(k\\) is treated as a testing set and the rest is a training set.\nEstimate the \\(MSE_{testcv}^*\\) as the average of the \\(K\\) estimates of \\(MSE_{test}^*\\) from each pass and compare \\(MSE_{testcv}^*\\) across candidate models.\n\nShould one calculate dispersion (variance) metrics across the \\(MSE_{testcv, k}^*\\) measures? Because one could imagine that one may not want a model that has low \\(\\bar{MSE_{testcv}^*}\\) but occasionally performs very poorly. This is essentially getting my wondering about reliability. Rachel’s suggestion is to measure the “MSE of the MSE” — here meaning the average squared deviation of the \\(MSE^*_{k, testcv}\\) from \\(\\bar{MSE_{k, testcv}^*}\\).\n\nTypical numbers for \\(K\\) are 5 or 10. One important special case is when \\(K = n\\), leave-one-out cross-validation.\n\n# using a tidymodels approach to k-fold cross-validation: \n# https://www.tidymodels.org/start/resampling/ \nlibrary(tidyverse)\nlibrary(here)\nlibrary(tidymodels)\n\nhers <- readr::read_csv(here::here(\"data/hers.csv\"))\n\nhers_split <- initial_split(hers)\n\nhers_train <- training(hers_split)\nhers_test  <- testing(hers_split)\n\nrf_mod <- \n  rand_forest(trees = 1000) %>% \n  set_engine(\"ranger\") %>% \n  set_mode(\"prediction\")\n\n\n\nn-fold Validation and the PRESS Statistic\nClearly the DFFITS, Cook’s Distance, and jackknife residuals are related to \\(n\\)-fold cross validation (aka leave-one-out cross-validation).\nThe PRESS Statistic is defined\n\\[PRESS = \\sum_{i=1}^n \\left(Y_i - \\underbrace{\\hat Y_{i(-i)}}_{\\substack{\\text{prediction from a model} \\\\ \\text{that doesn't include obs. } i}}\\right)^2\\]\nPRESS is just \\(n \\times MSE_{testcv}^*\\) from leave-one-out cross-validation.\nAs we have seen from other “delete-one” diagnostics, this can be computed from the original fit in linear models.\nOne can use the PRESS() function from the {MPV} package, which will compute the \\(MSE\\) from n-fold cross-validation."
  },
  {
    "objectID": "week5/week5.html#bias-variance-tradeoff",
    "href": "week5/week5.html#bias-variance-tradeoff",
    "title": "Week 5",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\nAs model complexity increases, test error initially decreases as we better approximate the true model form, and then we pass through an inflection point, after-which the model starts to overfit.\nThis is due to the bias-variance decomposition, i.e., that MSE can be decomposed as:\n\\[\\text{MSE}_{\\text{test}}^* = \\text{bias}^2 + \\text{variance} + \\text{noise}.\\]\nFormally, suppose that \\(Y_i = f(x_i) + \\varepsilon_i\\) and we fit some model that gives us predictions \\(\\hat Y_i = \\hat g(x_i).\\) In this class, we usually have that \\(\\hat g(x_i) = x_i' \\hat \\beta\\).\nThen the expected squared error of the prediction for any test data point is\n\\[\\mathbb E((\\hat g(x_0) - Y_0)^2) = (\\mathbb E(\\hat g(x_0)) - f(x_0))^2 + \\text{Var}(\\hat g(x_0)) + \\text{Var}(\\varepsilon)\\]\nhttps://allenkunle.me/bias-variance-decomposition"
  },
  {
    "objectID": "week5/week5.html#penalized-regression-methods-for-variable-selection",
    "href": "week5/week5.html#penalized-regression-methods-for-variable-selection",
    "title": "Week 5",
    "section": "Penalized Regression Methods for Variable Selection",
    "text": "Penalized Regression Methods for Variable Selection\nThe two best-known penalized regression techniques are ridge regression and LASSO.\nThe basic idea is to shrink parameters toward zero. This increases bias but decreases variance.\nIn least-squares, we find coefficients that minimize the SSE. Meanwhile, in penalized regression, we minimize\n\\[SSE + \\lambda \\text{Penalty}(\\beta).\\]\n\\(\\lambda \\geq 0\\) and the penalty function can take various forms.\n\nRidge or \\(\\ell_2\\): \\(\\text{Penalty}(\\beta) = \\sum_{j=1}^p \\beta_j^2.\\)\nLASSO or \\(\\ell_1\\): \\(\\text{Penalty}(\\beta) = \\sum_{j=1}^p |\\beta_j|.\\)"
  }
]