[
  {
    "objectID": "week3/week3.html#variance-of-hat-beta",
    "href": "week3/week3.html#variance-of-hat-beta",
    "title": "Week 3",
    "section": "Variance of \\(\\hat \\beta\\)",
    "text": "Variance of \\(\\hat \\beta\\)\nThe variance of \\(\\hat \\beta\\) is expressed as the variance-covariance matrix\n\\[\\text{Var}(\\hat \\beta) = (X'X)^{-1} X' \\text{Var}(Y) X (X'X)^{-1}\\] \\[ = \\sigma^2 (X'X)^{-1}\\]\nIf we let \\(D = (X'X)^{-1}\\), the variance of \\(\\hat \\beta_j = \\sigma D_{jj}\\) and the covariance between \\(\\hat \\beta_i\\) and \\(\\hat \\beta_j\\) is \\(\\sigma^2 D_{ij}\\).\nHow would we get to this result?\nWe are using a shorthand where we denote \\((X'X)^TX' = A\\), and now we’re just looking at the \\(\\text{Var}(AY)\\). When we are working with the matrix-variance formula, we can rewrite \\(\\text{Var}(AY) = A\\text{Var}(Y)A'\\).\nPlugging in the formula for \\(A\\), we get to the above.\nRemember we said that \\(\\text{Var}(\\epsilon) = \\sigma^2\\) and \\(Y = X\\beta + \\epsilon\\) where the only randomness comes from \\(\\epsilon\\). In other words\n\\[\\text{Var}(\\hat \\beta) = (X'X)^{-1} X' \\text{Var}(Y) X (X'X)^{-1}\\] \\[ = (X'X)^{-1} X' \\text{Var}(X\\beta + \\epsilon) X (X'X)^{-1}\\] \\[ = (X'X)^{-1} X' \\text{Var}(\\epsilon) X (X'X)^{-1}\\] \\[ = (X'X)^{-1} X' \\sigma^2 I X (X'X)^{-1}\\] \\[ = \\sigma^2 (X'X)^{-1} X' I X (X'X)^{-1}\\] \\[ = \\sigma^2 \\cancel{(X'X)^{-1} X' X }\\underbrace{(X'X)^{-1}}_{\\stackrel{set}{=}D}.\\]\n\nGauss-Markov Theorem\n\nUnder the standard linear assumptions, \\(\\hat \\beta_{OLS}\\) is the best linear unbiased estimator (BLUE) for \\(\\beta\\).\nLinear unbiased estimator: \\(\\hat \\beta_{OLS}\\) is a linear combination of the observed \\(y\\) values (given that \\(\\hat \\beta = (X'X)^{-1}X'y\\) is a matrix of constants times a vector \\(y\\)) and is an unbiased estimator.\nIt’s “best” in the sense that it is the lowest variance (most precise).\n\nSo the Gauss Markov Theorem tells us that among all linear, unbiased estimators of \\(\\beta\\), \\(\\hat \\beta_{OLS}\\) has the lowest variance.\n\n\nSimple Linear Regression as a Special Case\nThe least squares estimators \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) can be expressed as\n\\[\\hat \\beta_0 = \\sum_{i=1}^n l_i y_i, \\quad \\hat \\beta_1 = \\sum_{i=1}^n k_i y_i,\\]\nwhere \\(l_i = \\frac{1}{n} - \\frac{\\bar x(x_i - \\bar x)}{\\sum_{i=1}^n (x_i - \\hat x)^2},\\) and \\(k_i = \\frac{(x_i - \\bar x)}{\\sum_{i=1}^n (x_i - \\bar x)^2}\\).\n\nVariance of LS Estimators\n\\[\\text{Var}(\\hat \\beta_0) = \\sigma \\left\\{ \\frac{1}{n} + \\frac{\\bar x^2}{\\sum_{i=1}^n (x_i-\\bar x)^2}\\right\\},\\] \\[\\text{Var}(\\hat \\beta_1) = \\sigma \\left\\{\\frac{1}{\\sum_{i=1}^n (x_i-\\bar x)^2}\\right\\},\\] \\[\\text{Cov}(\\hat \\beta_0, \\hat \\beta_1) = \\sigma \\left\\{ - \\frac{\\bar x}{\\sum_{i=1}^n (x_i-\\bar x)^2}\\right\\}.\\]\nThe variance-covariance matrix is\n\\[\\text{Var}(\\hat \\beta) = \\sigma(X'X)^{-1} = \\left[ \\begin{array}{cc} \\text{Var}(\\hat \\beta_0) & \\text{Cov}(\\hat \\beta_0, \\hat \\beta_1) \\\\ \\text{Cov}(\\hat \\beta_0, \\hat \\beta_1) & \\text{Var}(\\hat \\beta_1) \\end{array} \\right].\\]\n\n\nEstimation of \\(\\sigma^2\\)\nIn order to estimate \\(\\text{Var}(\\hat \\beta)\\), we need an estimator of \\(\\sigma^2\\):\nWe base this on the sum of squared errors (SSE):\n\\[SSE = (y - X\\hat\\beta)'(y-X\\hat \\beta)\\] \\[ = \\sum_{i=1}^n(y_i-x'_i\\hat\\beta)^2\\] \\[ = \\sum_{i=1}^(\\hat \\epsilon_i)^2\\]\n\\[\\hat \\sigma^2 = MSE = \\frac{SSE}{n - p - 1}.\\]\nThis estimator \\(\\hat \\sigma^2\\) is an unbiased estimator.\nThe \\(n-p-1\\) in the denominator is because we estimate \\(p+1\\) parameters and we divide by the degrees of freedom, which is \\(n - \\text{\\# things we had to estimate}\\). The Kutner book has a more rigorous presentation of why this is the right amount to divide by.\n\n\n\nNormality assumption\nIf we are willing to make the stronger assumption that \\(\\epsilon_i \\stackrel{iid}{\\sim} \\mathcal N(0, \\sigma^2)\\), then we can perform inference on \\(\\beta\\).\nFirst note that \\(\\epsilon_i \\stackrel{iid}{\\sim} \\mathcal N(0, \\sigma^2) \\Longrightarrow Y_i \\stackrel{ind}{\\sim} \\mathcal N(x_i'\\beta, \\sigma^2)\\), such that\n\\[f_Y(y_i|\\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left[ -\\frac{1}{2\\sigma^2} (y_i - x_i'\\beta)^2\\right]\\]\nNotice that the \\(Y_i\\) values are independent but not identically distributed.\nBefore we were just assuming that the \\(\\epsilon\\) values were uncorrelated, which in the special case of the normal distribution implies independence, but this isn’t necessarily so for other distributions.\nWe can then use maximum likelihood techniques to obtain \\[\\hat \\beta_{MLE} \\sim MVN_{p+1} \\left[ \\beta, \\sigma^2 (X'X)^{-1} \\right].\\]\n\n\nJoint Density\nRecap of maximum likelihood estimation.\nIn general, suppose we have data \\(Y_1, ..., Y_n\\), which are independent random variables with \\(Y_i\\) having probability density function \\[f_Y(y_i|\\theta)\\] where \\(\\theta\\) is a vector of unknown parameters.\nThen the joint density function of all the \\(y_i\\) given \\(\\theta\\) is the product of the individual densities\n\\[f(y_1, ..., y_n|\\theta) = \\prod_{i=1}^n f_Y(y_i|\\theta).\\]\n\nLikelihood Functions\nThe likelihood function of \\(\\theta\\) given the data has the same form as the joint pdf:\n\\[\\mathcal L(\\theta|y_1,...,y_n) = f(y_1, ..., y_n|\\theta) = \\prod_{i=1}^n f_Y(y_i|\\theta).\\]\nOf course this looks exactly the same as the joint density of the \\(Y_i\\) values, but instead this is a function of \\(\\theta\\) instead of a function of the \\(y_i\\) values.\nOnce you take a random sample of size \\(n\\), the \\(y_i\\) values are known, and the likelihood is considered as a function of unknown parameter \\(\\theta\\).\nThe likelihood function should still integrate to 1.\nThe MLE of \\(\\theta\\) is the value \\(\\hat \\theta\\) that maximizes the likelihood\n\\[\\mathcal L(\\theta | y_1, ..., y_n)\\]\nas a function of \\(\\theta\\).\nThe value \\(\\hat \\theta\\) that maximizes \\(\\mathcal L(\\theta)\\) also maximizes\n\\[\\ell(\\theta | y_1, ..., y_n) = \\log \\mathcal L(\\theta | y_1, ..., y_n).\\]\n\n\nSolving for MLE\n\\[ \\frac{\\partial \\ell}{\\partial \\theta} \\stackrel{set}{=} 0,\\]\nand technically we’re going to need to check that this is a maximum as opposed to a minimum, and we’ll do so by checking that\n\\[\\left[ \\frac{\\partial^2 \\ell}{\\partial \\theta^2} \\right]_{\\theta = \\hat \\theta} < 0.\\]\nIf we were in a matrix setting instead of a vector setting, we’d need to check that the matrix is negative definite for a maximum, or positive definite for a minimum.\nThe negative of the second derivative,\n\\[\\frac{-\\partial^2 \\ell(\\theta | y_1, ..., y_n)}{\\partial \\theta^2},\\]\nis called the information.\n\n\nReturning to MLE for Regression\nThus in the linear regression setting if we assume \\(\\epsilon_i \\stackrel{iid}{\\sim} \\mathcal N(0, \\sigma^2),\\) then \\(Y_i \\stackrel{ind}{\\sim} \\mathcal N(x'_i\\beta, \\sigma^2)\\) and \\[f_Y(y_i|\\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left[ -\\frac{1}{2\\sigma^2} (y_i - x_i'\\beta)^2\\right]\\]\n\\[\\mathcal L(\\beta, \\sigma^2 | y_1, ..., y_n) = \\prod_{i=1}^n f_Y(y_i|\\beta, \\sigma^2),\\]\nand\n\\[\\mathcal L(\\beta, \\sigma^2|y_1, ..., y_n) = \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n\n\\exp \\left[ - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i'\\beta)^2 \\right]\\] \\[= \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n\n\\exp \\left[ - \\frac{1}{2\\sigma^2} (y - X\\beta)'(y-X\\beta) \\right].\\]\nTurning to the log-likelihood function:\n\\[\\ell(\\beta, \\sigma^2 | y_1, ..., y_n) \\propto \\cancel{-n/2\\log(\\sigma^2)} \\underbrace{- \\frac{1}{2\\sigma^2} (y-X\\beta)'(y-X\\beta)}_{= \\frac{-1}{2\\sigma^2} S(\\beta)}.\\]\nThe values that maximize this log-likelihood with respect to \\(\\beta\\), call them \\(\\hat \\beta_{MLE}\\) are the same as those that minimize \\(S(\\hat \\beta)\\), i.e.,\n\\[\\hat \\beta_{MLE} = (X'X)^{-1}X'y\\]\nand it’s straightforward to show that\n\\[\\hat \\beta_{MLE} \\sim MVN_{p+1}\\left[ \\beta, \\sigma^2(X'X)^{-1} \\right].\\]\n\n\n\\(\\sigma^2_{MLE}\\)\nWhile the estimates for \\(\\hat \\beta\\) are the same for OLS vs. MLE, we have that \\[\\hat \\sigma^2_{MLE} = \\frac{1}{n}(y-X\\hat\\beta)'(y-X\\hat\\beta) = \\frac{(n-p-1)}{n}MSE\\]\nSo of note, the MLE for \\(\\beta\\) are the same as the least squares estimator. However the MLE for \\(\\sigma^2\\) is not.\nRecall that the least squares estimator of \\(\\sigma^2\\) is unbiased. The MLE of \\(\\sigma^2\\) is biased, although it is consistent: \\[\\lim_{n\\to\\infty} P(|\\hat\\sigma^2 - \\sigma^2| \\leq \\epsilon) \\to 1, \\, \\forall \\epsilon > 0.\\]"
  },
  {
    "objectID": "week3/week3.html#inference-in-linear-regression",
    "href": "week3/week3.html#inference-in-linear-regression",
    "title": "Week 3",
    "section": "Inference in Linear Regression",
    "text": "Inference in Linear Regression\nOften it’s of interest to determine if, collectively, a group of predictors significantly contribute to the variability in \\(y\\) given another group of predictors are in the model.\nCommon examples are:\n\nIs a categorical variable, represented by dummy variables, significant (analagous to the overall ANOVA F-test)?\nCan the effect of a predictor be represented as a linear effect or is a higher-level polynomial (i.e., using \\(x^2\\), \\(x^3\\), etc.) necessary?\nIs a model that contains only main effects adequate or do we need to incorporate a set of interactions between variables in the models?\n\n\nSum of squares decomposition\n\\[(y_i - \\bar y)^2 = ((y_i - \\hat y_i) + (\\hat y_i - \\bar y))^2\\]\nThen, when computing the sums of squares, we get\n\\[\\sum(y_i - \\bar y)^2 = \\sum_{i=1}^n (\\hat y_i - \\bar y)^2 + \\sum_{i=1}^n (y_i - \\hat y_i)^2,\\]\nwhich happily features a “freshman’s dream”.\nWe thus have that \\[SST = \\underbrace{SSR}_{\\text{explained by regression}} + \\underbrace{SSE}_{\\text{left over}},\\]\nwhere \\(SST = \\text{Sums of Squares Total}\\), \\(SSR = \\text{Sums of Squares Regression}\\), nad \\(SSE = \\text{Sums of Squares Error}\\).\n\nThe ANOVA-like table\nWe often will write something like this type of table:\n\n\n\n\n\n\n\n\n\n\nSource\n\\(SS\\)\n\\(\\text{df}\\)\n\\(\\text{MS}\\)\n\\(\\mathbb E[\\text{MS}]\\)\n\n\n\n\nRegression\n\\(SSR = \\hat \\beta'X'y-n\\bar y^2\\)\n\\(p\\)\n\\(\\frac{SSR}{p}\\)\n\\(\\sigma^2 + \\frac{\\beta'_Rx'_Cx_C\\beta_R}{p}\\)\n\n\nError\n\\(SSE = y'y - \\hat \\beta' X'y\\)\n\\(n-(p+1)\\)\n\\(\\frac{SSE}{n-(p+1)}\\)\n\\(\\sigma^2\\)\n\n\nTotal\n\\(SST=y'y - n \\bar y^2\\)\n\\(n-1\\)\n\n\n\n\n\nwhere \\(MS = \\text{Mean Square Error}\\), and \\[X_c = \\left( \\begin{array}{cccc}\nx_{11}-\\bar{x_1} & x_{12}- \\bar{x_2} & \\cdots & x_{1p}-\\bar{x_p} \\\\\nx_{21}-\\bar{x_1} & x_{22}- \\bar{x_2} & \\cdots & x_{2p}-\\bar{x_p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1}-\\bar{x_1} & x_{n2}- \\bar{x_2} & \\cdots & x_{np}-\\bar{x_p}\n\\end{array}\\right)\\]\n\n\nTesting for Groups of Predictors\nHow do we use this decomposition to test for a group of coefficients?\nThe hypothesis can be formulated as\n\\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_q = 0, q \\leq p\\] \\[H_1: \\text{ at least one of } \\beta_1, ..., \\beta_q \\neq 0.\\]\nAs an aside, tests of the overall regression and tests for a single variable fall within this framework as well:\nThe overall test:\n\\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_p = 0\\] \\[H_1: \\beta_j \\neq 0 \\text{ for at least one } j, j = 1,...,p\\]\nFor a single predictor:\n\\[H_0: \\beta_j = 0\\] \\[H_1: \\beta_j \\neq 0\\]\nWe like the property that testing for significance among a “group of coefficients” reduces in two special cases to either the overall test or a test for an individual coefficient.\nIf we consider the model in matrix form:\n\\[Y = X\\beta + \\epsilon,\\]\nto construct a test based on sums of squares, partition \\(\\beta\\) accordingly:\n\\[\\beta = (\\beta_1^, \\beta_2')',\\]\nwhere \\(\\beta_1\\) is a \\(q \\times 1\\) and \\(\\beta_2\\) is \\((p+1-q) \\times 1\\). We want to test the null hypothesis \\[H_0: \\beta_1 = 0\\] \\[H_1: \\beta_1 \\neq 1\\] and \\(\\beta_2\\) is left unspecified.\nDefining \\(X = \\left[ X_1, X_2 \\right]\\), rewrite the model as\n\\[Y = X_1 \\beta_1 + X_2 + \\beta_2 + \\epsilon.\\]\nNow our model is partitioned so we’re ready to test for significance among the predictors and \\(\\beta\\) coefficients of interest.\nThe full model has SSR expressed as\n\\[SSR(X) = \\hat \\beta' X' y - n \\bar y^2\\]\nand Mean Square Error\n\\[MSE(X) = \\frac{y'y - \\hat \\beta' X' y}{n-p-1}.\\]\nTo find the contribution of \\(X_1\\), fit the model assuming \\(H_0\\) is true. The reduced model is \\[Y = X_2 \\beta_2 + \\epsilon,\\] which yields \\[\\hat \\beta_2 = (X_2'X_2)^{-1}X_2'y \\quad \\text{ and } \\quad SSR(X_2) = \\hat \\beta_2' X_2' y - n' \\bar y^2.\\]\nThe regression sums of squares due to \\(X_1\\) given \\(X_2\\) is in the model is\n\\[SSR(X_1|X_2) = SSR(X) - SSR(X_2)\\]\nwith \\(q\\) degrees of freedom. This is known as the extra sum of squares due to \\(X_1\\) given \\(X_2\\).\nUnder the null hypothesis,\n\\(SSR(X_1|X_2)/\\sigma^2 \\sim \\chi_q^2\\) and \\(SSE/\\sigma^2 \\sim \\chi_{(n-p-1)}^2\\), and these quantities are independent.\nIn general, if one \\(\\chi^2\\) distribution has degrees of freedom \\(d_1\\) and another has \\(d_2\\), then \\((\\chi_{d_1}^2/d_2)/(\\chi_{d_2}^2/d_2) \\sim F_{d_1,d_2}\\) if the two are independent.\nSo we can test \\(H_0: \\beta_1 = 0\\) with the statistic \\[F = \\frac{SSR(X_1|X_2)/q}{MSE(X)} \\stackrel{H_0}{\\sim} F_{q,n-p-1}\\]\nThis \\(F\\) distributional result requires either\n\nnormality of errors \\(\\epsilon_i \\sim \\mathcal N(0,\\sigma^2)\\)\nlarge sample theory\n\nThere’s a handful of things above that we just have to take for granted assumed from a probability & inference class and don’t have time to re-prove here.\nThe \\(F\\) written above is an \\(F\\)-statistic (or \\(F\\)-distributed) because it is the quotient of two \\(\\chi^2\\)-distributed variables divided by their degrees of freedom.\nWith reasonably large sample size, \\(\\mathbb E[F_{q,n-p-1}] \\approx 1\\).\nFor example, one can see that if one runs the regressions:\n\nlm(data = mtcars, hp ~ rnorm(n = nrow(mtcars)))\nsummary(.Last.value)\n#> ... \n#> F-statistic: 0.06081 on 1 and 30 DF\n\nlm(data = mtcars, hp ~ mpg)\n#> ... \n#> F-statistic: 45.46 on 1 and 30 DF\n\nWe can think of this procedure as asking: Is the increase in the regression sums of squares associated with adding \\(q\\) additional predictors, given the presence of the remaining variables in the model, sufficient to warrant removing \\(q\\) additional degrees of freedom from the denominator of MSE?\nAdding an unimportant predictor may increase the MSE, which will increase the uncertainty in the regression coefficient estimates and the variance of \\(\\hat y\\) - so we should include only predictors that explain the response.\nNote however that for the purpose of explanation confounders may not reach significance at given level (e.g. \\(\\alpha = 0.05\\)) but still have a clinically relevant effect on both outcome and exposure and therefore affect the regression coefficients of interest.\n\n\nExample: Test for 2 BMI Terms, HERS Data\n\nlibrary(gt)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nhers <- readr::read_csv(here::here(\"data/hers.csv\"))\n\nRows: 2763 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): HT, raceth, nonwhite, smoking, drinkany, exercise, physact, globra...\ndbl (24): age, medcond, weight, BMI, waist, WHR, glucose, weight1, BMI1, wai...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhers$BMIc <- hers$BMI - mean(hers$BMI, na.rm=TRUE)\n\nlm.ldl.interact <- \n  lm(data = hers %>% filter(! is.na(BMIc)), LDL ~ BMIc*statins + age + nonwhite + drinkany + smoking)\n\nlm.ldl.noBMI <- \n  lm(data = hers %>% filter(! is.na(BMIc)), LDL ~ statins + age + nonwhite + drinkany + smoking)\n\n# perform f-test using anova(reducedModel, fullModel)\nbmi.test <- broom::tidy(anova(lm.ldl.noBMI, lm.ldl.interact))\n\n## format and print results table \ngt(bmi.test) %>% \n  tab_header(title = md(\"**Test of significance of BMI**\"),\n             subtitle = md(\"From LDL model with BMI * statin interaction\")) %>% \n  cols_width(term ~ px(375)) %>% sub_missing(missing_text = '') %>% \n  fmt_number(columns=c('statistic','p.value'),decimals=3) %>% \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Test of significance of BMI\n    \n    \n      From LDL model with BMI * statin interaction\n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LDL ~ statins + age + nonwhite + drinkany + smoking\n2739\n3725955\n\n\n\n\n    LDL ~ BMIc * statins + age + nonwhite + drinkany + smoking\n2737\n3707501\n2\n18454.31\n6.812\n0.001\n  \n  \n  \n\n\n\n\n\n\n\nOverall Test\nUnder the null hypothesis, \\(SSR/\\sigma^2 \\sim \\chi^2_p\\) and \\(SSE/\\sigma^2 \\sim \\chi^2_{n-(p+1)}\\) are independent.\nTherefore we have \\[F = \\frac{SSR/p}{SSE/[n-(p+1)]} = \\frac{MSR}{MSE} \\stackrel{H_0}{\\sim} F_{p,n-p-1}\\]\nWe note that this is reported automatically in a lm().\n\noverall.test <- broom::tidy(anova(lm(data = hers, LDL ~ BMI + age)))\n\ngt(overall.test) %>% \n  tab_header(title = md(\"**Overall test**\"),\n             subtitle = md(\"Model of LDL with BMI and age\")) %>% \n  sub_missing(missing_text = '') %>% \n  fmt_number(columns = c('statistic', 'p.value'), decimals = 3) %>% \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n      Overall test\n    \n    \n      Model of LDL with BMI and age\n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    BMI\n1\n14446.022\n14446.022\n10.155\n0.001\n    age\n1\n7567.195\n7567.195\n5.320\n0.021\n    Residuals\n2744\n3903361.455\n1422.508\n\n\n  \n  \n  \n\n\n\n\nWe can interpret the entries above in the sumsq column as \\(SSR(BMI)\\) and then \\(SSR(age|BMI)\\). These are called the “extra sums of squares” contributed by each variable, and sometimes called the “type 1 sums of squares” (no relation to Type 1 error, but more of a historical idiosyncrasy as a result of how old software [either SAS or S or S-plus] printed these out).\n\\[F = \\frac{(14446 + 7567)/2}{3903361/2744} = 7.74\\]\nUnder \\(H_0\\), \\(F \\sim F_{2, 2744}\\), yielding \\(p = 0.0004458\\).\nWe reject the null hypothesis at \\(\\alpha = 0.05\\) and conclude that at least one of \\(\\beta_1\\) or \\(\\beta_2\\) is not equal to zero.\nOne should note that the above table is one of the places in which order matters because each \\(SSR\\) is conditional on the inclusion of the previously listed variables."
  },
  {
    "objectID": "week3/week3.html#wald-tests",
    "href": "week3/week3.html#wald-tests",
    "title": "Week 3",
    "section": "Wald Tests",
    "text": "Wald Tests\nFor testing individual coefficients \\((H_0: \\beta_j = 0\\) vs \\(H_1: \\beta_j \\neq 0\\)) we can also use the conventional Wald test. To construct the test statistic, consider that\n\\[\\hat \\beta_j \\sim \\mathcal N(\\beta_j, \\sigma^2) D_{jj} \\quad \\text{ and } \\quad\n\\frac{\\hat{\\text{Var}} (\\hat \\beta_j)}{\\sigma^2 D_{jj}} \\sim \\frac{\\chi^2_{n-p-1}}{(n-p-1)}.\\]\nNote that if \\(Z \\sim \\mathcal N(0,1)\\) and \\(S \\sim \\chi^2_d\\) and \\(Z \\perp\\!\\!\\!\\perp S\\) then \\(\\frac{Z}{\\sqrt{S/d}} \\sim t_d\\).\n\\[\\left( \\frac{\\hat \\beta_j - \\beta_j}{\\sqrt{\\sigma^2 D_{jj}}} \\right) \\biggr /\n\\left( \\sqrt{\\frac{\\widehat{\\text{Var}}(\\hat \\beta_j)}{\\sigma^2D_{jj}}} \\right) = \\underbrace{\\boxed{\\frac{\\hat \\beta_j - \\beta_j}{\\sqrt{\\widehat{\\text{Var}}(\\hat \\beta_j)}}}}_{\\text{this should look like a t-statistic}} \\stackrel{H_0}{\\sim} t_{n-p-1}\\]\nIt should be noted that a \\(t^2\\) value where \\(t\\) is a \\(t\\)-statistic follows an \\(F\\)-distribution. This implies that in the case of testing a single coefficient, the \\(t\\)-test and the \\(F\\)-test give the exact same results.\nIn the summary() function, the \\(p\\)-values shown will be from \\(t\\)-tests for each \\(\\beta_j\\), while the \\(F\\)-statistic shown is for the overall model.\n\\[E(LDL_i) = \\beta_0 + \\beta_1 BMI_i + \\beta_2 Age_i\\]\n\nwald.test <- broom::tidy(lm(data = hers, LDL ~ BMI + age))\ngt(wald.test) |> \n  tab_header(title = \n               md(\"**Individual coefficient Wald test**\"),\n             subtitle = \"Test of BMI in model of LDL with age already included\") |> \n  fmt_number(decimals = 3) |> \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n      Individual coefficient Wald test\n    \n    \n      Test of BMI in model of LDL with age already included\n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n151.443\n8.774\n17.260\n0.000\n    BMI\n0.367\n0.132\n2.778\n0.006\n    age\n−0.253\n0.110\n−2.306\n0.021\n  \n  \n  \n\n\n\n\n\\(T = 0.366/0.132 = 2.78 \\quad p = 0.0006\\)\nThis Wald testing strategy extends to testing groups of cofficients\n\\[Y = X_1 \\beta_1 + X_2 \\beta_2 + \\epsilon\\]\nwhere \\(\\beta_1\\) is \\(q \\times 1\\) and \\(\\beta_2\\) is \\((p + 1 - q) \\times 1\\).\n\\[H_0: \\beta_1 = 0\\] \\[H_1: \\beta_1 \\neq 0\\]\nThe multivariate Wald test statistic is \\[W = \\hat \\beta_1' \\left[ \\widehat{\\text{Var}}(\\hat \\beta_1 ) \\right]  \\hat \\beta_1\\]\nUnder the null,\n\n\\((1/q)W \\sim F_{q,n-p-1}\\)\nAsymptotically, \\(W \\sim \\chi_q^2\\)\n\n\nwald.test.group <- broom::tidy(lm.ldl.interact)\n\ngt(wald.test.group) |> \n  tab_header(title = \n               md(\"**LDL model with BMI * statin interaction**\")) |> \n  fmt_number(decimals = 3) |> \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n      LDL model with BMI * statin interaction\n    \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n162.405\n7.583\n21.416\n0.000\n    BMIc\n0.582\n0.160\n3.636\n0.000\n    statinsyes\n−16.253\n1.469\n−11.066\n0.000\n    age\n−0.173\n0.111\n−1.563\n0.118\n    nonwhiteyes\n4.073\n2.275\n1.790\n0.074\n    drinkanyyes\n−2.075\n1.467\n−1.415\n0.157\n    smokingyes\n3.110\n2.167\n1.435\n0.151\n    BMIc:statinsyes\n−0.702\n0.269\n−2.606\n0.009\n  \n  \n  \n\n\n\n\nIn this scenario, \\(H_0: \\beta_2 - \\beta_8 = 0\\).\n\n## Generic function for a Wald test from the output of lm()\n\nwaldTest <- function(fit, vec, digits=c(2,4)) {\n  \n  beta     <- coef(fit)[vec]\n  varMat   <- summary(fit)$cov.unscaled[vec,vec] * (summary(fit)$sigma^2)\n  testStat <- t(beta) %*% solve(varMat) %*% beta \n  pVal     <- 1 - pchisq(testStat, length(vec))\n  value    <- c(Fstat = round(testStat, digits=digits[1]),\n             p = round(pVal, digits=digits[2]))\n  return(value)\n}\n\nwaldTest(lm.ldl.interact, vec = c(2,8))\n\n  Fstat       p \n13.6200  0.0011"
  },
  {
    "objectID": "week3/week3.html#testing-general-linear-hypotheses",
    "href": "week3/week3.html#testing-general-linear-hypotheses",
    "title": "Week 3",
    "section": "Testing general linear hypotheses",
    "text": "Testing general linear hypotheses\nSuppose we are interested in testing linear combinations of the regression coefficients. For example, we may be interested in testing\n\\[H_0: \\beta_i = \\beta_j\\]\nequivalently \\(H_0: \\beta_i - \\beta_j = 0\\).\nSuch hypotheses can be expressed as \\(H_0: C \\beta = 0\\).\nWhere \\(C\\) is an \\(r \\times (p+1)\\) matrix of linearly independent contrasts with \\(r\\) the number of restrictions imposed by the null.\nFor example, consider the model \\[Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i3} + \\epsilon_i,\\]\nand testing the hypothesis \\[H_i : \\beta_1 = 0, \\beta_2 = \\beta_3\\]\nThis could also be written as \\[\\left( \\begin{array}{c} \\beta_1 \\\\ \\beta_2 - \\beta_3 \\end{array} \\right) =\n\\left( \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right)\\]\nThis null hypothesis is equivalent to \\[H_0: \\left( \\begin{array}{cccc} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & -1 \\end{array} \\right)\\beta = 0\\] were \\(\\beta = (\\beta_0, \\beta_1, \\beta_2, \\beta_3)'\\).\nWe can obtain the reduced model by solving \\(C\\beta\\) for \\(r\\) of the regression coefficients in terms of the remaining \\(p+1-r\\) regression coefficients. Substituting these values into the full model will yield a reduced model under the null hypothesis,\n\\[Y = Z \\gamma + \\epsilon,\\]\nwhere \\(\\dim(Z) = n \\times (p+1 -r)\\) matrix and \\(\\dim(\\gamma) = (p + 1 - r) \\times 1\\) vector of regression coefficients.\nThe residual SS for this reduced model is \\[SSE(RM) = y'y - \\hat\\gamma' Z'y \\quad \\quad (n - p - 1 + r \\, \\text{ degrees of freedom})\\]\n\\(SSR(\\text{Full Model}) - SSR(\\text{Reduced Model})\\) is called the sum of squares due to the hypothesis \\(C\\beta=0\\).\nWe can test this hypothesis using \\[F = \\frac{(SSR(FM)-SSR(RM))/r}{MSE} \\stackrel{H_0}{\\sim} F_{r,n-p-1}.\\]\n\nExample with HERS data\nConsider using the physical activity score (1-5):\n\n\n\nPhysact\nActivity\n\n\n\n\n1\nMuch less active\n\n\n2\nSomewhat less active\n\n\n3\nAbout as active\n\n\n4\nSomewhat more active\n\n\n5\nMuch more active\n\n\n\nAn ANOVA model for glucose level regressed on physical activity is\n\\[E(glucose_i) = \\beta_0 + \\beta_1D_{i1} + \\beta_2D_{i2} + \\beta_3D_{i3} + \\beta_4 D_{i4}\\]\nQuestion: For the purposes of predicting glucose level, is the cruder physical activity categorization below adequate?\n\n\n\nCollapsed Physact\nActivity\n\n\n\n\n1\nLess active\n\n\n2\nAbout as active\n\n\n3\nMore active\n\n\n\nRecall the full model is\n\\[E(glucose_i) = \\beta_0 + \\beta_1D_{i1} + \\beta_2D_{i2} + \\beta_3D_{i3} + \\beta_4 D_{i4}\\]\nand this question corresponds to the null hypothesis .\nRemember that we can also write \\(H_0\\) as \\(\\beta_1 - \\beta_2 = 0, \\beta_4 = 0\\). We can think about this as having solved for \\(\\beta_1\\) in terms of \\(\\beta_2\\) or vice-versa.\n\\[H_0: \\beta_1 = \\beta_2, \\beta_4 = 0\\]\nand the reduced model \\[E(glucose_i) = \\beta_0 + \\beta_1(D_{i1} + D_{i2}) + \\beta_3D_{i3}\\]\n\nlm.glucose.pa <- lm(glucose ~ factor(physact), data = hers)\npa.test.fine <- broom::tidy(anova(lm.glucose.pa))\n\ngt(pa.test.fine) |> \n   tab_header(title = \n               md(\"**Overall test of 5-level physical activity**\"),\n                  subtitle = \"Model of glucose with 5 PA categories\") |> \n  fmt_number(decimals = 1) |> \n  tab_options(table.align='left')\n\n\n\n\n\n  \n    \n      Overall test of 5-level physical activity\n    \n    \n      Model of glucose with 5 PA categories\n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    factor(physact)\n4.0\n87,696.5\n21,924.1\n16.5\n0.0\n    Residuals\n2,758.0\n3,662,765.0\n1,328.1\nNA\nNA\n  \n  \n  \n\n\n\nhers$collapsed_physact <- \n  case_when(hers$physact %in% c(\"much less active\", 'somewhat less active') ~ 'less',\n            hers$physact %in% c(\"much more active\", 'somewhat more active') ~ 'more',\n            TRUE ~ hers$physact)\n\nlm.glucose.pacoarse <- lm(glucose ~ factor(collapsed_physact), data = hers)\n\npa.test.coarse <- broom::tidy(anova(lm.glucose.pacoarse))\n\ngt(pa.test.coarse) |> \n  tab_header(\n    title = md(\"**Overall test of 3-level physical activity**\"),\n    subtitle = \"Model of glucose with 3 PA categories\") |> \n  fmt_number(columns = df:meansq,\n             decimals=0) |> \n  fmt_number(columns = statistic:p.value,\n             decimals=2) |> \n  tab_options(table.align = 'left')\n\n\n\n\n\n  \n    \n      Overall test of 3-level physical activity\n    \n    \n      Model of glucose with 3 PA categories\n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    factor(collapsed_physact)\n2\n76,501\n38,250\n28.73\n0.00\n    Residuals\n2,760\n3,673,961\n1,331\nNA\nNA\n  \n  \n  \n\n\n\n\nOur \\(F\\)-test then becomes:\n\\[ \\frac{(87,697 - 76,501)/2}{1330} = 4.21 \\stackrel{H_0}{=} F_{2,2760} \\, \\, (p = 0.0149)\\]\nHow would we get that p-value?\nIn our case we can run:\n\nF_stat <- ((pa.test.fine$sumsq[[1]]-pa.test.coarse$sumsq[[1]])/2) / \n  pa.test.fine$meansq[[2]]\n\nprint(F_stat)\n\n[1] 4.215159\n\npf( # the distribution function (cdf) of the F distribution \n  q = F_stat,\n  df1 = 2,\n  df2 = pa.test.fine$df[[2]],\n  lower.tail = FALSE)\n\n[1] 0.01486524\n\n# compare to the anova table p-value\nanova(lm.glucose.pacoarse, lm.glucose.pa) |> \n  broom::tidy() |> \n  gt() |> \n  tab_header(\n    title = md(\"**ANOVA table comparing the 5-level to 3-level model**\"),\n    subtitle = \"Glucose regressed on physical activity\") |> \n  fmt_number(\n    columns = df.residual:df,\n    decimals = 0) |> \n  fmt_number(\n    columns = statistic:`p.value`,\n    decimals = 3\n    )\n\n\n\n\n\n  \n    \n      ANOVA table comparing the 5-level to 3-level model\n    \n    \n      Glucose regressed on physical activity\n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    glucose ~ factor(collapsed_physact)\n2,760\n3,673,961\nNA\nNA\nNA\nNA\n    glucose ~ factor(physact)\n2,758\n3,662,765\n2\n11195.89\n4.215\n0.015"
  },
  {
    "objectID": "week3/week3.html#confidence-intervals",
    "href": "week3/week3.html#confidence-intervals",
    "title": "Week 3",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nRecall that often we obtain CIs by inverting test statistics.\nThus we can construct a confidence interval for \\(\\beta_j\\) by inverting the univariate \\(t\\)-test.\nFirst, letting \\(c\\) denote \\(t_{n-p-1,1-\\alpha/2}\\), note that \\[P(-c < \\frac{\\hat \\beta_j - \\beta_j}{\\sigma(\\hat \\beta_j)} < c) = 0.95\\]\n\\[ \\Longrightarrow (\\hat \\beta_j - c \\times \\sigma(\\hat \\beta_j) < \\beta_j < \\hat \\beta_j + c \\times \\sigma(\\hat \\beta_j)) = 0.95\\]\n\\[\\hat \\beta_j \\pm t_{n - p -1, 1 - \\alpha/2} \\sqrt{\\hat \\sigma^2 D_{jj}}\\]"
  },
  {
    "objectID": "week3/week3.html#model-estimated-expected-value",
    "href": "week3/week3.html#model-estimated-expected-value",
    "title": "Week 3",
    "section": "Model Estimated Expected Value",
    "text": "Model Estimated Expected Value\nA 100(1-\\(\\alpha)\\)% CI for \\(\\mu(x) = x'\\beta\\) is \\[\\hat \\mu(x) \\pm t_{n-p-1, 1-\\alpha/2} \\sqrt{\\hat \\sigma^2 x' (X' X)^{-1} x}\\]\n\nPrediction Intervals\nA 100(1-\\(\\alpha\\))% prediction interval for a single new observation with covariate values \\(x_{new}\\) is constructed by noting that\n\\[y_{new} = x_{new}' \\beta  + \\varepsilon_{new}\\]\nand \\(\\text{Var}(\\hat y_{new}) = \\sigma^2 x_{new}' (X'X)^{-1}x_{new} + \\sigma^2\\). Then\n\\[\\hat y_{new} \\pm t_{n-p-1, 1 -\\alpha} \\sqrt{ \\hat \\sigma^2 \\left(1 + x_{new}' (X'X)^{-1} x_{new}\\right)}\\]\nwhere \\(\\hat y_{new} = x_{new}' \\hat \\beta\\).\nThus the predictions for \\(y_{new}\\) have have uncertainty both from the estimate of \\(\\hat \\beta\\) and the estimated error variance \\(\\hat \\sigma^2\\).\n\nlm.sbp.age <- lm(SBP ~ age, data = hers)\n\npred_with_ci <- predict(lm.sbp.age, interval = 'confidence')\npred_with_pi <- predict(lm.sbp.age, interval = 'prediction')\n\nWarning in predict.lm(lm.sbp.age, interval = \"prediction\"): predictions on current data refer to _future_ responses\n\nintervals <- data.frame(hers$age, pred_with_ci, pred_with_pi[,2:3])\n\nnames(intervals) <- c('age', 'yhat', 'lwr_ci', 'upr_ci',\n                      'lwr_pi', 'upr_pi')\n\nintervals <- intervals %>% arrange(age)\n\nggplot(intervals, aes(age)) + \n  geom_ribbon(aes(ymin= lwr_pi, ymax = upr_pi, fill = 'prediction interval'), alpha = 0.5) + \n  geom_ribbon(aes(ymin= lwr_ci, ymax = upr_ci, fill = 'confidence interval'), alpha = 0.8) + \n  geom_line(aes(y = yhat)) + \n  geom_jitter(data = hers, aes(age, SBP), size = .75, width = .5, height = 0, alpha = 0.15) + \n  scale_fill_manual(values = c('prediction interval' = 'orange', 'confidence interval' = 'cornflowerblue')) + \n  theme_bw() +\n  labs(y = 'SBP', fill = 'interval type') + \n  ggtitle(\"Regression Prediction and Confidence Intervals\") + \n  theme(legend.position = 'bottom')\n\n\n\n\nNote that we have assumed \\(\\varepsilon \\sim \\mathcal N(0, \\sigma^2)\\) to construct the prediction interval. If the error terms are not close to normal, then the prediction interval could be misleading. This is not the case for the interval for the expected value, which only requires approximate normality for \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\)."
  },
  {
    "objectID": "week3/week3.html#r2-and-adjusted-r2",
    "href": "week3/week3.html#r2-and-adjusted-r2",
    "title": "Week 3",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\\[R^2 = 1 - \\frac{SSE}{SST} = \\frac{SSR}{SST}\\]\nThe proportion of the total variation in \\(Y_i\\) explained by \\(X_i\\).\nBecause \\(0 \\leq SSE \\leq SST\\), \\(0 \\leq R^2 \\leq 1\\).\n\\(R^2\\) increases whenever new terms are added to the model.\nTherefore for model comparison, most people often use a version of the \\(R^2\\) that is adjusted for the number of predictors in the model. This is the adjusted \\(R^2\\), defined as \\[R^2 = 1 - \\left( \\frac{n-1}{\\text{Error df}} \\right) \\frac{SSE}{SST}  = 1 - \\frac{MSE}{SST/(n-1)}\\]\nUsing the MSE is essentially a penalization on wasting unused parameters since \\[MSE = \\frac{\\sum_{i=1}^n (x_i - \\bar x_i)^2}{n - p - 1}.\\]"
  },
  {
    "objectID": "week3/week3.html",
    "href": "week3/week3.html",
    "title": "Week 3",
    "section": "",
    "text": "Lab\nOne perspective is that statistics is really only good for two things:"
  },
  {
    "objectID": "week3/week3.html#variance-estimates-to-confidence-intervals",
    "href": "week3/week3.html#variance-estimates-to-confidence-intervals",
    "title": "Week 3",
    "section": "Variance estimates to confidence intervals",
    "text": "Variance estimates to confidence intervals\nWe know that if \\(Z\\) is a random vector and \\(A\\) is a matrix of constants, then\n\\[\\text{Var}(AZ) = A \\text{Var}(Z) A^T.\\]\nThis implies that\n\\[\\text{Var}(\\hat \\beta) = \\text{Var}[(X^TX)^{-1} X^TY] = \\sigma^2 (X^TX)^{-1}\\]\nThis is a quadratic form. If we were in a scalar world with a variable \\(a \\in \\mathbb R\\) and \\(z\\) a random variable. Then \\(\\text{Var}(az) = a^2 \\text{Var}(z)\\). When we work with matrices, the square of a matrix is analogous to writing \\(AA^T\\)\nRemember that \\(\\hat \\beta = \\underbrace{(X^TX)^{-1} X^T}y\\).\n\\[\\text{Var}(\\beta) = \\left( \\begin{array}{cccc}\n\\text{Var}(\\beta_1) & \\text{Cov}(\\beta_1,\\beta_2) & ... & ... \\\\\n\\text{Cov}(\\beta_2, \\beta_1) & \\text{Var}(\\beta_2) & ... & ... \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\n\\end{array}\\right).\\]\n\\[95\\%CI(\\hat \\beta_j) = \\hat \\beta_j \\pm t_{n-p-1, 1-\\alpha/2} \\hat \\sigma \\sqrt{[(X^TX)^{-1}]_{j,j}}.\\]\nBut we can use the same idea to obtain variance and CI estimates for the prediction of a new/future observation \\(x_{new}\\). Given \\(x_{new}\\) we can predict their mean response \\(\\mathbb E(\\hat y|x_{new}) = x^T_{new}\\hat \\beta\\).\nNote that we cannot predict the response itself since we don’t know what \\(\\varepsilon_{new}\\) is.\nWhen it comes to confidence intervals, we can indeed get two different types of intervals.\n\\[\\text{Predicted mean response: } \\quad \\text{Var}(\\hat{\\mathbb E}(\\hat y|x_{new})) =\n\\text{Var}(x_{new}^T \\hat \\beta) = \\sigma^2 x^T_{new} (X^TX)^{-1} x_{new},\\]\nwhich gives the 95% confidence interval:\n\\[x_{new}^T \\hat \\beta \\pm t_{n-p-1,1-\\alpha/2} \\hat \\sigma \\sqrt{x_{new}^T (X^TX)^{-1} x_{new}}.\\]\n\\[\\text{The predicted response: }\n\\quad \\text{Var}(\\hat y|x_{new}) = \\text{Var}(x_{new}^T \\hat \\beta + \\varepsilon_{new}) = \\sigma^2 x_{new}^T (X^TX)^{-1}x_{new} + \\sigma^2,\\]\nwhich gives the 95% prediction interval:\n\\[x_{new}^T \\hat \\beta \\pm t_{n-p-1, 1-\\alpha/2} \\hat \\sigma \\sqrt{1 + x_{new}^T(X^TX)^{-1}x_{new}}.\\]\nIn general, we can’t predict \\(y_new = \\hat \\beta_0 + \\hat \\beta_1 x_1 + ... + \\varepsilon_{new}\\) because we don’t know \\(\\varepsilon_{new}\\). However, we can predict \\[\\text{Var}(\\hat y_{new}|x_{new}) = \\text{Var}(\\hat \\beta_0 + \\hat \\beta_1 x_1 + ...) + \\underbrace{\\text{Var}(\\varepsilon_{new})}_{=0,\\, \\text{ by assumption}}.\\]\nWhy should these values be \\(t\\)-distributed? Because finite samples of the \\(\\beta\\) distributed \nOften we just write \\(\\mathbb E(y) = X\\beta\\), but this isn’t really the full model. It’s missing an assumption: \\(\\text{Var}(y) = \\sigma^2I_n\\) where \\(I_n\\) is the \\(n \\times n\\) identity matrix.\nWe get \\(\\hat \\beta\\) often from either \\(OLS\\) or \\(MLE\\), and we get the \\(\\hat \\sigma^2\\) from the MSE.\nWhy do we care about these values? One way is to just say “center and spread” — but a more sophisticated way is to realize that these two statistics completely characterize a normal distribution.\nThe moment generating function says that \\[D(Y) = 1 + \\mathbb Ey + \\text{Var}(y) + \\text{Skew}(y) + \\text{Kurtosis}(y) + ...\\]\nConfidence intervals tell us what are the plausible range of model parameters."
  },
  {
    "objectID": "week3/week3.html#hypothesis-testing",
    "href": "week3/week3.html#hypothesis-testing",
    "title": "Week 3",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nHypothesis testing is about having a hunch and seeing if we’re right.\nFor multiple linear regression, there are many options for testing the significance of predictor variables. These tests fall under two main categories: F tests and Wald tests. Both are asymptotically equivalent, so yield comparable results in hypothesis testing.\nThe F-test is looking at the variances. The Wald test is looking at the behavior of the means.\n\nF-tests (comparison of variances)\nRecall that\n\nSST (Total Sum of Squares) is a measure of the total variance in the outcome from the given sample.\nSSR (Regression Sum of Squares) represents the total variance in the outcome explained by the regression model.\nSSE (Sum of Squared Errors) represents the remaining total variance in the outcome that is not captured by the regression model. We use the SSE to estimate the true (unobserved) variance \\(\\sigma^2\\) of the residuals. Let \\(\\hat \\sigma^2 = MSE = SSE/(n-p-1)\\) for a model with \\(p\\) predictors and an intercept.\n\n\\[SST \\stackrel{def}{=} \\sum_{i=1}^n (Y_i - \\bar Y)^2 = \\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2 + \\sum_{i=1}^n (Y_i - \\hat Y_i)^2 \\stackrel{def}{=} SSR + SST\\]\nThe \\(F\\)-tests are:\n\nTest for no covariate effect \\(H_0: \\beta_1 = ... = \\beta_p = 0\\)\n\n\\[F = \\frac{SSR/p}{SSE/(n-p-1)} = \\frac{SSR/p}{MSE} \\sim F_{p,n-p-1}.\\] 2. Test for a single variable \\(x_j\\). \\(H_0 : \\beta_j = 0\\).\n\\[F = \\frac{[SSR - SSR(\\text{model without } x_j)]/1}{SSE/(n-p-1)}\\] \\[ =\n  \\frac{SSR - SSR(\\text{model without } x_j)}{MSE} \\sim F_{1, n-p-1}.\\]\n\nTest for a group of \\(r\\) variables. \\(H_0: \\beta_{j_1} = ... = \\beta_{j_r} = 0\\).\n\n\\[F = \\frac{[SSR - SSR(\\text{model without } r \\text{ variables})]/r}{SSE/(n-p-1)}\\] \\[ = \\frac{[SSR - SSR(\\text{model without } r \\text{ variables})]/r}{MSE} \\sim F_{1,n-p-1}.\\]\n\nTest of a general linear hypothesis, \\(H_0: C\\beta= 0\\) where \\(C\\) is an \\(r \\times (p+1)\\) matrix of linearly independent contrasts. The test requires calculation of the sum of squares for the reduced model, where we parameterize according to the null hypothesis.\n\n\\[F = \\frac{(SSR - SSR(\\text{reduced model}))/r}{MSE} \\sim F_{r,n-p-1}\\]\nThe MSE is from the full model.\nNote: the reduced model must be nested within the full model for us to use the \\(F\\)-test.\n\nA great question is to look at this formula and say that the numerator is a marginal quantity, so why shouldn’t the denominator also be marginal?\nOne reason might be that if it were, the denominator would no longer have the same degrees of freedom as the numerator.\nAnother reason, arguably more important, is that this quantity would no longer be \\(F\\)-distributed, and we really want to have a quantity with nice asymptotic distributional properties.\n\nLet’s look at an example of situation 4:\nThe full model might be \\[\\mathbb E(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\]\nAnd model 2 is given as \\[\\mathbb E(y) = \\gamma_0 + \\gamma_1 x_1\\]\nThen\n\\[\\left( \\begin{array}{cccc} 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{array} \\right) \\left( \\begin{array}{c} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\end{array} \\right) = \\left( \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right)\\]\nIn general, when we say that the reduced model must be nested within the full model, what we’re saying is that the reduced model can be expressed as a linear constraint imposed on the full model.\nFor example, we might have a scenario where \\(Age \\in \\{ 57, 58, 59 \\}\\) and our reduced model is \\[y = \\beta_0 + \\beta_1 Age\\]\nAnd our full model is \\[y = \\gamma_0 + \\gamma_1 \\mathbb 1(Age = 57) + \\gamma_2 \\mathbb 1(Age = 58) + \\gamma_3 \\mathbb 1 (Age = 59)\\]\nIf we make a new variable that is \\[57 \\mathbb 1(Age = 57) + 58 \\mathbb 1(Age = 58) + 59 \\mathbb 1 (Age = 59)\\], and this recovers the original age variable.\n\nThe \\(F\\)-test can be thought of as a cost-benefit ratio."
  },
  {
    "objectID": "week3/week3.html#wald-test-comparison-of-means",
    "href": "week3/week3.html#wald-test-comparison-of-means",
    "title": "Week 3",
    "section": "Wald test (comparison of means)",
    "text": "Wald test (comparison of means)\nAs we touched on in lecture, we can also consider an alternative hypothesis test formulation for linear regression—this will also help motivate the form of our confidence intervals below. Recall the following properties: For an \\(r \\times (p+1)\\) matrix \\(C\\) and a random vector \\(y\\), then\n\\[\\mathbb E(Cy) = C\\mathbb E(y) \\quad \\text{and} \\quad \\text{Var}(Cy) = C\\text{Var}(y)C^T\\]\nAs we have shown previously, \\[\\hat \\beta \\sim MVN_{p+1} (\\beta, \\sigma^2 C(X^TX)^{-1}C^T)\\]\nSo, to test the general linear hypothesis \\(H_0: C\\beta = 0\\), we have the following statistic (which generalizes the familiar univariate Wald statistic \\(W = \\hat \\beta^2 / \\widehat{\\text{Var}}(\\hat \\beta) \\sim \\chi^2_1\\), asypmptotically):\n\\[W = (C\\hat \\beta)^T\\underbrace{[\\hat \\sigma^2 C(X^TX)^{-1}C^T]^{-1}}_{\\text{variance of } C\\beta}(C\\hat \\beta)\\]\nIf we think of what we’d do in a standard intro stats class, we’d do one of two things.\n\nWe’d look at \\(\\hat \\beta_1 / \\hat \\sigma(\\hat \\beta_1)\\), which is \\(t\\)-distributed.\nOr we’d look in \\(\\hat \\beta_1^2/\\widehat{Var}(\\hat \\beta_1)\\) which is \\(F\\)-distributed and asymptotically \\(\\chi^2\\) distributed.\n\nThen by properties of the \\(F\\)-statistic, \\(W/r \\sim F_{r,n-p-1}\\), and also asymptotically \\(W \\sim \\chi_r^2\\)."
  },
  {
    "objectID": "week3/week3.html#additional-remarks",
    "href": "week3/week3.html#additional-remarks",
    "title": "Week 3",
    "section": "Additional Remarks",
    "text": "Additional Remarks\nNote that the univariate Wald Statistic \\(W = \\hat \\beta^2 / \\widehat{\\text{Var}}(\\hat \\beta) \\sim \\chi_1^2\\) is equivalent to the \\(t\\)-test statistic\n\\[t_{obs} = \\frac{\\hat \\beta_1}{s.e.(\\hat\\beta_1)} \\sim t_{n-2,(1-\\alpha/2)}.\\]\nWhile we will not go into this in detail, the theoretical motivation for all of these tests is that they are ratios of \\(\\chi^2\\)-distributed random variables that are scaled by their degrees of freedom, which in turn defines the \\(F\\)-distribution.\nThe \\(F\\)-test is valid if and only if the \\(\\chi^2\\) assumption for the numerator and denominator holds true. Yet, \\(\\chi^2\\) random variables are obtained from the sums of squared normal random variables. This is why it is necessary for the residuals \\(\\varepsilon_i\\) to follow a normal distribution and/or 2) large sample theory to hold (so that the \\(\\varepsilon_i\\) may be approximately normal).\nThese \\(F\\)-tests are sometimes given in the context of an analysis of variance (ANOVA) model and table, which presents the sum of squares explained by each variable, conditional on all previous variables in the model, and then the sum of squares for the error terms. Each test compares two models, one nested within the other. Nested models will come up again when we discuss likeli- hood ratio tests, of which the F -test is a special case under the assumption of normally-distributed outcomes.\nA lack of significance of the effect of a covariate or group of covariates does not necessarily indicate the absence of an effect. Whether to remove these covariates from the model will depend on the scientific goal of the study."
  }
]