---
title: Week 14
---

::: content-hidden
$$
\newcommand{\E}[0]{\mathbb E}

\newcommand\independent{\perp\!\!\!\perp}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcommand{\Var}[0]{\text{Var}}
\newcommand{\Cov}[0]{\text{Cov}}
\newcommand{\Pr}[0]{\mathrm{Pr}}
\newcommand{\e}[0]{\epsilon}
\newcommand{\t}[1]{\text{#1}}
\newcommand{\bmat}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\logit}[0]{\text{logit}}
\newcommand{\expit}[0]{\text{expit}}
$$

window.MathJax = {
  loader: {load: ['[tex]/cancel']},
  tex: {packages: {'[+]': ['cancel']}}
};
:::

# Preface

We're continuing on to matched case-control studies and conditional logistic regression.

Review Moral: Only logistic regression can be applied "same as always" in case
control studies to estimate odds ratios as long as one doesn't care about the 
intercept (which is biased).

Today we'll talk about difficulties with matched case control study. 

## North Carolina Birth Example Data

Suppose interest lies in the relationship between gestational age at birth and 
infant mortality, i.e., death within the first year of life. 

The `infants` dataset has information on 225,152 births for all white and 
African-American births in North Carolina in 2003-04. 

Infant mortality is a rare event: 1,752 events in the available data constituting
a rate of 8 per 1,000 births.

In the absence of these complete data, we would need to conduct a study. 

## Emulating a case-control study

Suppose we have sufficient resources to collect information on $n=400$ births. 

Simple random sampling would yield few cases, probably around the average of 3
deaths. 

A case-control design would be much more efficient: 

  * Randomly sample $n_1 = 200$ cases from the 1,752 infant deaths
  * Randomly sample $n_0 = 200$ controls from the 223,400 non-deaths
  * Retrospectively 'observe' their covariate values, $X$, and 
  * Include information on a number of potential confounders such as: 
   - Mothers' age, smoking during pregnancy, weight gained, baby's race and sex
  
```{r}
load(here::here("data/NorthCarolina_data.dat"))

## create an indicator of case/control status
infants$Y <- as.numeric(infants$dTime < 365)

table(infants$Y)

## randomly sample 200 cases and 200 controls
cases <- sample(c(1:nrow(infants))[infants$Y == 1], 200)
conts <- sample(c(1:nrow(infants))[infants$Y == 0], 200)
dataCC <- infants[c(conts, cases),]

## confirming that we have 200 cases/controls in dataCC
table(dataCC$Y)

## check covariate distribution
tapply(dataCC$sex, dataCC$Y, FUN = mean)
tapply(dataCC$race, dataCC$Y, FUN = mean)
tapply(dataCC$smoker, dataCC$Y, FUN = mean)
```

## Covariate imbalance in case-control studies 

There appears to be substantial imbalance in the distributions of several 
key covariates between the cases and controls. 

Many of these covariates are also likely to be strongly associated with gestational 
age at birth, suggesting that: 

  * Strong confounding may be present 
  * We may need to adjust for a large number of covariates 
  * There may be areas of covariate space (i.e., combinations of covariate values)
  for which there's little or no variability in the exposure and/or outcome

It would be desirable to have greater balance between cases and controls. One 
way of achieving this is through matching. 

:::{.hottip}
Usually in a GLM, we think of the $Y$ as being random;  however, in this 
setting, the $X$ variables are themselves random. 

Do recall that we worked out a retrospective likelihood in logistic regression
showing that it gives the same estimates as the prospective likelihood that
we're used to. 

This means we can proceed treating $Y$ as random conditional on $X$ and get
the same results as if we'd treated $X$ as random. 
:::

# Matching

The case-control design assumes the population can be stratified on the basis 
of $Y$. 

Suppose we also have access to information on certain covariates for everyone: 
e.g., we know the sex and race of the baby, but we don't (readily) have information
on the mother or the pregnancy.  We'll denote this set of covariates by $Z$. 

We could impose balance on $Z$ by drawing controls such that the distribution of $Z$
is the same as in cases. E.g., each time you draw a case, only draw from the 
controls with the same (or very similar) values of $Z$. 

In matched case-control studies, the observations are structured into a series of 
"matched sets". Often each matched set contains a single case, a fixed common number 
of controls, $M$, which is referred to as an $M:1$ matched case-control study. 

In general, we will allow for: 

  * $K$ matched sets (also sometimes called 'strata')
  * $N_k$ individuals in the $k$th set

The data structure is given as: 

  * $Y_{ki}$ being the outcome for the $i$th individual in the $k$th matched
  set $(k = 1, ..., K; \;\; i = 1, ..., N_k)$. 
  * $X_{ki}$ is the corresponding covariate vector (assume now that the $Z$ variables
  have been removed from $X$, so they're no longer included [one can see why shortly]).

The data might look like:

| Set | $Y$ | $X_1, ..., X_p$ |
| --- | --- | --------------- |
| 1   | 1   | ...             |
| 1   | 0   | ...             |
| 1   | 0   | ...             |
| 2   | 1   | ...             |
| 2   | 0   | ...             |
| 2   | 0   | ...             |

Recall the retrospective likelihood: $\mathcal L_R = \prod P(X_i | Y_i)$. 

Recall we were able to rewrite that as a function of the prospective likelihood
multiplied by a nuisance term.

By having matched, we have introduced a new form of non-randomness. Retrospective
sampling of $X$ is only random within each matched set. 

The appropriate retrospective likelihood is thus:
$$\mathcal L_r = \prod_{k=1}^K \prod_{i=1}^{N_k} P(X_{ki} | Y_{ki}, \text { set } k).$$

We could follow the same arguments as before and base estimation/inference on the 
prospective likelihood

$$\mathcal L^* = \prod_{k=1}^K \prod_{i=1}^{N_k} P(Y_{ki} | X_{ki}, \text{ set } k, S = 1)$$

again ignoring the constraints imposed by the fixed number of cases and controls. 

Note for what follows, we'll often be implicitly conditioning on $S$. 

The likelihood requires specification of the distribution of $Y$ conditional 
on both $X$ and being in the $k$th matched set. 

This can be achieved by including a set-specific intercept into the model.
For a single covariate, 

$$\logit P(Y_{ki} = 1 \mid X_{ki}, \text{ set }k) = \alpha_k + \beta_1 X_{ki}.$$

As with a standard case-control study, we could proceed with estimation of $\{ \alpha_1, ..., \alpha_K, \beta_1 \}$ by ignoring the (retrospective) sampling scheme
(Prentice and Pyke, 1979).

However, the asymptotics are not well-behaved because the number of parameters
increases with the sample size due to the set-specific intercepts. 

:::{.bluetip}
One student suggests that: 

> Instead of $\alpha_k$, if one introduces all-way interactions between the $Z$ 
variables, one recovers the same $\beta$ estimates. In the epidemiology 
literature, this is talked about as equivalence between the conditional 
and unconditional logistic regression in matched case-control studies. 
:::

The solution is that set-specific intercepts should be treated as nuisance parameters. 
Interest lies in $\beta_1$. The interpretation of the $\alpha_k$ values doesn't
correspond to anything real, the matched set constructs that they represent are
artificial. 

The $K$ intercepts $\{ \alpha_1, ..., \alpha_K \}$ are nuisance parameters. 

Two general techniques for eliminating nuisance parameters include: 

  * Marginalizing (or integrating) over them, or
  * Conditioning 

### Foray into Sufficient Statistics

:::{.cooltip}
Definition: Consider random variables $W_1, ..., W_n \sim f_{\psi}$ whose
distribution depends on some unknown parameter $\psi$. Then $T = g(W_1, ... W_n)$ 
is a sufficient statistic for $\psi$ if the conditional distribution 
$W_1, ..., W_n \mid T$ does not depend on $\psi$. 
:::

We can use the strategy of conditioning on a sufficient statistic to eliminate 
nuisance parameters. 

This will require us to work with a conditional likelihood. 

Sufficient statistics are covered in greater detail in statistical inference classes. 
For our purposes here, we don't need to know how to identify sufficient statistics, just
how to use them to eliminate nuisance parameters.

For exponential or exponential dispersion family models, the sufficient statistic
is almost always a sum. 

## Conditional Logistic Regression 

We use the conditioning approach to derive a likelihood that depends on the $\beta_1$
but not the $\{ \alpha_k \}$. 

The sufficient statistic for $\alpha_k$ is $T_k = \sum_{i=1}^{N_k} Y_{ki}$, i.e., 
the total number of cases in matched set $k$. 

We will form a conditional likelihood using the conditional distribution of the observed
data $Y_k = (Y_{k1}, ..., Y_{kN_k})'$ given $T_k$ for each matched set $k$. 

We use the conditional likelihood just as we would any other: 

  * Solve score equations to obtain <span class='vocab'>conditional maximum likelihood estimates</span>
  * Calculate Fisher information from the second derivatives of the log of this
  conditional likelihood. 
  * Use the inverse Fisher information for variance estimation. 

Let's first write out the unconditional prospective likelihood

$$
\begin{aligned}
\mathcal L^* & = \prod_{k=1}^K \prod_{i = 1}^{N_k} P(Y_{ki} = y_{ki} \mid X_{ki}, \text{ set } k) \\ 
& = \prod_{k=1}^K \prod_{i = 1}^{N_k} [P(Y_{ki} = 1 \mid X_{ki}, \text{ set } k)]^{y_{ki}} [P(Y_{ki} = 0 \mid X_{ki}, \text{ set } k)]^{1-y_{ki}} \\ 
& = \prod_{k=1}^K \prod_{i = 1}^{N_k} 
\left[ \frac{\exp \{\alpha_k + \beta_1 X_{ki} \}}{1 + \exp \{\alpha_k + \beta_1 X_{ki} \}} \right]^{y_{ki}} \left[ \frac{1}{1 + \exp \{\alpha_k + \beta_1 X_{ki} \}} \right]^{1-y_{ki}} \\ 
& = \prod_{k=1}^K \prod_{i = 1}^{N_k} \frac{\exp \{\alpha_k + \beta_1 X_{ki} \}^{y_{ki}}}{1 + \exp \{\alpha_k + \beta_1 X_{ki} \}} \\ 
& = \prod_{k=1}^K \frac{\exp \{ \sum_{i} y_{ki} \alpha_k + \beta_1 \sum_i y_{ki} X_{ki} \}}{\prod_i [1 + \exp \{\alpha_k + \beta_1 X_{ki} \}]} \\ 
& = \prod_{k=1}^K \frac{\exp \{ t_k \alpha_k + \beta_1 \sum_i y_{ki} X_{ki} \}}{\prod_i [1 + \exp \{\alpha_k + \beta_1 X_{ki} \}]} 
\end{aligned}
$$

We'll call each product term the likelihood of the $k$th matched set. 

Deriving the conditional likelihood:

$Y_k = (Y_{k1}, ..., Y_{kN_k})'$ conditional on $T_k = \sum_{i=1}^{N_k} Y_{ki}$ is

$$P(Y_k = y_k \mid T_k = t_k) = \frac{P(Y_k = y_k, \, T_k = t_k)}{P(T_k = t_k)},$$

by a simple application of Bayes' rule. 

(We're implicitly conditioning on $X$ throughout)

Numerator: 

$$
\begin{aligned}
P(Y_k = y_k, \, T_k = t_k) & = P(Y_k = y_k) I(t_k = \sum_i y_{ki}) \\
& = \frac{\exp \{ t_k \alpha_k + \beta_1 \sum_i y_{ki} X_{ki} \}}{\prod_i [1 + \exp \{\alpha_k + \beta_1 X_{ki} \}]} I(t_k = \sum_i y_{ki}).
\end{aligned}
$$

The denominator:

$$
P(T_k = t_k) = \sum_{y_k : T_k = t_k} P(Y_k = y_k) = \sum_{y_k : T_k = t_k} \frac{\exp \{ t_k \alpha_k + \beta_1 \sum_i y_{ki} X_{ki} \}}{\prod_i [1 + \exp \{\alpha_k + \beta_1 X_{ki} \}]} 
$$

The denominator of the denominator term can pull out, since it doesn't depend on the sum, 
and thus it will cancel with the denominator in the sum. 

It follows that 

$$
\begin{aligned}
P(Y_k = y_k \mid T_k = t_k) & = \frac{I(t_k = \sum_i y_{ki}) \times \exp \{ t_k \alpha_k + \beta_1 \sum_i y_{ki} X_{ki} \}}{\sum_{u_k : T_k = t_k} \exp \{ t_k \alpha_k + \beta_1 \sum_i u_{ki} X_{ki} \}} \\ 
& = \frac{I(t_k = \sum_i y_{ki}) \times \exp \{ \beta_1 \sum_i y_{ki} X_{ki} \}}{\sum_{u_k : T_k = t_k} \exp \{\beta_1 \sum_i u_{ki} X_{ki} \}}.
\end{aligned}
$$

Because we can pull out the $t_k \alpha_k$ terms from inside the $\exp$ and then they 
just cancel. 

Taking the product over the $K$ sets gives the conditional likelihood: 

$$\mathcal L_C(\beta_1) = \prod_{k=1}^K = \frac{I(t_k = \sum_i y_{ki}) \times \exp \{ \beta_1 \sum_i y_{ki} X_{ki} \}}{\sum_{u_k : T_k = t_k} \exp \{\beta_1 \sum_i u_{ki} X_{ki} \}}.$$

This is solely a function of $\beta_1$. By conditioning on $\{ T_1, ..., T_K\}$, the 
likelihood is no longer a function of nuisance parameters. 

Suppose there is only one case in the matched set and they occupy the first
index: 

  * $Y_{k1} = 1 \quad \forall k$ 
  * $Y_{ki} = 0 \quad \forall i \in \{ 2, ..., N_k \}$
  * $T_k = 1, \quad \forall k$. 

The conditional likelihood simplifies to

$$
\begin{aligned}
\mathcal L_C(\beta_1) & = \prod_{k=1}^K \frac{\exp\{ \beta_1 X_{k1}\}}{\sum_i \exp\{ \beta_1 X_{ki}\}} \\ 
& = \prod_{k=1}^K \frac{\exp\{ \beta_1 X_{k1}\}}{\exp\{ \beta_1 X_{k1}\} + ... + \exp\{ \beta_1 X_{kN_k}\}}.
\end{aligned}
$$

## Practical Considerations

Matched sets with no exposure variability do not contribute to the likelihood, as 
in when all the $X_{ki}$ are the same within the set.

For example, in a 1:1 matched study, the contributions by any set for which 
$X_{k1} = X_{k2}$ is 

$$\frac{\exp\{\beta_1 X_{k1}\}}{\exp\{ \beta_1 X_{k1}\} + \exp\{ \beta_1 X_{k2}\}} = \frac{1}{2},
$$

which is independent of $\beta_1$ and so do not contribute any information. 

Analyses using conditional logistic regression for matched data thus only use
matched sets with discordant exposures. 

A consequence of this is that we cannot use conditional logistic regression to learn about 
covariates used in the matching process. 

Suppose we conducted a matched case-control study of the association between 
gestational age and infant mortality which was
  
  * Matched on child sex at birth 
  * Within each matched set, there is no variation in sex 
  * None of the matched sets can contribute to estimation of the effect of 
  sex at birth 

A benefit of having no variation within each set is that we do not have to 
worry about sex as a confounder. 

Consider the interpretation of $\beta_1$ in the underlying logistic regression:

$$\logit P(Y_{ki} = 1 \mid X_{ki}) = \alpha_k + \beta_1 X_{ki}$$

By holding the matched set constant, sex is implicitly held constant. 

