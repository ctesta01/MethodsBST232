---
title: Week 14
---

::: content-hidden
$$
\newcommand{\E}[0]{\mathbb E}

\newcommand\independent{\perp\!\!\!\perp}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcommand{\Var}[0]{\text{Var}}
\newcommand{\Cov}[0]{\text{Cov}}
\newcommand{\Pr}[0]{\mathrm{Pr}}
\newcommand{\e}[0]{\epsilon}
\newcommand{\t}[1]{\text{#1}}
\newcommand{\bmat}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\logit}[0]{\text{logit}}
\newcommand{\expit}[0]{\text{expit}}
$$

window.MathJax = {
  loader: {load: ['[tex]/cancel']},
  tex: {packages: {'[+]': ['cancel']}}
};
:::

# Preface

We're continuing on to matched case-control studies and conditional logistic regression.

Review Moral: Only logistic regression can be applied "same as always" in case
control studies to estimate odds ratios as long as one doesn't care about the 
intercept (which is biased).

Today we'll talk about difficulties with matched case control study. 

## North Carolina Birth Example Data

Suppose interest lies in the relationship between gestational age at birth and 
infant mortality, i.e., death within the first year of life. 

The `infants` dataset has information on 225,152 births for all white and 
African-American births in North Carolina in 2003-04. 

Infant mortality is a rare event: 1,752 events in the available data constituting
a rate of 8 per 1,000 births.

In the absence of these complete data, we would need to conduct a study. 

## Emulating a case-control study

Suppose we have sufficient resources to collect information on $n=400$ births. 

Simple random sampling would yield few cases, probably around the average of 3
deaths. 

A case-control design would be much more efficient: 

  * Randomly sample $n_1 = 200$ cases from the 1,752 infant deaths
  * Randomly sample $n_0 = 200$ controls from the 223,400 non-deaths
  * Retrospectively 'observe' their covariate values, $X$, and 
  * Include information on a number of potential confounders such as: 
   - Mothers' age, smoking during pregnancy, weight gained, baby's race and sex
  
```{r}
load(here::here("data/NorthCarolina_data.dat"))

## create an indicator of case/control status
infants$Y <- as.numeric(infants$dTime < 365)

table(infants$Y)

## randomly sample 200 cases and 200 controls
cases <- sample(c(1:nrow(infants))[infants$Y == 1], 200)
conts <- sample(c(1:nrow(infants))[infants$Y == 0], 200)
dataCC <- infants[c(conts, cases),]

## confirming that we have 200 cases/controls in dataCC
table(dataCC$Y)

## check covariate distribution
tapply(dataCC$sex, dataCC$Y, FUN = mean)
tapply(dataCC$race, dataCC$Y, FUN = mean)
tapply(dataCC$smoker, dataCC$Y, FUN = mean)
```

## Covariate imbalance in case-control studies 

There appears to be substantial imbalance in the distributions of several 
key covariates between the cases and controls. 

Many of these covariates are also likely to be strongly associated with gestational 
age at birth, suggesting that: 

  * Strong confounding may be present 
  * We may need to adjust for a large number of covariates 
  * There may be areas of covariate space (i.e., combinations of covariate values)
  for which there's little or no variability in the exposure and/or outcome

It would be desirable to have greater balance between cases and controls. One 
way of achieving this is through matching. 

:::{.hottip}
Usually in a GLM, we think of the $Y$ as being random;  however, in this 
setting, the $X$ variables are themselves random. 

Do recall that we worked out a retrospective likelihood in logistic regression
showing that it gives the same estimates as the prospective likelihood that
we're used to. 

This means we can proceed treating $Y$ as random conditional on $X$ and get
the same results as if we'd treated $X$ as random. 
:::

# Matching

The case-control design assumes the population can be stratified on the basis 
of $Y$. 

Suppose we also have access to information on certain covariates for everyone: 
e.g., we know the sex and race of the baby, but we don't (readily) have information
on the mother or the pregnancy.  We'll denote this set of covariates by $Z$. 

We could impose balance on $Z$ by drawing controls such that the distribution of $Z$
is the same as in cases. E.g., each time you draw a case, only draw from the 
controls with the same (or very similar) values of $Z$. 

In matched case-control studies, the observations are structured into a series of 
"matched sets". Often each matched set contains a single case, a fixed common number 
of controls, $M$, which is referred to as an $M:1$ matched case-control study. 

In general, we will allow for: 

  * $K$ matched sets (also sometimes called 'strata')
  * $N_k$ individuals in the $k$th set

The data structure is given as: 

  * $Y_{ki}$ being the outcome for the $i$th individual in the $k$th matched
  set $(k = 1, ..., K; \;\; i = 1, ..., N_k)$. 
  * $X_{ki}$ is the corresponding covariate vector (assume now that the $Z$ variables
  have been removed from $X$, so they're no longer included [one can see why shortly]).

The data might look like:

| Set | $Y$ | $X_1, ..., X_p$ |
| --- | --- | --------------- |
| 1   | 1   | ...             |
| 1   | 0   | ...             |
| 1   | 0   | ...             |
| 2   | 1   | ...             |
| 2   | 0   | ...             |
| 2   | 0   | ...             |

Recall the retrospective likelihood: $\mathcal L_R = \prod P(X_i | Y_i)$. 

Recall we were able to rewrite that as a function of the prospective likelihood
multiplied by a nuisance term.

By having matched, we have introduced a new form of non-randomness. Retrospective
sampling of $X$ is only random within each matched set. 

The appropriate retrospective likelihood is thus:
$$\mathcal L_r = \prod_{k=1}^K \prod_{i=1}^{N_k} P(X_{ki} | Y_{ki}, \text { set } k).$$

We could follow the same arguments as before and base estimation/inference on the 
prospective likelihood

$$\mathcal L^* = \prod_{k=1}^K \prod_{i=1}^{N_k} P(Y_{ki} | X_{ki}, \text{ set } k, S = 1)$$

again ignoring the constraints imposed by the fixed number of cases and controls. 

Note for what follows, we'll often be implicitly conditioning on $S$. 

The likelihood requires specification of the distribution of $Y$ conditional 
on both $X$ and being in the $k$th matched set. 

This can be achieved by including a set-specific intercept into the model.
For a single covariate, 

$$\logit P(Y_{ki} = 1 \mid X_{ki}, \text{ set }k) = \alpha_k + \beta_1 X_{ki}.$$

As with a standard case-control study, we could proceed with estimation of $\{ \alpha_1, ..., \alpha_K, \beta_1 \}$ by ignoring the (retrospective) sampling scheme
(Prentice and Pyke, 1979).

However, the asymptotics are not well-behaved because the number of parameters
increases with the sample size due to the set-specific intercepts. 

:::{.bluetip}
One student suggests that: 

> Instead of $\alpha_k$, if one introduces all-way interactions between the $Z$ 
variables, one recovers the same $\beta$ estimates. In the epidemiology 
literature, this is talked about as equivalence between the conditional 
and unconditional logistic regression in matched case-control studies. 
:::

The solution is that set-specific intercepts should be treated as nuisance parameters. 
Interest lies in $\beta_1$. The interpretation of the $\alpha_k$ values doesn't
correspond to anything real, the matched set constructs that they represent are
artificial. 

The $K$ intercepts $\{ \alpha_1, ..., \alpha_K \}$ are nuisance parameters. 

Two general techniques for eliminating nuisance parameters include: 

  * Marginalizing (or integrating) over them, or
  * Conditioning 

### Foray into Sufficient Statistics

:::{.cooltip}
Definition: Consider random variables $W_1, ..., W_n \sim f_{\psi}$ whose
distribution depends on some unknown parameter $\psi$. Then $T = g(W_1, ... W_n)$ 
is a sufficient statistic for $\psi$ if the conditional distribution 
$W_1, ..., W_n \mid T$ does not depend on $\psi$. 
:::

We can use the strategy of conditioning on a sufficient statistic to eliminate 
nuisance parameters. 

This will require us to work with a conditional likelihood. 

Sufficient statistics are covered in greater detail in statistical inference classes. 
For our purposes here, we don't need to know how to identify sufficient statistics, just
how to use them to eliminate nuisance parameters.

For exponential or exponential dispersion family models, the sufficient statistic
is almost always a sum. 

## Conditional Logistic Regression 

We use the conditioning approach to derive a likelihood that depends on the $\beta_1$
but not the $\{ \alpha_k \}$. 

The sufficient statistic for $\alpha_k$ is $T_k = \sum_{i=1}^{N_k} Y_{ki}$, i.e., 
the total number of cases in matched set $k$. 

We will form a conditional likelihood using the conditional distribution of the observed
data $Y_k = (Y_{k1}, ..., Y_{kN_k})'$ given $T_k$ for each matched set $k$. 

We use the conditional likelihood just as we would any other: 

  * Solve score equations to obtain <span class='vocab'>conditional maximum likelihood estimates</span>
  * Calculate Fisher information from the second derivatives of the log of this
  conditional likelihood. 
  * Use the inverse Fisher information for variance estimation. 

Let's first write out the unconditional prospective likelihood

$$
\begin{aligned}
\mathcal L^* & = \prod_{k=1}^K \prod_{i = 1}^{N_k} P(Y_{ki} = y_{ki} \mid X_{ki}, \text{ set } k) \\ 
& = \prod_{k=1}^K \prod_{i = 1}^{N_k} [P(Y_{ki} = 1 \mid X_{ki}, \text{ set } k)]^{y_{ki}} [P(Y_{ki} = 0 \mid X_{ki}, \text{ set } k)]^{1-y_{ki}} \\ 
& = \prod_{k=1}^K \prod_{i = 1}^{N_k} 
\left[ \frac{\exp \{\alpha_k + \beta_1 X_{ki} \}}{1 + \exp \{\alpha_k + \beta_1 X_{ki} \}} \right]^{y_{ki}} \left[ \frac{1}{1 + \exp \{\alpha_k + \beta_1 X_{ki} \}} \right]^{1-y_{ki}} \\ 
& = \prod_{k=1}^K \prod_{i = 1}^{N_k} \frac{\exp \{\alpha_k + \beta_1 X_{ki} \}^{y_{ki}}}{1 + \exp \{\alpha_k + \beta_1 X_{ki} \}} \\ 
& = \prod_{k=1}^K \frac{\exp \{ \sum_{i} y_{ki} \alpha_k + \beta_1 \sum_i y_{ki} X_{ki} \}}{\prod_i [1 + \exp \{\alpha_k + \beta_1 X_{ki} \}]} \\ 
& = \prod_{k=1}^K \frac{\exp \{ t_k \alpha_k + \beta_1 \sum_i y_{ki} X_{ki} \}}{\prod_i [1 + \exp \{\alpha_k + \beta_1 X_{ki} \}]} 
\end{aligned}
$$

We'll call each product term the likelihood of the $k$th matched set. 

Deriving the conditional likelihood:

$Y_k = (Y_{k1}, ..., Y_{kN_k})'$ conditional on $T_k = \sum_{i=1}^{N_k} Y_{ki}$ is

$$P(Y_k = y_k \mid T_k = t_k) = \frac{P(Y_k = y_k, \, T_k = t_k)}{P(T_k = t_k)},$$

by a simple application of Bayes' rule. 

(We're implicitly conditioning on $X$ throughout)

Numerator: 

$$
\begin{aligned}
P(Y_k = y_k, \, T_k = t_k) & = P(Y_k = y_k) I(t_k = \sum_i y_{ki}) \\
& = \frac{\exp \{ t_k \alpha_k + \beta_1 \sum_i y_{ki} X_{ki} \}}{\prod_i [1 + \exp \{\alpha_k + \beta_1 X_{ki} \}]} I(t_k = \sum_i y_{ki}).
\end{aligned}
$$

The denominator:

$$
P(T_k = t_k) = \sum_{y_k : T_k = t_k} P(Y_k = y_k) = \sum_{y_k : T_k = t_k} \frac{\exp \{ t_k \alpha_k + \beta_1 \sum_i y_{ki} X_{ki} \}}{\prod_i [1 + \exp \{\alpha_k + \beta_1 X_{ki} \}]} 
$$

The denominator of the denominator term can pull out, since it doesn't depend on the sum, 
and thus it will cancel with the denominator in the sum. 

It follows that 

$$
\begin{aligned}
P(Y_k = y_k \mid T_k = t_k) & = \frac{I(t_k = \sum_i y_{ki}) \times \exp \{ t_k \alpha_k + \beta_1 \sum_i y_{ki} X_{ki} \}}{\sum_{u_k : T_k = t_k} \exp \{ t_k \alpha_k + \beta_1 \sum_i u_{ki} X_{ki} \}} \\ 
& = \frac{I(t_k = \sum_i y_{ki}) \times \exp \{ \beta_1 \sum_i y_{ki} X_{ki} \}}{\sum_{u_k : T_k = t_k} \exp \{\beta_1 \sum_i u_{ki} X_{ki} \}}.
\end{aligned}
$$

Because we can pull out the $t_k \alpha_k$ terms from inside the $\exp$ and then they 
just cancel. 

Taking the product over the $K$ sets gives the conditional likelihood: 

$$\mathcal L_C(\beta_1) = \prod_{k=1}^K = \frac{I(t_k = \sum_i y_{ki}) \times \exp \{ \beta_1 \sum_i y_{ki} X_{ki} \}}{\sum_{u_k : T_k = t_k} \exp \{\beta_1 \sum_i u_{ki} X_{ki} \}}.$$

This is solely a function of $\beta_1$. By conditioning on $\{ T_1, ..., T_K\}$, the 
likelihood is no longer a function of nuisance parameters. 

Suppose there is only one case in the matched set and they occupy the first
index: 

  * $Y_{k1} = 1 \quad \forall k$ 
  * $Y_{ki} = 0 \quad \forall i \in \{ 2, ..., N_k \}$
  * $T_k = 1, \quad \forall k$. 

The conditional likelihood simplifies to

$$
\begin{aligned}
\mathcal L_C(\beta_1) & = \prod_{k=1}^K \frac{\exp\{ \beta_1 X_{k1}\}}{\sum_i \exp\{ \beta_1 X_{ki}\}} \\ 
& = \prod_{k=1}^K \frac{\exp\{ \beta_1 X_{k1}\}}{\exp\{ \beta_1 X_{k1}\} + ... + \exp\{ \beta_1 X_{kN_k}\}}.
\end{aligned}
$$

## Practical Considerations

Matched sets with no exposure variability do not contribute to the likelihood, as 
in when all the $X_{ki}$ are the same within the set.

For example, in a 1:1 matched study, the contributions by any set for which 
$X_{k1} = X_{k2}$ is 

$$\frac{\exp\{\beta_1 X_{k1}\}}{\exp\{ \beta_1 X_{k1}\} + \exp\{ \beta_1 X_{k2}\}} = \frac{1}{2},
$$

which is independent of $\beta_1$ and so do not contribute any information. 

Analyses using conditional logistic regression for matched data thus only use
matched sets with discordant exposures. 

A consequence of this is that we cannot use conditional logistic regression to learn about 
covariates used in the matching process. 

Suppose we conducted a matched case-control study of the association between 
gestational age and infant mortality which was
  
  * Matched on child sex at birth 
  * Within each matched set, there is no variation in sex 
  * None of the matched sets can contribute to estimation of the effect of 
  sex at birth 

A benefit of having no variation within each set is that we do not have to 
worry about sex as a confounder. 

Consider the interpretation of $\beta_1$ in the underlying logistic regression:

$$\logit P(Y_{ki} = 1 \mid X_{ki}) = \alpha_k + \beta_1 X_{ki}$$

By holding the matched set constant, sex is implicitly held constant. 

When we match on a continuous covariate, we have to be careful about residual 
confounding. Suppose we matched on mothers' age in 5-year age bands (≤20, 21-25, 40-45, ...).
Within any given set there will likely remain some variation in the mothers exact age leading
to residual confounding. 

The remedy is to include mothers' age as a covariate in the model. However,
one should be careful about interpreting the coefficient since one has already controlled
for most of the effect of age by controlling for the age-bands. 

<!-- 
Something I'm curious about is fuzzy matching;
Like could you use something like a radial basis function / Gaussian kernel 
to ascertain matches
--> 

### Example: Diabetes and acute MI 

Consider a matched case-control study among Navajo Indians.

144 cases experienced myocardial infarction (MI) who were matched to 144 controls who 
were free of heart disease. (This constitutes 1:1 matching based on age and sex.)

They recorded whether or not each individual had diabetes:

| Diabetic among Cases | Diabetic among Controls | N   |
| -------------------- | ----------------------- | --- |
| 0                    | 0                       | 82  |
| 0                    | 1                       | 16  |
| 1                    | 0                       | 37  |
| 1                    | 1                       | 9   |

```{r}
set <- rep(1:144, rep(2, 144))

mi <- rep(c(1,0), 144)

diab <- c(rep(c(0,0), 82), rep(c(0,1), 16), rep(c(1,0), 37), rep(c(1,1), 9))

navajo <- data.frame(set, mi, diab)

head(navajo)
```

```{r}
library(survival)

fit0 <- clogit(mi ~ diab + strata(set), data = navajo)

summary(fit0)
```

What do we get if we ignore the matching? 
Fitting a logistic regression model, we get
an odds ratio estimate and 95% CI of 2.23 (1.28, 3.89).

Ignoring the matching will, in general, bias the estimated odds ratio towards
the null. This is most evident when matching on important predictors of the outcome. 

:::{.bluetip}
It's important to remember when interpreting the results that one has to 
emphasize they're conditional on the matched covariates.
:::

# Count Data 

"This course considers the use of regression as a tool for data analysis
with outcomes that are univariate and independent."

So far, we've primarily considered two types of outcomes: 

Continuous outcomes, $Y \in \mathbb R$ where 

$$Y_i \sim \mathcal N(\mu_i, \sigma^2)$$
$$\mu_i = x_i' \beta$$

And binary outcomes,

$$Y_i \sim \text{Bernoulli}(\mu_i)$$
$$\logit(\mu_i) = x_i'\beta$$

In many applications, the response variable is in the form of a count:

$Y = 0, 1, 2, ...$

Examples could include: 

  * The number of major complications following surgery 
  * The number of incident cases of cancer in each Boston neighborhood in 2010 
  * The number of prescribed treatment doses (e.g., pills) taken

When we count events, we can consider them as arising in one of two ways: 

  * From a fixed number of trials 
    * i.e., Binomial data 
  * Over some time-frame and/or spatial area 
    * i.e., Poisson data 

## Binomial count data 

Recall that a binomial random variable is the number of successes from $N$
independent (Bernoulli) trials with a common probability of success. 

In applications with binomial count outcomes, data are often of the form: 

| \# Trials | \# Successes | Covariates |
| --------- | ------------ | ---------- |
| $N_1$     | $Y_1$        | $x_1$      |
| $N_2$     | $Y_2$        | $x_2$      |
| ...       | ...          | ...        |
| $N_n$     | $Y_n$$       | $x_n$      |

<!-- why do I practically never hear about Beta regression --> 

An example might be a study investigating factors associated
with medication adherence where each individual's outcome 
is the count of medication doses taken (out of a fixed number of 
doses prescribed).

If we assume that each individual's $N_i$ trials are independent and have probability 
of success $\pi_i$, then 

$$Y_i \sim \text{Binomial}(N_i, \pi_i)$$

$\E[Y_i] = \mu_i = N_i \pi_i$ and $\Var[Y_i] = N_i \pi_i(1-\pi_i)$.

Since $N_i$ is fixed, we can structure a model for $\pi_i$ (which
implies a model for $\mu_i$). 

$$g(\pi_i) = x_i' \beta \quad \Longrightarrow \quad \mu_i = \underbrace{g^{-1}(x_i \beta)}_{=\pi_i} \times N_i$$

We can choose $g(\cdot)$ from among the usual candidates. 

  * Logit is the most common
  * And coefficient estimates have analagous interpretations to the binary outcome 
  setting. 

In many settings, rather than $N_i$ trials on the $i$th study unit, the count data 
we observe are collapsed binary data. 

Consider this Ohio lung cancer death data:

| Age   | Sex | Race      | N      | Death |
| ----- | --- | --------- | ------ | ----- |
| 55-64 | M   | White     | 429617 | 1025  |
| 55-64 | M   | Non-White | 48382  | 172   |
| 55-64 | F   | White     | 476170 | 507   |
| 55-64 | F   | Non-White | 54662  | 81    |
| ...   | ... | ...       | ...    | ...   |

These data are disguised as binomial counts, but they are technically 
collapsed binary outcomes. 

This dataset consists of a total of $n=12$ rows, but represents outcomes for 
$N = 2,220,177$ individuals. 


Let $Y_{ij} = 0/1$ be a binary indicator of death due to lung cancer for the $j$th 
individual in the $i$th age/sex/race-stratum. 

$i = 1,...,12$, $j = 1,...,N_i$, and let $\pi_{ij} = P(Y_{ij} = 1)$ and suppose 

$$
\begin{aligned}
\logit(\pi_{ij}) = \beta_0 + & 
\beta_{A1} \mathbb{1}(\text{Age}_{ij} = "65-74") 
+ \beta_{A2} \mathbb{2}(\text{Age}_{ij} = "75-84") + \\ 
\beta_S \mathbb {1}(\text{Sex}_{ij} = "F") + 
\beta_R \mathbb 1 (\text{Race} = \text{"Non-White"})
\end{aligned}$$

But under this model, $\pi_{ij} = \pi_i$ for all individuals in age/sex/race 
stratum $i$. So for individuals in the 65-74 age range who are women and 
non-white:

$$\pi_{ij} = \pi_i = \expit \{ \beta_0 + \beta_{A1} + \beta_S + \beta_R \}$$

Assuming independence across individuals, the likelihood is:

$$\begin{aligned}
\mathcal L(\beta) = & \prod_{i=1}^{12} \prod_{j=1}^{N_i} \pi_i^{y_{ij}} (1-\pi_i)^{1-y_{ij}} \\ 
& = \prod_{i=1}^{12} \pi_{i}^{\sum_j y_{ij}} (1-\pi_i)^{\sum_j (1-y_{ij})} \\ 
& = \prod_{i=1}^{12} \pi_{i}^{Y_i}(1-\pi_i)^{N_i - Y_i}
\end{aligned}$$ 

Note that this only differs from a binomial likelihood for the collapsed outcmoes in that there's no ${N_i \choose Y_i}$. But the ${N_i \choose Y_i}$ term doesn't involve $\beta$
anyway. 

Thus we can treat the 12 collapsed observations as if they were binomial
counts and model them as described in the collapsed table.

In `R` we can fit GLMs to binomial count data with a slight modification 
to the syntax used to fit GLMs to binary data. The only thing that 
looks different is that we change the left-hand-side:

```{r}
#| eval: false
glm(cbind(Death, N-Death) ~ factor(Age) + Sex + Race,
  data = Ohio,
  family = binomial())
```

As with logistic regression analyses of binary data, exponentiate 
to report contrasts on the odds ratio scale: 

```{r}
#| eval: false
jtools::export_summs(fit0, exp=T, error_pos = 'right',
  error_format = "[{conf.low}, {conf.high}]")
```

## Poisson Count Data 

In many settings, we're interested in a count of events but there's no 
notion of a fixed number of trials. E.g., the number of major
complications during a surgery. 

A counting process $Y(t)$, counts the number of events in some population by 
time $t$ (starting from a designated time 0). 

As we move forward in time, $Y(t)$ counts the number of events or 'jumps'. 

$$Y(0) = 0 \; \longrightarrow \; Y(t_1) = 1 \; \longrightarrow \; Y(t_2) = 2 ...$$

A counting process is a <span class='vocab'>time-homogeneous Poisson process</span> under certain assumptions.

Crucially, the rate at which events occur per unit time, $\lambda$, should be 
constant (not time-dependent). Then, for fixed $t$, $Y(t) \sim \text{Poisson}(\lambda t)$. 

For Poisson count data, the data are (typically) of the form: 

| Follow-up time/person-time | Number of events | Covariates |
| -------------------------- | ---------------- | ---------- |
| $t_1$                      | $Y_1$            | $x_1$      |
| $t_2$                      | $Y_2$            | $x_2$      |
| ...                        | ...              | ...        |

Assuming each $Y_i$ arises from a time-homogeneous Poisson process with rate $\lambda_i$, 

$$Y_i \sim \text{Poisson}(\lambda_i t_i).$$

$$\E[Y_i] = \Var[Y_i] = \lambda_i t_i$$

$t_i$ is often "person-time" to allow for the fact that each unit in the data could represent
populations of different sizes. 

In Poisson regression, we assume that 

$$Y_i \sim \text{Poisson}(\mu_i)$$
$$\mu_i = \lambda_i t_i$$ 

Similar to $N_i$ in the binomial count setting, $t_i$ is assumed to be fixed
and known, so we build the systematic component of the model around the rate
parameter $\lambda_i$. 

Since $\lambda_i$ is strictly positive, Poisson regression uses the log link: 

$$\log(\lambda_i) = \log \left( \frac{\mu_i}{t_i} \right) = x_i' \beta$$

This can be equivalently written as 
$$\log(\mu_i) = x_i' \beta + \log(t_i)$$

# Lab 

What is a saturated model?  A model where the number of observations is 
equal to the number of parameters.  The result is often a perfectly fit model. 

This can be useful for a binomial model on a contingency table. 

## Closed Form Logistic (Binary Outcome, Binary Treatment)

$$\logit(\mu_i) = \beta_0 + \beta_1 x_i$$

In general we cannot obtain closed-form expressions for $\hat \beta_0$ and 
$\hat \beta_1$ or for higher dimension $\beta$ by solving the logistic regression
score equations analytically. However, let's consider a scenario in 
which we can obtain closed-form solutions. In particular, suppose our 
covariate $x_i$ is binary and we want to fit a model to our data: 

|       | Y = 1         | 0             |                   |
| ----- | ------------- | ------------- | ----------------- |
| X = 1 | $O_{11}$      | $O_{12}$      | $O_{1\cdot}$      |
| 0     | $O_{21}$      | $O_{22}$      | $O_{2\cdot}$      |
|       | $O_{\cdot 1}$ | $O_{2 \cdot}$ | $O_{\cdot \cdot}$ |

Let $\mu_0 = P(Y_i = 1 | X_i = 0)$ and $\mu_1 = P(Y_i = 1 | X_i = 1)$. 

Write an expression for the log-likelihood in terms of $\mu_0$, $\mu_1$ and 
the observed cell counts.

$$L(\mu_0, \mu_1 \mid y ) = \prod_{i=1}^n \mu_i^{y_i}(1-\mu_i)^{1-y_i}$$

$$ = \prod_{i=1}^{O_{2\cdot}} \mu_0^{y_i}(1-\mu_0)^{1-y_i} \cdot \prod_{j=1}^{O_{1\cdot}} \mu_1^{y_i} (1-\mu_1)^{1-y_i}$$

$$ = \mu_0^{\sum_{i=1}^{O_{2 \cdot }} y_i} (1 - \mu_0)^{\sum_{i=1}^{O_{2\cdot}} (1-y_i)} 
\cdot 
\mu_1^{\sum_{i=1}^{O_{1 \cdot }} y_i} (1 - \mu_1)^{\sum_{i=1}^{O_{1\cdot}} (1-y_i)}$$

$$ = \mu_0^{O_{21}} (1-\mu_0)^{O_{22}} \mu_1^{O_{11}} (1-\mu_1)^{O_{12}}$$

(Note that the indexing is a little bit messed up; really we should have been writing 
$\sum_{i : X_i = 0}^n y_i = O_{21}$ and such.)

$$\ell (\mu_0, \mu_1 \mid y ) = 
  O_{21} \log (\mu_0) + O_{22} \log (1-\mu_0) + 
  O_{11} \log (\mu_1) + O_{12} \log (1-\mu_1) $$

Based on this new log-likelihood, let's obtain the score equations $U(\mu_0)$ and 
$U(\mu_1)$ and solve to obtain closed-form expressions for $\hat \mu_0$ and $\hat \mu_1$. 

So the score equations are 

$$
\begin{aligned}
U(\mu_0) & = \frac{d}{d \mu_0} \ell (\mu_0, \mu_1 \mid y) \\
& = \frac{d}{d\mu_0} \left( 
   O_{21} \log (\mu_0) + O_{22} \log (1-\mu_0) + 
  O_{11} \log (\mu_1) + O_{12} \log (1-\mu_1)
\right) \\
& = \frac{O_{21}}{\mu_0} - \frac{O_{22}}{1-\mu_0}
\end{aligned}
$$

and 
$$
\begin{aligned}
U(\mu_1) & = \frac{d}{d \mu_0} \ell (\mu_0, \mu_1 \mid y) \\
& = \frac{d}{d\mu_0} \left( 
   O_{21} \log (\mu_0) + O_{22} \log (1-\mu_0) + 
  O_{11} \log (\mu_1) + O_{12} \log (1-\mu_1)
\right) \\
& = \frac{O_{11}}{\mu_1} - \frac{O_{12}}{1-\mu_1}
\end{aligned}
$$

To solve to obtain closed form expressions of $\hat \mu_0$ and $\hat \mu_1$, we
take these derivatives and set them equal to zero. 

So 
$$0 = \frac{O_{11}}{\hat\mu_1} - \frac{O_{12}}{1-\hat\mu_1} \Longrightarrow 
\frac{O_{12}}{1-\hat\mu_1}  = \frac{O_{11}}{\hat\mu_1} 
$$

$$ \Longleftrightarrow (1-\hat\mu_1)O_{11} = \hat\mu_1 O_{12} = O_{11} - \hat\mu_1 O_{11}$$

$$ \hat \mu_1 (O_{12} + O_{11}) = O_{11} \Longleftrightarrow \mu_1 = \frac{O_{11}}{O_{1\cdot}}$$

And similar for $\hat \mu_0 = \frac{O_{21}}{O_{2\cdot}}$.

What we're really after are the $\hat \beta$ values. 

$$\logit(\mu_0) = \beta_0 \quad \text{or} \quad \logit(\mu_1) = \beta_0 + \beta_1$$

So 
$$
\begin{aligned}
\hat \beta_0 = \logit(\hat \mu_0) & = \log\left(\frac{O_{21} / O_{2\cdot}}{1 - O_{21}/O_{2\cdot} } \right) \\ 
& = \log \left( \frac{O_{21} / \cancel{O_{2\cdot}}}{O_{22}/\cancel{O_{2\cdot}}}\right) \\
& = \log \left( \frac{O_{21}}{O_{22}} \right) \\
& = \text{ the log odds.}
\end{aligned}
$$

Similarly, 

$$
\begin{aligned}
\hat \beta_1 = \logit(\hat \mu_1) - \logit(\hat \mu_0) = \log\left( \frac{O_{11}}{O_{12}} \right) - 
\log \left( \frac{O_{21}}{O_{22}}\right) \\ 
& = \log \left( \frac{O_{11}O_{22}}{O_{21}O_{12}}\right)
\end{aligned}
$$

:::{.hottip}
How do we know that these are MLEs? If we plug in MLEs for the $\hat \mu$ values 
into the formula for the $\hat \beta$ values, how do we know we get MLEs? 

MLEs have nice asymptotic properties that if you plug in an MLE into a function, you 
get an MLE for the function output. 
:::

We showed that the Fisher information matrix $\mathcal I(\beta)$ takes the following 
form for a general logistic regression: 

$$
\mathcal I(\beta) = \begin{pmatrix} \sum_{i=1}^n \mu_i ( 1- \mu_i) & \sum_{i=1}^n x_i \mu_i ( 1- \mu_i) \\ 
\sum_{i=1}^n x_i \mu_i ( 1- \mu_i) & \sum_{i=1}^n x_i^2 \mu_i ( 1- \mu_i) \end{pmatrix}
$$


Derive an asymptotic variance estimator for $\hat \beta_1$ by: 

   * Simplifying $\mathcal I(\beta)$ from the above in terms of $\mu_0$ and $\mu_1$ and the observed cell counts. 
   * Calculating $\mathcal I(\beta)^{-1} \vert_{\mu = \hat \mu}$. 

   Hint: 
   $$\begin{pmatrix} a & b \\ c & d \end{pmatrix}^{-1} = \frac{1}{ad - bc} \begin{pmatrix}
   d & -b \\ -c & a \end{pmatrix}
   $$

:::{.cooltip}
$$\mathcal I(\beta) = \begin{bmatrix}
\sum_{i:X_i = 0}^{n} \mu_i(1-\mu_i) +  \sum_{i:X_i = 1}^{n} \mu_i(1-\mu_i) & ... \\
... & ... 
\end{bmatrix}$$

$$\mathcal I(\beta) = \begin{bmatrix}
O_{2\cdot} \mu_0 (1-\mu_0) + O_{1\cdot} \mu_1(1-\mu_1) & O_{1\cdot} \mu_1(1-\mu_1) \\
O_{1\cdot} \mu_1(1-\mu_1) & O_{1\cdot} \mu_1(1-\mu_1)
\end{bmatrix}$$

So we get that

$$\mathcal I^{-1}(\beta) = \frac{1}{(O_{2\cdot} \mu_0 (1-\mu_0) + O_{1\cdot} \mu_1(1-\mu_1))(O_{1\cdot} \mu_1(1-\mu_1)) - (O_{1\cdot} \mu_1(1-\mu_1))^2} \begin{bmatrix}
O_{1\cdot} \mu_1(1-\mu_1) & -O_{1\cdot} \mu_1(1-\mu_1) \\ 
- O_{1\cdot} \mu_1(1-\mu_1) & O_{2\cdot} \mu_0 (1-\mu_0) + O_{1\cdot} \mu_1(1-\mu_1)
\end{bmatrix}$$ 

Note the simplification we can make in $a$. 

$$\begin{aligned}
\Var(\hat \beta_1) & = \mathcal I^{-1}(\beta)_{22} \vert_{\mu = \hat \mu} \\ 
& = \frac{a}{ad-bc} = \frac{O_{2\cdot} \mu_0 (1-\mu_0) + O_{1 \cdot } \mu_1 (1-\mu_1)}{
  O_{2\cdot} \mu_0 (1-\mu_0) \times O_{1 \cdot } \mu_1 (1-\mu_1)
} \\ 
& = \frac{1}{O_{1\cdot} \mu_{1}(1-\mu_1)} + \frac{1}{O_{2\cdot} \mu_0 (1-\mu_0)} \\ 
& = \frac{1}{O_{1\cdot} \frac{O_{11}O_{12}}{O_{1\cdot} O_{1\cdot}}} + \frac{1}{O_{2\cdot} \frac{O_{21}O_{22}}{O_{2\cdot} O_{2\cdot}}}  \\ 
& = \frac{1}{O_{12}} + \frac{1}{O_{11}} + \frac{1}{O_{22}} + \frac{1}{O_{21}}
\end{aligned}
$$

This matches what we derived using the delta method
:::

## Conditional Logistic Regression 

Suppose by experimental design, or by sampling from the data we have, we've 
matched cases and controls by their characteristics. 

| Strata | $Y$ | Matched $Z$ | $X$ |
| ------ | --- | ----------- | --- |
| 1      | 1   | Female, 29  |     |
| 1      | 0   | Female, 29  |     |
| 1      | 0   | Female, 29  |     |
| 2      | 1   | Male, 32    |     |
| 2      | 0   | Male, 32    |     |

The likelihood is instead retrospective now: 

$$\mathcal L_R = \prod_{k=1}^{K} \prod_{i=1}^{N_k} P(Y_{ki} = y_{ki} \mid X_{ki}, \text{Strata} = k)$$

So one could just treat the strata as another variable, and write that 

$$\logit(Y) = \beta_0 + \beta_1 X + \beta_2 \text{Strata}$$

The problem with this is that the more strata we have, the more parameters we have,
we can't do inference because we're not increasing the number of samples relative to 
the number of parameters that we have. 

Instead we'll use conditional logistic regression to analyze matched case-control 
studies. 

A challenging question is in what situation conditioning doesn't really change anything. 

The full proof requires the principle of sufficiency, which will be seen in later classes. 

For $K$ matched sets, where $T_k$ is the number of cases in set $k$ and $N_k$ is the 
number of observations in set $k$. Let $X_{ki}$ be th eexposure for observation $i$ 
in set $k$ and $y_{ki}$ the outcome for observation $i$ in set $k$. Then the conditional
likelihood is given as 

$$\mathcal L_C(\beta_1) = \prod_{k=1}^K \frac{I(T_k = t_k) \exp \left( \beta_1 \sum_{i=1}^{N_k} y_{ki} X_{ki} \right)}{\sum_{u_k : T_k = t_k} \exp \left( \beta_1 \sum_{i=1}^{N_k} u_{ki} X_{ki} \right)}$$

We will show that for a 1:1 matched case-control study with a binary exposure and 
outcome, the conditional odds ratio is $\frac{n_{10}}{n_{01}}$$ where $n_{10}$ is 
the number of discordant pairs where the case is exposed and the control is not, and $n_{01}$
is the number of discordant pairs where the case is unexposed and the control is exposed. 
$$u_{k} = (u_{k1}, u_{k2}) = \{ (0,1), (1, 0) \}.$$

$u$ encodes the sampling schema, so $(0,1)$ represents sampling the control first, then
the case, and $(1,0)$ vice versa. 

Derive the conditional log-likelihood of $\beta_1$. 

$$\begin{aligned}
\mathcal L_C(\beta_1) & = \prod_{k=1}^K \frac{\exp \left( \beta_1 (1 \cdot X_{k1} + 0 \cdot X_{k1} \right)}{\exp \left( \beta_1 (0 \cdot X_{k1} + 1 \cdot X_{k2}) \right) + \exp \left( \beta_1(1 \cdot X_{k1} + 0 \cdot X_{k2})\right)} \\
& = \prod_{k=1}^K \frac{\exp(\beta_1 X_{k1})}{\exp(\beta_1 X_{k1}) + \exp (\beta_1 X_{k2})} \\ 
\ell_C(\beta_1) & = \sum_{k=1}^K \log \left( \frac{\exp(\beta_1 X_{k1})}{\exp(\beta_1 X_{k1}) + \exp (\beta_1 X_{k2})} \right) \\ 
& = \sum_{k=1}^K \beta_1 X_{k1} - \log(\exp(\beta_1 X_{k1}) + \exp(\beta_1 X_{k2})) \\ 
& = \sum_{k : X_{k1} = X_{k2}} \underbrace{...}_{\small =-\log(2)} + \sum_{k : X_{k1} = 1, X_{k2} = 0} \underbrace{...}_{\small =\beta_1 - \log(1 + \exp(\beta_1))} + \sum_{k : X_{k1} = 0, X_{k2} = 1} \underbrace{...}_{\small -\log(1 + \exp(\beta_1))} \\ 
& = (n_{00} + n_{11})(-\log 2) + n_{10}(\beta_1 - \log(1 + \exp(\beta_1))) + n_{01}(-\log(1+\exp(\beta_1))) \\ 
& = (n_{00} + n_{11})(-\log 2) + n_{10} \beta_1 - (n_{10} + n_{01})(\log(1 + \exp(\beta_1)))
\end{aligned}
$$

Show that $\exp(\hat \beta_1) = \frac{n_{10}}{n_{01}}.$$

Now we just solve the score equation. 

$$\frac{d}{d \beta_1} \ell_C(\beta_1) = n_{10} - (n_{10} + n_{01}) \frac{\exp(\beta_1)}{1 + \exp(\beta_1)} \stackrel{\text{set}}{=} 0$$

<!-- $$\frac{\exp(\beta_1)}{1 + \exp(\beta_1)} = \frac{n_{10}}{n_{01} + n_{10}}$$ --> 

$$n_{10} + n_{10} \exp(\beta_1) = n_{10} \exp(\beta_1) + n_{01} \exp(\beta_1)$$
$$\Rightarrow \exp (\hat \beta_1) = \frac{n_{10}}{n_{01}}$$

Find the observed and estimated information $\mathcal I (\beta_1)$ and $\hat{\mathcal I} (\hat \beta_1)$:

$$\mathcal I(\beta_1) = -\frac{\partial^2}{\partial \beta_1^2} \ell_C(\beta_1) = (n_{10} + n_{01}) \frac{\exp(\beta_1)}{(1+\exp(\beta_1))^2}$$

$$\hat{ \mathcal I} (\hat \beta_1) = (n_{10} + n_{01}) \frac{n_{10}/n_{01}}{(1+n_{10}/n_{01})^2} = \frac{n_{10}n_{01}}{n_{10} + n_{01}} $$